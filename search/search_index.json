{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Hey, thanks for stopping by!</p> <p>Network Analysis Made Simple is a collection of Jupyter notebooks designed to help you get up and running with the NetworkX package in the Python programming language. It's written by programmers for programmers, and will give you a basic introduction to graph theory, applied network science, and advanced topics to help kickstart your learning journey. There's even case studies to help those of you for whom example narratives help a ton!</p> <p>We hope you enjoy learning from it.</p>"},{"location":"#introduction-videos","title":"Introduction Videos","text":"<p>At the beginning of each \"chapter\", there's an introduction video just like the one you'll see embedded below. Those videos will give you an overview of the chapter, particularly what to look out for and what the learning goals are, and are designed to orient you on the right path. If you're not the audio/visual kind, feel free to skip past them :). Because they're hosted on YouTube, if you need captions, hit the captions button to get access to them.</p>"},{"location":"#using-the-book","title":"Using the book","text":"<p>There are three ways to use this website/web book.</p> <p>Firstly, you can view everything online at this site. Use the navigation to help you get around, or search for a specific topic that you're interested in.</p> <p>Secondly, you can launch a binder session. Binder lets you execute the notebook code inside the book. Click on the Binder button below to get started!</p> <p></p> <p>Finally, you can pick up the official EPUB/MOBI/PDF version of the book on LeanPub! Purchasing a copy helps support the authors, and funds future improvements and updates to the book, which you will continue to receive as we make updates!</p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you have feedback for the eBook, please head over to our GitHub repository and raise an issue there.</p>"},{"location":"#support-us","title":"Support us!","text":"<p>If you find the book useful, you can support the creators in the following ways:</p> <ol> <li>Star the repository! It costs you nothing, and helps raise the profile of the book.</li> <li>Share the website with your colleagues! It also costs you nothing, and helps share the good stuff with those you think might benefit from it.</li> <li>Take the official companion courses and projects on DataCamp! It does cost some money, so we totally understand if you'd prefer not to, but it does buy us coffee :).</li> <li>Support Eric Ma on Patreon with a monthly coffee pledge to keep him caffeinated, which helps him make other good material to share.</li> <li>Follow Eric and Mridul on Twitter at @ericmjl and @Mridul_Seth</li> <li>Purchase the companion book on LeanPub and fund coffee that way too!</li> </ol>"},{"location":"learn-more/","title":"Further Learning","text":"<p>Thank you for making it this far! We hope you've enjoyed the book. If you want to further your learning, here's a few resources to keep you going.</p>"},{"location":"learn-more/#academic-books","title":"Academic Books","text":"Statistics <p>\"Statistical Analysis of Network Data\" is an incredible resource for learning how to analyze graph data from a statistical viewpoint. It is written by Boston University's professor of mathematics Eric D. Kolaczyck. I used it during graduate school as part of my personal learning journey. The book's website can be found here, and is available on Amazon (click on the book link below).</p> <p></p> Network Science <p>This is a book by Prof. Albert-Laszlo Barabasi, and is freely available online. In it, he explores network analysis from the perspective of an applied academic discipline, showing universal properties and processes that underly networks.</p> Think Complexity <p>This is a book by Prof. Allen Downey at the Olin College of Engineering. In fact, this was the first book that exposed me (Eric Ma) to network science and its ideas, which thus inspired my thesis topic, which then gave me the impetus to learn graph theory and make this tutorial. I hope it becomes a useful thing for you too. You can find the book at Green Tea Press for free, but do consider purchasing a copy to support Allen's work!</p> Spectral Graph Theory <p>This is a book by UCSD Prof. Fan Chung. It is being partially revised, with chapters available online. Contains valuable information on the connections between graphs and linear algebra.</p>"},{"location":"learn-more/#online-resources","title":"Online Resources","text":"Snacks <p>Snacks is a repository of network analysis learning tools curated in the same spirit as the \"Awesome-X\" repositories that show up on GitHub. You can find it here.</p>"},{"location":"learn-more/#interesting-papers","title":"Interesting Papers","text":"Network Analysis <p>Here are some interesting papers that showcase the power of network analysis:</p> <ul> <li>Matrix-Weighted Networks for Modeling Multidimensional Dynamics: Theoretical Foundations and Applications to Network Coherence - A comprehensive overview of network analysis techniques and applications.</li> </ul>"},{"location":"00-preface/01-setup/","title":"Get Setup","text":""},{"location":"00-preface/01-setup/#introduction","title":"Introduction","text":"<p>In order to get the most of this book, you will want to be able to execute the examples in the notebooks, modify them, break the code, and fix it. Pedagogically, that is the best way for you to learn the concepts. Here's the recommended way to get set up.</p>"},{"location":"00-preface/01-setup/#quick-setup","title":"Quick Setup","text":"<p>To get started with the notebooks, follow these simple steps:</p> <ol> <li> <p>Install uv (the Python package manager):    Follow the installation instructions at https://docs.astral.sh/uv/getting-started/installation/</p> </li> <li> <p>Navigate to the notebook directory:    <pre><code>cd notebooks/subdir/\n</code></pre></p> </li> <li> <p>Run the notebook:    <pre><code>uvx marimo edit --sandbox &lt;notebook_name&gt;.py\n</code></pre></p> </li> </ol> <p>That's it! The <code>--sandbox</code> flag ensures a clean, isolated environment for running the notebooks with all necessary dependencies automatically managed.</p>"},{"location":"00-preface/01-setup/#what-this-does","title":"What this does","text":"<ul> <li>uv is a fast Python package manager that handles dependency resolution and virtual environments</li> <li>marimo is an interactive notebook environment optimized for Python</li> <li>The <code>--sandbox</code> flag creates an isolated environment for each notebook, preventing dependency conflicts</li> <li>All required packages are automatically installed when you run the notebook</li> </ul> <p>This approach eliminates the need for manual environment setup, conda environments, or Docker containers while ensuring reproducible execution of the tutorial content.</p>"},{"location":"00-preface/02-prereqs/","title":"Prerequisites","text":"<p>To get maximum benefit from this book, you should know how to program in Python. (Hint: it's an extremely useful skill to know!) In particular, knowing how to:</p> <ol> <li>use dictionaries,</li> <li>write list comprehensions, and</li> <li>handle <code>pandas</code> DataFrames,</li> </ol> <p>will help you a ton during the tutorial.</p> <p>Given the following line of code:</p> <pre><code>[s for s in my_fav_things if s[\u2018name\u2019] == \u2018raindrops on roses\u2019]\n</code></pre> <p>What are plausible data structures for <code>s</code> and <code>my_fav_things</code>?</p> <p>Given the following data:</p> <pre><code>names = [\n    {\n        'name': 'Eric',\n         'surname': 'Ma'\n    },\n    {\n        'name': 'Jeffrey',\n        'surname': 'Elmer'\n    },\n    {\n        'name': 'Mike',\n        'surname': 'Lee'\n    },\n    {\n        'name': 'Jennifer',\n        'surname': 'Elmer'\n    }\n]\n</code></pre> <p>Write a function that takes in the <code>names</code> list of dictionaries and returns the dictionaries in which the <code>surname</code> value matches exactly some <code>query_surname</code>.</p> <pre><code>def find_persons_with_surname(persons, query_surname):\n    # Assert that the persons parameter is a list. \n    # This is a good defensive programming practice.\n    assert isinstance(persons, list)   \n\n    results = []\n    for ______ in ______:\n        if ___________ == __________:\n            results.append(________)\n\n    return results\n</code></pre> <p>To test your implementation, check it with the following code. No errors should be raised.</p> <pre><code># Test your result below.\n# results = find_persons_with_surname(names, 'Lee')\n# assert len(results) == 1\n\n# results = find_persons_with_surname(names, 'Elmer')\n# assert len(results) == 2\n</code></pre>"},{"location":"00-preface/02-prereqs/#exercises","title":"Exercises","text":"<p>We have a few exercises below that should help you get warmed up.</p>"},{"location":"00-preface/02-prereqs/#exercise-1","title":"Exercise 1","text":""},{"location":"00-preface/02-prereqs/#exercise-2","title":"Exercise 2","text":""},{"location":"00-preface/03-goals/","title":"Learning Goals","text":"<p>Our learning goals for you with this book can be split into the technical and the intellectual.</p>"},{"location":"00-preface/03-goals/#technical-takeaways","title":"Technical Takeaways","text":"<p>Firstly, we would like to equip you to be familiar with the NetworkX application programming interface (API). The reason for choosing NetworkX is because it is extremely beginner-friendly, and has an API that matches graph theory concepts very closely.</p> <p>Secondly, we would like to show you how you can visualize graph data in a fashion that doesn't involve showing mere hairballs. Throughout the book, you will see examples of what we call rational graph visualizations. One of our authors, Eric Ma, has developed a companion package, <code>nxviz</code>, that provides a declarative and convenient API (in other words an attempt at a \"grammar\") for graph visualization.</p> <p>Thirdly, in this book, you will be introduced to basic graph algorithms, such as finding special graph structures, or finding paths in a graph. Graph algorithms will show you how to \"think on graphs\", and knowing how to do so will broaden your ability to interact with graph data structures.</p> <p>Fourthly, you will also be equipped with the connection between graph theory and other areas of math and computing, such as statistical inference and linear algebra.</p>"},{"location":"00-preface/03-goals/#intellectual-goals","title":"Intellectual Goals","text":"<p>Beyond the technical takeaways, we hope to broaden how you think about data.</p> <p>The first idea we hope to give you the ability to think about your data in terms of \"relationships\". As you will learn, relationships are what give rise to the interestingness of graphs. That's where relational insights can come to fore.</p> <p>The second idea we hope to give you is the ability to \"think on graphs\". This comes with practice. Once you master it, though, you will find yourself becoming more and more familiar with algorithmic thinking. which is where you look at a problem in terms of the algorithm that solves it.</p>"},{"location":"00-preface/preface/","title":"Preface","text":"<p>Hey, thanks for picking up this e-Book. We had a ton of fun making the material, and we hope you have a ton of fun learning new things from it too.</p> <p>Applied network analysis, and graph theory concepts, are getting more and more relevant in our world. Graph problems are abound. Once you pick up how to use graphs in an applied setting, you'll find your view of data problems change tremendously. We hope this book can become part of your learning journey.</p> <p>The act of purchasing this book means you've chosen to support us, the authors. It means a ton to us, as this book is the culmination of 5 years of learning and teaching applied network analysis at conferences around the world. The reason we went with LeanPub to publish this book is this: For as long as we issue updates to the book, you will also receive an updated copy of it. And because the book is digital, it's easy for us to get updates out to you.</p> <p>Just so you know, the full text of the book is available online too, at the accompanying website, https://ericmjl.github.io/Network-Analysis-Made-Simple. On there, you'll find a link to Binder so you can interact with the code, and through the act of playing around with the code and breaking it yourself, learn new things. (Breaking code and fixing it is something you should be doing - it's one of the best ways to learn!)</p> <p>If you have questions about the content, or find an errata that you'd like to point out, please head over to https://github.com/ericmjl/Network-Analysis-Made-Simple/, and post an issue up there. We'll be sure to address it and acknowledge it appropriately.</p> <p>We hope that this book becomes a stepping stone in your learning journey. Enjoy!</p> <p>Eric &amp; Mridul</p>"},{"location":"01-introduction/01-graphs/","title":"Chapter 1: Introduction to Graphs","text":"<pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"k4KHoLC7TFE\", width=\"100%\")\n</code></pre> <p>In our world, networks are an immensely useful data modelling tool to model complex relational problems. Building on top of a network-oriented data model, they have been put to great use in a wide variety of settings.</p>"},{"location":"01-introduction/01-graphs/#introduction","title":"Introduction","text":""},{"location":"01-introduction/01-graphs/#a-formal-definition-of-networks","title":"A formal definition of networks","text":"<p>Before we explore examples of networks, we want to first give you a more formal definition of what networks are. The reason is that knowing a formal definition helps us refine our application of networks. So bear with me for a moment.</p> <p>In the slightly more academic literature, networks are more formally referred to as graphs.</p> <p>Graphs are comprised of two sets of objects:</p> <ul> <li>A node set: the \"entities\" in a graph.</li> <li>An edge set: the record of \"relationships\" between the entities in the graph.</li> </ul> <p>For example, if a node set n is comprised of elements:</p> n = \\{a, b, c, d, ...\\} <p>Then, the edge set e would be represented as tuples of pairs of elements:</p> e = \\{(a, b), (a, c), (c, d), ...\\} <p>If you extracted every node from the edge set e, it should form at least a subset of the node set n. (It is at least a subset because not every node in n might participate in an edge.)</p> <p>If you draw out a network, the \"nodes\" are commonly represented as shapes, such as circles, while the \"edges\" are the lines between the shapes.</p>"},{"location":"01-introduction/01-graphs/#examples-of-networks","title":"Examples of Networks","text":"<p>Now that we have a proper definition of a graph, let's move on to explore examples of graphs.</p> <p>One example I (Eric Ma) am fond of, based on my background as a biologist, is a protein-protein interaction network. Here, the graph can be defined in the following way:</p> <ul> <li>nodes/entities are the proteins,</li> <li>edges/relationships are defined as \"one protein is known to bind with another\".</li> </ul> <p>A more colloquial example of networks is an air transportation network. Here, the graph can be defined in the following way:</p> <ul> <li>nodes/entities are airports</li> <li>edges/relationships are defined as \"at least one flight carrier flies between the airports\".</li> </ul> <p>And another even more relatable example would be our ever-prevalent social networks! With Twitter, the graph can be defined in the following way:</p> <ul> <li>nodes/entities are individual users</li> <li>edges/relationships are defined as \"one user has decided to follow another\".</li> </ul> <p>Now that you've seen the framework for defining a graph, we'd like to invite you to answer the following question: What examples of networks have you seen before in your profession?</p> <p>Go ahead and list it out.</p>"},{"location":"01-introduction/01-graphs/#types-of-graphs","title":"Types of Graphs","text":"<p>As you probably can see, graphs are a really flexible data model for modelling the world, as long as the nodes and edges are strictly defined. (If the nodes and edges are sloppily defined, well, we run into a lot of interpretability problems later on.)</p> <p>If you are a member of both LinkedIn and Twitter, you might intuitively think that there's a slight difference in the structure of the two \"social graphs\". You'd be absolutely correct on that count!</p> <p>Twitter is an example of what we would intuitively call a directed graph. Why is this so? The key here lies in how interactions are modelled. One user can follow another, but the other need not necessarily follow back. As such, there is a directionality to the relationship.</p> <p>LinkedIn is an example of what we would intuitively call an undirected graph. Why is this so? The key here is that when two users are LinkedIn connections, we automatically assign a bi-directional edge between them. As such, for convenience, we can collapse the bi-directional edge into an undirected edge, thus yielding an undirected graph.</p> <p>If we wanted to turn LinkedIn into a directed graph, we might want to keep information on who initiated the invitation. In that way, the relationship is automatically bi-directional.</p>"},{"location":"01-introduction/01-graphs/#edges-define-the-interesting-part-of-a-graph","title":"Edges define the interesting part of a graph","text":"<p>While in graduate school, I (Eric Ma) once sat in a seminar organized by one of the professors on my thesis committee. The speaker that day was John Quackenbush, a faculty member of the Harvard School of Public Health. While the topic of the day remained fuzzy in my memory, one quote stood out:</p> <p>&gt; The heart of a graph lies in its edges, not in its nodes. &gt; (John Quackenbush, Harvard School of Public Health)</p> <p>Indeed, this is a key point to remember! Without edges, the nodes are merely collections of entities. In a data table, they would correspond to the rows. That alone can be interesting, but doesn't yield relational insights between the entities.</p>"},{"location":"01-introduction/02-networkx-intro/","title":"Chapter 2: The NetworkX API","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id='sdF0uJo2KdU', width=\"100%\")\n</code></pre> <p>In this chapter, we will introduce you to the NetworkX API. This will allow you to create and manipulate graphs in your computer memory, thus giving you a language  to more concretely explore graph theory ideas.</p> <p>Throughout the book, we will be using different graph datasets to help us anchor ideas. In this section, we will work with a social network of seventh graders. Here, nodes are individual students, and edges represent their relationships. Edges between individuals show how often the seventh graders indicated other seventh graders as their favourite.</p> <p>The data are taken from the Konect graph data repository</p> <pre><code>import networkx as nx\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nfrom nams import load_data as cf\n\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>G = cf.load_seventh_grader_network()\n</code></pre> <pre><code>type(G)\n</code></pre> <pre>\n<code>networkx.classes.digraph.DiGraph</code>\n</pre> <p>Because the graph is a <code>DiGraph</code>, this tells us that the graph is a directed one.</p> <p>If it were undirected, the type would change:</p> <pre><code>H = nx.Graph()\ntype(H)\n</code></pre> <pre>\n<code>networkx.classes.graph.Graph</code>\n</pre> <pre><code>list(G.nodes())[0:5]\n</code></pre> <pre>\n<code>[np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]</code>\n</pre> <p><code>G.nodes()</code> returns a \"view\" on the nodes. We can't actually slice into the view and grab out a sub-selection, but we can at least see what nodes are present. For brevity, we have sliced into <code>G.nodes()</code> passed into a <code>list()</code> constructor, so that we don't pollute the output. Because a <code>NodeView</code> is iterable, though, we can query it for its length:</p> <pre><code>len(G.nodes())\n</code></pre> <pre>\n<code>29</code>\n</pre> <p>If our nodes have metadata attached to them, we can view the metadata at the same time by passing in <code>data=True</code>:</p> <pre><code>list(G.nodes(data=True))[0:5]\n</code></pre> <pre>\n<code>[(np.int64(1), {'gender': 'male'}),\n (np.int64(2), {'gender': 'male'}),\n (np.int64(3), {'gender': 'male'}),\n (np.int64(4), {'gender': 'male'}),\n (np.int64(5), {'gender': 'male'})]</code>\n</pre> <p>G.nodes(data=True) returns a <code>NodeDataView</code>, which you can see is dictionary-like.</p> <p>Additionally, we can select out individual nodes:</p> <pre><code>G.nodes[1]\n</code></pre> <pre>\n<code>{'gender': 'male'}</code>\n</pre> <p>Now, because a <code>NodeDataView</code> is dictionary-like, looping over <code>G.nodes(data=True)</code> is very much like looping over key-value pairs of a dictionary. As such, we can write things like:</p> <pre><code>for n, d in G.nodes(data=True):\n    # n is the node\n    # d is the metadata dictionary\n    ...\n</code></pre> <p>This is analogous to how we would loop over a dictionary:</p> <pre><code>for k, v in dictionary.items():\n    # do stuff in the loop\n</code></pre> <p>Naturally, this leads us to our first exercise.</p> <pre><code>from nams.solutions.intro import node_metadata\n\n#### REPLACE THE NEXT LINE WITH YOUR ANSWER\nmf_counts = node_metadata(G)\n</code></pre> <p>Test your implementation by checking it against the <code>test_answer</code> function below.</p> <pre><code>from typing import Dict\n\ndef test_answer(mf_counts: Dict):\n    assert mf_counts['female'] == 17\n    assert mf_counts['male'] == 12\n\ntest_answer(mf_counts)\n</code></pre> <p>With this dictionary-like syntax, we can query back the metadata that's associated with any node.</p> <pre><code>list(G.edges())[0:5]\n</code></pre> <pre>\n<code>[(np.int64(1), np.int64(2)),\n (np.int64(1), np.int64(3)),\n (np.int64(1), np.int64(4)),\n (np.int64(1), np.int64(5)),\n (np.int64(1), np.int64(6))]</code>\n</pre> <p>Similar to the <code>NodeView</code>, <code>G.edges()</code> returns an <code>EdgeView</code> that is also iterable. As with above, we have abbreviated the output inside a sliced list to keep things readable. Because <code>G.edges()</code> is iterable, we can get its length to see the number of edges that are present in a graph.</p> <pre><code>len(G.edges())\n</code></pre> <pre>\n<code>376</code>\n</pre> <p>Likewise, we can also query for all of the edge's metadata:</p> <pre><code>list(G.edges(data=True))[0:5]\n</code></pre> <pre>\n<code>[(np.int64(1), np.int64(2), {'count': np.int64(1)}),\n (np.int64(1), np.int64(3), {'count': np.int64(1)}),\n (np.int64(1), np.int64(4), {'count': np.int64(2)}),\n (np.int64(1), np.int64(5), {'count': np.int64(2)}),\n (np.int64(1), np.int64(6), {'count': np.int64(3)})]</code>\n</pre> <p>Additionally, it is possible for us to select out individual edges, as long as they exist in the graph:</p> <pre><code>G.edges[15, 10]\n</code></pre> <pre>\n<code>{'count': np.int64(2)}</code>\n</pre> <p>This yields the metadata dictionary for that edge.</p> <p>If the edge does not exist, then we get an error:</p> <pre><code>&amp;gt;&amp;gt;&amp;gt; G.edges[15, 16]\n</code></pre> <p><pre><code>---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-21-ce014cab875a&gt; in &lt;module&gt;\n----&amp;gt; 1 G.edges[15, 16]\n\n~/anaconda/envs/nams/lib/python3.7/site-packages/networkx/classes/reportviews.py in __getitem__(self, e)\n    928     def __getitem__(self, e):\n    929         u, v = e\n--&amp;gt; 930         return self._adjdict[u][v]\n    931 \n    932     # EdgeDataView methods\n\nKeyError: 16\n</code></pre> </p> <p>As with the <code>NodeDataView</code>, the <code>EdgeDataView</code> is dictionary-like, with the difference being that the keys are 2-tuple-like instead of being single hashable objects. Thus, we can write syntax like the following to loop over the edgelist:</p> <pre><code>for n1, n2, d in G.edges(data=True):\n    # n1, n2 are the nodes\n    # d is the metadata dictionary\n    ...\n</code></pre> <p>Naturally, this leads us to our next exercise.</p> <pre><code>from nams.solutions.intro import edge_metadata\n\n#### REPLACE THE NEXT LINE WITH YOUR ANSWER\nmaxcount = edge_metadata(G)\n</code></pre> <p>Likewise, you can test your answer using the test function below:</p> <pre><code>def test_maxcount(maxcount):\n    assert maxcount == 3\n\ntest_maxcount(maxcount)\n</code></pre> <pre><code>from nams.solutions.intro import adding_students\n\n#### REPLACE THE NEXT LINE WITH YOUR ANSWER\nG = adding_students(G)\n</code></pre> <p>You can verify that the graph has been correctly created by executing the test function below.</p> <pre><code>def test_graph_integrity(G):\n    assert 30 in G.nodes()\n    assert 31 in G.nodes()\n    assert G.nodes[30]['gender'] == 'male'\n    assert G.nodes[31]['gender'] == 'female'\n    assert G.has_edge(30, 31)\n    assert G.has_edge(30, 7)\n    assert G.has_edge(31, 7)\n    assert G.edges[30, 7]['count'] == 3\n    assert G.edges[7, 30]['count'] == 3\n    assert G.edges[31, 7]['count'] == 3\n    assert G.edges[7, 31]['count'] == 3\n    assert G.edges[30, 31]['count'] == 3\n    assert G.edges[31, 30]['count'] == 3\n    print('All tests passed.')\n\ntest_graph_integrity(G)\n</code></pre> <pre>\n<code>All tests passed.\n</code>\n</pre> <pre><code>from nams.solutions.intro import unrequitted_friendships_v1\n#### REPLACE THE NEXT LINE WITH YOUR ANSWER\nunrequitted_friendships = unrequitted_friendships_v1(G)\nassert len(unrequitted_friendships) == 124\n</code></pre> <p>In a previous session at ODSC East 2018, a few other class participants provided the following solutions, which you can take a look at by uncommenting the following cells.</p> <p>This first one by @schwanne is the list comprehension version of the above solution:</p> <pre><code>from nams.solutions.intro import unrequitted_friendships_v2\n# unrequitted_friendships_v2??\n</code></pre> <p>This one by @end0 is a unique one involving sets.</p> <pre><code>from nams.solutions.intro import unrequitted_friendships_v3\n# unrequitted_friendships_v3??\n</code></pre> <pre><code>import nams.solutions.intro as solutions\nimport inspect\n\nprint(inspect.getsource(solutions))\n</code></pre> <pre>\n<code>\"\"\"\nSolutions to Intro Chapter.\n\"\"\"\n\n\ndef node_metadata(G):\n    \"\"\"Counts of students of each gender.\"\"\"\n    from collections import Counter\n\n    mf_counts = Counter([d[\"gender\"] for n, d in G.nodes(data=True)])\n    return mf_counts\n\n\ndef edge_metadata(G):\n    \"\"\"Maximum number of times that a student rated another student.\"\"\"\n    counts = [d[\"count\"] for n1, n2, d in G.edges(data=True)]\n    maxcount = max(counts)\n    return maxcount\n\n\ndef adding_students(G):\n    \"\"\"How to nodes and edges to a graph.\"\"\"\n    G = G.copy()\n    G.add_node(30, gender=\"male\")\n    G.add_node(31, gender=\"female\")\n    G.add_edge(30, 31, count=3)\n    G.add_edge(31, 30, count=3)  # reverse is optional in undirected network\n    G.add_edge(30, 7, count=3)  # but this network is directed\n    G.add_edge(7, 30, count=3)\n    G.add_edge(31, 7, count=3)\n    G.add_edge(7, 31, count=3)\n    return G\n\n\ndef unrequitted_friendships_v1(G):\n    \"\"\"Answer to unrequitted friendships problem.\"\"\"\n    unrequitted_friendships = []\n    for n1, n2 in G.edges():\n        if not G.has_edge(n2, n1):\n            unrequitted_friendships.append((n1, n2))\n    return unrequitted_friendships\n\n\ndef unrequitted_friendships_v2(G):\n    \"\"\"Alternative answer to unrequitted friendships problem. By @schwanne.\"\"\"\n    return len([(n1, n2) for n1, n2 in G.edges() if not G.has_edge(n2, n1)])\n\n\ndef unrequitted_friendships_v3(G):\n    \"\"\"Alternative answer to unrequitted friendships problem. By @end0.\"\"\"\n    links = ((n1, n2) for n1, n2, d in G.edges(data=True))\n    reverse_links = ((n2, n1) for n1, n2, d in G.edges(data=True))\n\n    return len(list(set(links) - set(reverse_links)))\n\n</code>\n</pre>"},{"location":"01-introduction/02-networkx-intro/#introduction","title":"Introduction","text":""},{"location":"01-introduction/02-networkx-intro/#data-model","title":"Data Model","text":"<p>In NetworkX, graph data are stored in a dictionary-like fashion. They are placed under a <code>Graph</code> object, canonically instantiated with the variable <code>G</code> as follows:</p> <pre><code>G = nx.Graph()\n</code></pre> <p>Of course, you are free to name the graph anything you want!</p> <p>Nodes are part of the attribute <code>G.nodes</code>. There, the node data are housed in a dictionary-like container, where the key is the node itself and the values are a dictionary of attributes.  Node data are accessible using syntax that looks like:</p> <pre><code>G.nodes[node1]\n</code></pre> <p>Edges are part of the attribute <code>G.edges</code>, which is also stored in a dictionary-like container. Edge data are accessible using syntax that looks like: </p> <p><pre><code>G.edges[node1, node2]\n</code></pre> Because of the dictionary-like implementation of the graph, any hashable object can be a node. This means strings and tuples, but not lists and sets.</p>"},{"location":"01-introduction/02-networkx-intro/#load-data","title":"Load Data","text":"<p>Let's load some real network data to get a feel for the NetworkX API. This dataset comes from a study of 7th grade students.</p> <p>&gt; This directed network contains proximity ratings between students &gt; from 29 seventh grade students from a school in Victoria. &gt; Among other questions the students were asked &gt; to nominate their preferred classmates for three different activities. &gt; A node represents a student. &gt; An edge between two nodes shows that &gt; the left student picked the right student as his or her answer. &gt; The edge weights are between 1 and 3  &gt; and show how often the left student chose the right student as his/her favourite.</p> <p>In the original dataset, students were from an all-boys school. However, I have modified the dataset to instead be a mixed-gender school.</p>"},{"location":"01-introduction/02-networkx-intro/#understanding-a-graphs-basic-statistics","title":"Understanding a graph's basic statistics","text":"<p>When you get graph data, one of the first things you'll want to do is to check its basic graph statistics: the number of nodes and the number of edges that are represented in the graph. This is a basic sanity-check on your data that you don't want to skip out on.</p>"},{"location":"01-introduction/02-networkx-intro/#querying-graph-type","title":"Querying graph type","text":"<p>The first thing you need to know is the <code>type</code> of the graph:</p>"},{"location":"01-introduction/02-networkx-intro/#querying-node-information","title":"Querying node information","text":"<p>Let's now query for the nodeset:</p>"},{"location":"01-introduction/02-networkx-intro/#exercise-summarizing-node-metadata","title":"Exercise: Summarizing node metadata","text":"<p>&gt; Can you count how many males and females are represented in the graph?</p>"},{"location":"01-introduction/02-networkx-intro/#querying-edge-information","title":"Querying edge information","text":"<p>Now that you've learned how to query for node information, let's now see how to query for all of the edges in the graph:</p>"},{"location":"01-introduction/02-networkx-intro/#exercise-summarizing-edge-metadata","title":"Exercise: Summarizing edge metadata","text":"<p>&gt; Can you write code to verify &gt; that the maximum times any student rated another student as their favourite &gt; is 3 times?</p>"},{"location":"01-introduction/02-networkx-intro/#manipulating-the-graph","title":"Manipulating the graph","text":"<p>Great stuff! You now know how to query a graph for:</p> <ul> <li>its node set, optionally including metadata</li> <li>individual node metadata</li> <li>its edge set, optionally including metadata, and </li> <li>individual edges' metadata</li> </ul> <p>Now, let's learn how to manipulate the graph. Specifically, we'll learn how to add nodes and edges to a graph.</p>"},{"location":"01-introduction/02-networkx-intro/#adding-nodes","title":"Adding Nodes","text":"<p>The NetworkX graph API lets you add a node easily:</p> <pre><code>G.add_node(node, node_data1=some_value, node_data2=some_value)\n</code></pre>"},{"location":"01-introduction/02-networkx-intro/#adding-edges","title":"Adding Edges","text":"<p>It also allows you to add an edge easily:</p> <pre><code>G.add_edge(node1, node2, edge_data1=some_value, edge_data2=some_value)\n</code></pre>"},{"location":"01-introduction/02-networkx-intro/#metadata-by-keyword-arguments","title":"Metadata by Keyword Arguments","text":"<p>In both cases, the keyword arguments that are passed into <code>.add_node()</code> are automatically collected into the metadata dictionary.</p> <p>Knowing this gives you enough knowledge to tackle the next exercise.</p>"},{"location":"01-introduction/02-networkx-intro/#exercise-adding-students-to-the-graph","title":"Exercise: adding students to the graph","text":"<p>&gt; We found out that there are two students that we left out of the network, &gt; student no. 30 and 31.  &gt; They are one male (30) and one female (31),  &gt; and they are a pair that just love hanging out with one another  &gt; and with individual 7 (i.e. <code>count=3</code>), in both directions per pair.  &gt; Add this information to the graph.</p>"},{"location":"01-introduction/02-networkx-intro/#coding-patterns","title":"Coding Patterns","text":"<p>These are some recommended coding patterns when doing network analysis using NetworkX, which stem from my personal experience with the package.</p>"},{"location":"01-introduction/02-networkx-intro/#iterating-using-list-comprehensions","title":"Iterating using List Comprehensions","text":"<p>I would recommend that you use the following for compactness: </p> <pre><code>[d['attr'] for n, d in G.nodes(data=True)]\n</code></pre> <p>And if the node is unimportant, you can do:</p> <pre><code>[d['attr'] for _, d in G.nodes(data=True)]\n</code></pre>"},{"location":"01-introduction/02-networkx-intro/#iterating-over-edges-using-list-comprehensions","title":"Iterating over Edges using List Comprehensions","text":"<p>A similar pattern can be used for edges:</p> <pre><code>[n2 for n1, n2, d in G.edges(data=True)]\n</code></pre> <p>or</p> <pre><code>[n2 for _, n2, d in G.edges(data=True)]\n</code></pre> <p>If the graph you are constructing is a directed graph, with a \"source\" and \"sink\" available, then I would recommend the following naming of variables instead:</p> <pre><code>[(sc, sk) for sc, sk, d in G.edges(data=True)]\n</code></pre> <p>or </p> <pre><code>[d['attr'] for sc, sk, d in G.edges(data=True)]\n</code></pre>"},{"location":"01-introduction/02-networkx-intro/#further-reading","title":"Further Reading","text":"<p>For a deeper look at the NetworkX API, be sure to check out the NetworkX docs.</p>"},{"location":"01-introduction/02-networkx-intro/#further-exercises","title":"Further Exercises","text":"<p>Here's some further exercises that you can use to get some practice.</p>"},{"location":"01-introduction/02-networkx-intro/#exercise-unrequited-friendships","title":"Exercise: Unrequited Friendships","text":"<p>&gt; Try figuring out which students have \"unrequited\" friendships, that is,  &gt; they have rated another student as their favourite at least once,  &gt; but that other student has not rated them as their favourite at least once.</p> <p>Hint: the goal here is to get a list of edges for which the reverse edge is not present.</p> <p>Hint: You may need the class method <code>G.has_edge(n1, n2)</code>. This returns whether a graph has an edge between the nodes <code>n1</code> and <code>n2</code>.</p>"},{"location":"01-introduction/02-networkx-intro/#solution-answers","title":"Solution Answers","text":"<p>Here are the answers to the exercises above.</p>"},{"location":"01-introduction/03-viz/","title":"Chapter 3: Graph Visualization","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"v9HrR_AF5Zc\", width=\"100%\")\n</code></pre> <p>In this chapter, We want to introduce you to the wonderful world of graph visualization.</p> <p>You probably have seen graphs that are visualized as hairballs. Apart from communicating how complex the graph is, hairballs don't really communicate much else. As such, my goal by the end of this chapter is  to introduce you to what I call rational graph visualization.</p> <p>But before we can do that, let's first make sure we understand how to use NetworkX's drawing facilities to draw graphs to the screen. In a pinch, and for small graphs, it's very handy to have.</p> <pre><code>from nams import load_data as cf\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nG = cf.load_seventh_grader_network()\n</code></pre> <pre><code>nx.draw(G)\n</code></pre> <p>Nodes more tightly connected with one another are clustered together.  Initial node placement is done typically at random, so really it's tough to deterministically generate the same figure. If the network is small enough to visualize, and the node labels are small enough to fit in a circle, then you can use the <code>with_labels=True</code> argument to bring some degree of informativeness to the drawing:</p> <pre><code>G.is_directed()\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>nx.draw(G, with_labels=True)\n</code></pre> <p>The downside to drawing graphs this way is that large graphs end up looking like hairballs. Can you imagine a graph with more than the 28 nodes that we have? As you probably can imagine, the default <code>nx.draw(G)</code> is probably not suitable for generating visual insights.</p> <pre><code>import nxviz as nv \nfrom nxviz import annotate\n\n\nnv.matrix(G, group_by=\"gender\", node_color_by=\"gender\")\nannotate.matrix_group(G, group_by=\"gender\")\n</code></pre> <p>What can you tell from the graph visualization? A few things are immediately obvious:</p> <ul> <li>The diagonal is empty: no student voted for themselves as their favourite.</li> <li>The matrix is asymmetric about the diagonal: this is a directed graph!</li> </ul> <p>(An undirected graph would be symmetric about the diagonal.)</p> <p>You might go on to suggest that there is some clustering happening, but without applying a proper clustering algorithm on the adjacency matrix, we would be hard-pressed to know for sure. After all, we can simply re-order the node ordering along the axes to produce a seemingly-random matrix.</p> <pre><code># a = ArcPlot(G, node_color='gender', node_grouping='gender')\nnv.arc(G, node_color_by=\"gender\", group_by=\"gender\")\nannotate.arc_group(G, group_by=\"gender\")\n</code></pre> <p>The Arc Plot forms the basis of the next visualization, the highly popular Circos plot.</p> <pre><code>nv.circos(G, group_by=\"gender\", node_color_by=\"gender\")\nannotate.circos_group(G, group_by=\"gender\")\n</code></pre> <p>Generally speaking, you can think of a Circos Plot as being a more compact and aesthetically pleasing version of Arc Plots.</p> <pre><code>from nxviz import plots\nimport matplotlib.pyplot as plt \n\nnv.hive(G, group_by=\"gender\", node_color_by=\"gender\")\nannotate.hive_group(G, group_by=\"gender\")\n</code></pre> <p>As you can see, with Hive Plots, we first group nodes along two or three radial axes. In this case, we have the boys along one radial axis and the girls along the other. We can also order the nodes along each axis if we so choose to. In this case, no particular ordering is chosen.</p> <p>Next, we draw edges. We start first with edges between groups. That is shown on the left side of the figure, joining nodes in the \"yellow\" and \"green\" (boys/girls) groups. We then proceed to edges within groups. This is done by cloning the node radial axis before drawing edges.</p>"},{"location":"01-introduction/03-viz/#introduction","title":"Introduction","text":""},{"location":"01-introduction/03-viz/#hairballs","title":"Hairballs","text":"<p>The node-link diagram is the canonical diagram we will see in publications. Nodes are commonly drawn as circles, while edges are drawn s lines.</p> <p>Node-link diagrams are common, and there's a good reason for this: it's convenient to draw! In NetworkX, we can draw node-link diagrams using:</p>"},{"location":"01-introduction/03-viz/#matrix-plot","title":"Matrix Plot","text":"<p>A different way that we can visualize a graph is by visualizing it in its matrix form. The nodes are on the x- and y- axes, and a filled square represent an edge between the nodes.</p> <p>We can draw a graph's matrix form conveniently by using <code>nxviz.MatrixPlot</code>:</p>"},{"location":"01-introduction/03-viz/#arc-plot","title":"Arc Plot","text":"<p>The Arc Plot is another rational graph visualization. Here, we line up the nodes along a horizontal axis, and draw arcs between nodes if they are connected by an edge. We can also optionally group and colour them by some metadata. In the case of this student graph, we group and colour them by \"gender\".</p>"},{"location":"01-introduction/03-viz/#circos-plot","title":"Circos Plot","text":"<p>The Circos Plot was developed by Martin Krzywinski at the BC Cancer Research Center. The <code>nxviz.CircosPlot</code> takes inspiration from the original by joining the two ends of the Arc Plot into a circle. Likewise, we can colour and order nodes by node metadata:</p>"},{"location":"01-introduction/03-viz/#hive-plot","title":"Hive Plot","text":"<p>The final plot we'll show is, Hive Plots.</p>"},{"location":"01-introduction/03-viz/#principles-of-rational-graph-viz","title":"Principles of Rational Graph Viz","text":"<p>While I was implementing these visualizations in <code>nxviz</code>, I learned an important lesson in implementing graph visualizations in general:</p> <p>&gt; To be most informative and communicative, &gt; a graph visualization should first prioritize node placement &gt; in a fashion that makes sense.</p> <p>In some ways, this makes a ton of sense. The nodes are the \"entities\" in a graph, corresponding to people, proteins, and ports. For \"entities\", we have natural ways to group, order and summarize (reduce). (An example of a \"reduction\" is counting the number of things.) Prioritizing node placement allows us to appeal to our audience's natural sense of grouping, ordering and reduction.</p> <p>So the next time you see a hairball, I hope you're able to critique it for what it doesn't communicate, and possibly use the same principle to design a better visualization!</p>"},{"location":"02-algorithms/01-hubs/","title":"Chapter 4: Hubs","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"-oimHbVDdDA\", width=560, height=315)\n</code></pre> <p>Because of the relational structure in a graph, we can begin to think about \"importance\" of a node that is induced because of its relationships to the rest of the nodes in the graph.</p> <p>Before we go on, let's think about a pertinent and contemporary example.</p> <pre><code>from nams import load_data as cf\n\nG = cf.load_sociopatterns_network()\n</code></pre> <p>It is loaded as an undirected graph object:</p> <pre><code>type(G)\n</code></pre> <pre>\n<code>networkx.classes.graph.Graph</code>\n</pre> <p>As usual, before proceeding with any analysis, we should know basic graph statistics.</p> <pre><code>len(G.nodes()), len(G.edges())\n</code></pre> <pre>\n<code>(410, 2765)</code>\n</pre> <pre><code>G.neighbors(7)\n</code></pre> <pre>\n<code>&lt;dict_keyiterator at 0x7f8177cc0720&gt;</code>\n</pre> <p>It returns a generator that doesn't immediately return the exact neighbors list. This means we cannot know its exact length, as it is a generator. If you tried to do:</p> <pre><code>len(G.neighbors(7))\n</code></pre> <p>you would get the following error:</p> <pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-13-72c56971d077&gt; in &lt;module&gt;\n----&amp;gt; 1 len(G.neighbors(7))\n\nTypeError: object of type 'dict_keyiterator' has no len()\n</code></pre> <p>Hence, we will need to cast it as a list in order to know both its length and its members: </p> <pre><code>list(G.neighbors(7))\n</code></pre> <pre>\n<code>[np.int64(5),\n np.int64(6),\n np.int64(21),\n np.int64(22),\n np.int64(37),\n np.int64(48),\n np.int64(51)]</code>\n</pre> <p>In the event that some nodes have an extensive list of neighbors, then using the <code>dict_keyiterator</code> is potentially a good memory-saving technique, as it lazily yields the neighbors.</p> <pre><code>from nams.solutions.hubs import rank_ordered_neighbors\n\n#### REPLACE THE NEXT FEW LINES WITH YOUR ANSWER\n# answer = rank_ordered_neighbors(G)\n# answer\n</code></pre> <p>The original implementation looked like the following</p> <pre><code>from nams.solutions.hubs import rank_ordered_neighbors_original\n\n# rank_ordered_neighbors_original??\n</code></pre> <p>And another implementation that uses generators:</p> <pre><code>from nams.solutions.hubs import rank_ordered_neighbors_generator\n\n# rank_ordered_neighbors_generator??\n</code></pre> <pre><code>import networkx as nx\nimport pandas as pd\n\ndcs = pd.Series(nx.degree_centrality(G))\ndcs\n</code></pre> <pre>\n<code>100    0.070905\n101    0.031785\n102    0.039120\n103    0.063570\n104    0.041565\n         ...   \n89     0.009780\n91     0.051345\n96     0.036675\n99     0.034230\n98     0.002445\nLength: 410, dtype: float64</code>\n</pre> <p><code>nx.degree_centrality(G)</code> returns to us a dictionary of key-value pairs, where the keys are node IDs and values are the degree centrality score. To save on output length, I took the liberty of casting it as a pandas Series to make it easier to display.</p> <p>Incidentally, we can also sort the series to find the nodes with the highest degree centralities:</p> <pre><code>dcs.sort_values(ascending=False)\n</code></pre> <pre>\n<code>51     0.122249\n272    0.114914\n235    0.105134\n195    0.105134\n265    0.083130\n         ...   \n135    0.002445\n370    0.002445\n390    0.002445\n398    0.002445\n98     0.002445\nLength: 410, dtype: float64</code>\n</pre> <p>Does the list order look familiar? It should, since the numerator of the degree centrality metric is identical to the number of neighbors, and the denominator is a constant.</p> <pre><code>from nams.functions import ecdf\nfrom nams.solutions.hubs import ecdf_degree_centrality\n\n#### REPLACE THE FUNCTION CALL WITH YOUR ANSWER\necdf_degree_centrality(G)\n</code></pre> <p>Now do it for degree:</p> <pre><code>from nams.solutions.hubs import ecdf_degree\n\n#### REPLACE THE FUNCTION CALL WITH YOUR ANSWER\necdf_degree(G)\n</code></pre> <p>The fact that they are identically-shaped should not surprise you!</p> <pre><code>from nams.solutions.hubs import num_possible_neighbors\n\n#### UNCOMMENT TO SEE MY ANSWER\n# print(num_possible_neighbors())\n</code></pre> <pre><code>from nams.solutions.hubs import circos_plot\n\n#### REPLACE THE NEXT LINE WITH YOUR ANSWER\ncircos_plot(G)\n</code></pre> <p>And here's an alternative view using an arc plot.</p> <pre><code>import nxviz as nv\n\nnv.arc(G, sort_by=\"order\", node_color_by=\"order\")\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <pre><code>from nams.solutions.hubs import visual_insights\n\n#### UNCOMMENT THE NEXT LINE TO SEE MY ANSWER\n# print(visual_insights())\n</code></pre> <pre><code>from nams.solutions.hubs import dc_node_order\n\ndc_node_order(G)\n</code></pre> <p>The somewhat positive correlation between the degree centrality might tell us that this trend holds true. A further applied question would be to ask what behaviour of these nodes would give rise to this pattern. Are these nodes actually exhibit staff? Or is there some other reason why they are staying so long? This, of course, would require joining in further information that we would overlay on top of the graph (by adding them as node or edge attributes) before we might make further statements.</p> <pre><code>from nams.solutions import hubs\nimport inspect\n\nprint(inspect.getsource(hubs))\n</code></pre> <pre>\n<code>\"\"\"Solutions to Hubs chapter.\"\"\"\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\nimport nxviz as nv\nfrom nxviz import annotate\n\nfrom nams import ecdf\n\n\ndef rank_ordered_neighbors(G):\n    \"\"\"\n    Uses a pandas Series to help with sorting.\n    \"\"\"\n    s = pd.Series({n: len(list(G.neighbors(n))) for n in G.nodes()})\n    return s.sort_values(ascending=False)\n\n\ndef rank_ordered_neighbors_original(G):\n    \"\"\"Original implementation of rank-ordered number of neighbors.\"\"\"\n    return sorted(G.nodes(), key=lambda x: len(list(G.neighbors(x))), reverse=True)\n\n\ndef rank_ordered_neighbors_generator(G):\n    \"\"\"\n    Rank-ordered generator of neighbors.\n\n    Contributed by @dgerlanc.\n\n    Ref: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/75\n    \"\"\"\n    gen = ((len(list(G.neighbors(x))), x) for x in G.nodes())\n    return sorted(gen, reverse=True)\n\n\ndef ecdf_degree_centrality(G):\n    \"\"\"ECDF of degree centrality.\"\"\"\n    x, y = ecdf(list(nx.degree_centrality(G).values()))\n    plt.scatter(x, y)\n    plt.xlabel(\"degree centrality\")\n    plt.ylabel(\"cumulative fraction\")\n\n\ndef ecdf_degree(G):\n    \"\"\"ECDF of degree.\"\"\"\n    num_neighbors = [len(list(G.neighbors(n))) for n in G.nodes()]\n    x, y = ecdf(num_neighbors)\n    plt.scatter(x, y)\n    plt.xlabel(\"degree\")\n    plt.ylabel(\"cumulative fraction\")\n\n\ndef num_possible_neighbors():\n    \"\"\"Answer to the number of possible neighbors for a node.\"\"\"\n    return r\"\"\"\nThe number of possible neighbors can either be defined as:\n\n1. All other nodes but myself\n2. All other nodes and myself\n\nIf $K$ is the number of nodes in the graph,\nthen if defined as (1), $N$ (the denominator) is $K - 1$.\nIf defined as (2), $N$ is equal to $K$.\n\"\"\"\n\n\ndef circos_plot(G):\n    \"\"\"Draw a Circos Plot of the graph.\"\"\"\n    # c = CircosPlot(G, node_order=\"order\", node_color=\"order\")\n    # c.draw()\n    nv.circos(G, sort_by=\"order\", node_color_by=\"order\")\n    annotate.node_colormapping(G, color_by=\"order\")\n\n\ndef visual_insights():\n    \"\"\"Visual insights from the Circos Plot.\"\"\"\n    return \"\"\"\nWe see that most edges are \"local\" with nodes\nthat are proximal in order.\nThe nodes that are weird are the ones that have connections\nwith individuals much later than itself,\ncrossing larger jumps in order/time.\n\nAdditionally, if you recall the ranked list of degree centralities,\nit appears that these nodes that have the highest degree centrality scores\nare also the ones that have edges that cross the circos plot.\n\"\"\"\n\n\ndef dc_node_order(G):\n    \"\"\"Comparison of degree centrality by maximum difference in node order.\"\"\"\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import networkx as nx\n\n    # Degree centralities\n    dcs = pd.Series(nx.degree_centrality(G))\n\n    # Maximum node order difference\n    maxdiffs = dict()\n    for n, d in G.nodes(data=True):\n        diffs = []\n        for nbr in G.neighbors(n):\n            diffs.append(abs(G.nodes[nbr][\"order\"] - d[\"order\"]))\n        maxdiffs[n] = max(diffs)\n    maxdiffs = pd.Series(maxdiffs)\n\n    ax = pd.DataFrame(dict(degree_centrality=dcs, max_diff=maxdiffs)).plot(\n        x=\"degree_centrality\", y=\"max_diff\", kind=\"scatter\"\n    )\n\n</code>\n</pre>"},{"location":"02-algorithms/01-hubs/#introduction","title":"Introduction","text":""},{"location":"02-algorithms/01-hubs/#an-example-contact-tracing","title":"An example: contact tracing","text":"<p>At the time of writing (April 2020), finding important nodes in a graph has actually taken on a measure of importance that we might not have appreciated before. With the COVID-19 virus spreading, contact tracing has become quite important. In an infectious disease contact network, where individuals are nodes and contact between individuals of some kind are the edges, an \"important\" node in this contact network would be an individual who was infected who also was in contact with many people during the time that they were infected.</p>"},{"location":"02-algorithms/01-hubs/#our-dataset-sociopatterns","title":"Our dataset: \"Sociopatterns\"","text":"<p>The dataset that we will use in this chapter is the \"sociopatterns network\" dataset. Incidentally, it's also about infectious diseases. </p> <p>Note to readers: We originally obtained the dataset in 2014 from the Konect website. It is unfortunately no longer available. The sociopatterns.org website hosts an edge list of a slightly different format, so it will look different from what we have here.</p> <p>From the original description on Konect, here is the description of the dataset:</p> <p>&gt; This network describes the face-to-face behavior of people &gt; during the exhibition INFECTIOUS: STAY AWAY in 2009 &gt; at the Science Gallery in Dublin. &gt; Nodes represent exhibition visitors; &gt; edges represent face-to-face contacts that were active for at least 20 seconds. &gt; Multiple edges between two nodes are possible and denote multiple contacts. &gt; The network contains the data from the day with the most interactions.</p> <p>To simplify the network, we have represented only the last contact between individuals.</p>"},{"location":"02-algorithms/01-hubs/#a-measure-of-importance-number-of-neighbors","title":"A Measure of Importance: \"Number of Neighbors\"","text":"<p>One measure of importance of a node is the number of neighbors that the node has. What is a neighbor? We will work with the following definition:</p> <p>&gt; The neighbor of a node is connected to that node by an edge.</p> <p>Let's explore this concept, using the NetworkX API.</p> <p>Every NetworkX graph provides a <code>G.neighbors(node)</code> class method, which lets us query a graph for the number of neighbors of a given node:</p>"},{"location":"02-algorithms/01-hubs/#exercise-rank-ordering-the-number-of-neighbors-a-node-has","title":"Exercise: Rank-ordering the number of neighbors a node has","text":"<p>Since we know how to get the list of nodes that are neighbors of a given node, try this following exercise:</p> <p>&gt; Can you create a ranked list of the importance of each individual, based on the number of neighbors they have?</p> <p>Here are a few hints to help:</p> <ul> <li>You could consider using a <code>pandas Series</code>. This would be a modern and idiomatic way of approaching the problem.</li> <li>You could also consider using Python's <code>sorted</code> function.</li> </ul>"},{"location":"02-algorithms/01-hubs/#generalizing-neighbors-to-arbitrarily-sized-graphs","title":"Generalizing \"neighbors\" to arbitrarily-sized graphs","text":"<p>The concept of neighbors is simple and appealing, but it leaves us with a slight point of dissatisfaction: it is difficult to compare graphs of different sizes. Is a node more important solely because it has more neighbors? What if it were situated in an extremely large graph? Would we not expect it to have more neighbors?</p> <p>As such, we need a normalization factor. One reasonable one, in fact, is the number of nodes that a given node could possibly be connected to. By taking the ratio of the number of neighbors a node has to the number of neighbors it could possibly have, we get the degree centrality metric.</p> <p>Formally defined, the degree centrality of a node (let's call it d) is the number of neighbors that a node has (let's call it n) divided by the number of neighbors it could possibly have (let's call it N):</p> d = \\frac{n}{N} <p>NetworkX provides a function for us to calculate degree centrality conveniently:</p>"},{"location":"02-algorithms/01-hubs/#distribution-of-graph-metrics","title":"Distribution of graph metrics","text":"<p>One important concept that you should come to know is that the distribution of node-centric values can characterize classes of graphs.</p> <p>What do we mean by \"distribution of node-centric values\"? One would be the degree distribution, that is, the collection of node degree values in a graph.</p> <p>Generally, you might be familiar with plotting a histogram to visualize distributions of values, but in this book, we are going to avoid histograms like the plague. I detail a lot of reasons in a blog post I wrote in 2018, but the main points are that:</p> <ol> <li>It's easier to lie with histograms.</li> <li>You get informative statistical information (median, IQR, extremes/outliers) more easily.</li> </ol>"},{"location":"02-algorithms/01-hubs/#exercise-degree-distribution","title":"Exercise: Degree distribution","text":"<p>In this next exercise, we are going to get practice visualizing these values using empirical cumulative distribution function plots.</p> <p>I have written for you an ECDF function that you can use already. Its API looks like the following:</p> <pre><code>x, y = ecdf(list_of_values)\n</code></pre> <p>giving you <code>x</code> and <code>y</code> values that you can directly plot.</p> <p>The exercise prompt is this:</p> <p>&gt; Plot the ECDF of the degree centrality and degree distributions.</p> <p>First do it for degree centrality:</p>"},{"location":"02-algorithms/01-hubs/#exercise-what-about-that-denominator","title":"Exercise: What about that denominator?","text":"<p>The denominator N in the degree centrality definition is \"the number of nodes that a node could possibly be connected to\". Can you think of two ways N be defined?</p>"},{"location":"02-algorithms/01-hubs/#exercise-circos-plotting","title":"Exercise: Circos Plotting","text":"<p>Let's get some practice with the <code>nxviz</code> API.</p> <p>&gt; Visualize the graph <code>G</code>, while ordering and colouring them by the 'order' node attribute.</p>"},{"location":"02-algorithms/01-hubs/#exercise-visual-insights","title":"Exercise: Visual insights","text":"<p>Since we know that node colour and order are by the \"order\" in which the person entered into the exhibit, what does this visualization tell you?</p>"},{"location":"02-algorithms/01-hubs/#exercise-investigating-degree-centrality-and-node-order","title":"Exercise: Investigating degree centrality and node order","text":"<p>One of the insights that we might have gleaned from visualizing the graph is that the nodes that have a high degree centrality might also be responsible for the edges that criss-cross the Circos plot. To test this, plot the following:</p> <ul> <li>x-axis: node degree centrality</li> <li>y-axis: maximum difference between the neighbors' <code>order</code>s (a node attribute) and the node's <code>order</code>.</li> </ul>"},{"location":"02-algorithms/01-hubs/#reflections","title":"Reflections","text":"<p>In this chapter, we defined a metric of node importance: the degree centrality metric. In the example we looked at, it could help us identify potential infectious agent superspreaders in a disease contact network. In other settings, it might help us spot:</p> <ul> <li>message amplifiers/influencers in a social network, and </li> <li>potentially crowded airports that have lots of connections into and out of it (still relevant to infectious disease spread!)</li> <li>and many more!</li> </ul> <p>What other settings can you think of in which the number of neighbors that a node has can become a metric of importance for the node?</p>"},{"location":"02-algorithms/01-hubs/#solutions","title":"Solutions","text":"<p>Here are the solutions to the exercises above.</p>"},{"location":"02-algorithms/02-paths/","title":"Chapter 5: Paths","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"JjpbztqP9_0\", width=\"100%\")\n</code></pre> <p>Graph traversal is akin to walking along the graph, node by node, constrained by the edges that connect the nodes. Graph traversal is particularly useful for understanding  the local structure of certain portions of the graph and for finding paths that connect two nodes in the network.</p> <p>In this chapter, we are going to learn how to perform pathfinding in a graph, specifically by looking for shortest paths via the breadth-first search algorithm.</p> <pre><code>from nams import load_data as cf\n\nG = cf.load_sociopatterns_network()\n</code></pre> <pre><code>from nams.solutions.paths import bfs_algorithm\n\n# UNCOMMENT NEXT LINE TO GET THE ANSWER.\n# bfs_algorithm()\n</code></pre> <pre><code># FILL IN THE BLANKS BELOW\n\n\ndef path_exists(node1, node2, G):\n    \"\"\"\n    This function checks whether a path exists between two nodes (node1,\n    node2) in graph G.\n    \"\"\"\n    visited_nodes = _____\n    queue = [_____]\n\n    while len(queue) &amp;gt; 0:\n        node = ___________\n        neighbors = list(_________________)\n        if _____ in _________:\n            # print('Path exists between nodes {0} and {1}'.format(node1, node2))\n            return True\n        else:\n            visited_nodes.___(____)\n            nbrs = [_ for _ in _________ if _ not in _____________]\n            queue = ____ + _____\n\n    # print('Path does not exist between nodes {0} and {1}'.format(node1, node2))\n    return False\n</code></pre> <pre><code># UNCOMMENT THE FOLLOWING TWO LINES TO SEE THE ANSWER\nfrom nams.solutions.paths import path_exists\n\n# path_exists??\n</code></pre> <pre><code># CHECK YOUR ANSWER AGAINST THE TEST FUNCTION BELOW\nfrom random import sample\nimport networkx as nx\n\n\ndef test_path_exists(N):\n    \"\"\"\n    N: The number of times to spot-check.\n    \"\"\"\n    for i in range(N):\n        n1, n2 = sample(list(G.nodes()), 2)\n        assert path_exists(n1, n2, G) == bool(nx.shortest_path(G, n1, n2))\n    return True\n\n\nassert test_path_exists(10)\n</code></pre> <pre><code>path = nx.shortest_path(G, 7, 400)\npath\n</code></pre> <pre>\n<code>[7, np.int64(51), np.int64(188), np.int64(230), np.int64(335), 400]</code>\n</pre> <p>As you can see, it returns the nodes along the shortest path, incidentally in the exact order that you would traverse.</p> <p>One thing to note, though! If there are multiple shortest paths from one node to another, NetworkX will only return one of them.</p> <p>So how do you draw those nodes only?</p> <p>You can use the <code>G.subgraph(nodes)</code> to return a new graph that only has nodes in <code>nodes</code> and only the edges that exist between them. After that, you can use any plotting library you like. We will show an example here that uses nxviz's matrix plot.</p> <p>Let's see it in action:</p> <pre><code>import nxviz as nv\n\ng = G.subgraph(path)\nnv.matrix(g, sort_by=\"order\")\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <p>Voila! Now we have the subgraph (1) extracted and (2) drawn to screen! In this case, the matrix plot is a suitable visualization for its compactness. The off-diagonals also show that each node is a neighbor to the next one.</p> <p>You'll also notice that if you try to modify the graph <code>g</code>, say by adding a node:</p> <pre><code>g.add_node(2048)\n</code></pre> <p>you will get an error:</p> <pre><code>---------------------------------------------------------------------------\nNetworkXError                             Traceback (most recent call last)\n&lt;ipython-input-10-ca6aa4c26819&gt; in &lt;module&gt;\n----&amp;gt; 1 g.add_node(2048)\n\n~/anaconda/envs/nams/lib/python3.7/site-packages/networkx/classes/function.py in frozen(*args, **kwargs)\n    156 def frozen(*args, **kwargs):\n    157     \"\"\"Dummy method for raising errors when trying to modify frozen graphs\"\"\"\n--&amp;gt; 158     raise nx.NetworkXError(\"Frozen graph can't be modified\")\n    159 \n    160 \n\nNetworkXError: Frozen graph can't be modified\n</code></pre> <p>From the perspective of semantics, this makes a ton of sense: the subgraph <code>g</code> is a perfect subset of the larger graph <code>G</code>, and should not be allowed to be modified unless the larger container graph is modified. </p> <pre><code>from nams.solutions.paths import plot_path_with_neighbors\n\n### YOUR SOLUTION BELOW\n</code></pre> <pre><code>plot_path_with_neighbors(G, 7, 400)\n</code></pre> <p>In this case, we opted for an Arc plot because we only have one grouping of nodes but have a logical way to order them. Because the path follows the order, the edges being highlighted automatically look like hops through the graph.</p> <pre><code>import pandas as pd\n\npd.Series(nx.betweenness_centrality(G))\n</code></pre> <pre>\n<code>100    0.014809\n101    0.001398\n102    0.000748\n103    0.006735\n104    0.001198\n         ...   \n89     0.000004\n91     0.006415\n96     0.000323\n99     0.000322\n98     0.000000\nLength: 410, dtype: float64</code>\n</pre> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# YOUR ANSWER HERE:\n</code></pre> <pre><code>from nams.solutions.paths import plot_degree_betweenness\n\nplot_degree_betweenness(G)\n</code></pre> <pre><code>nx.draw(nx.barbell_graph(5, 1))\n</code></pre> <pre><code>from nams.solutions import paths\nimport inspect\n\nprint(inspect.getsource(paths))\n</code></pre> <pre>\n<code>\"\"\"Solutions to Paths chapter.\"\"\"\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\nimport seaborn as sns\nfrom nams.functions import render_html\n\n\ndef bfs_algorithm():\n    \"\"\"\n    How to design a BFS algorithm.\n    \"\"\"\n    ans = \"\"\"\nHow does the breadth-first search work?\nIt essentially is as follows:\n\n1. Begin with a queue that has only one element in it: the starting node.\n2. Add the neighbors of that node to the queue.\n    1. If destination node is present in the queue, end.\n    2. If destination node is not present, proceed.\n3. For each node in the queue:\n    1. Remove node from the queue.\n    2. Add neighbors of the node to the queue. Check if destination node is present or not.\n    3. If destination node is present, end. &lt;!--Credit: @cavaunpeu for finding bug in pseudocode.--&gt;\n    4. If destination node is not present, continue.\n\"\"\"\n    return render_html(ans)\n\n\ndef path_exists(node1, node2, G):\n    \"\"\"\n    This function checks whether a path exists between two nodes (node1,\n    node2) in graph G.\n    \"\"\"\n\n    visited_nodes = set()\n    queue = [node1]\n\n    while len(queue) &gt; 0:\n        node = queue.pop()\n        neighbors = list(G.neighbors(node))\n        if node2 in neighbors:\n            return True\n        else:\n            visited_nodes.add(node)\n            nbrs = [n for n in neighbors if n not in visited_nodes]\n            queue = nbrs + queue\n\n    return False\n\n\ndef path_exists_for_loop(node1, node2, G):\n    \"\"\"\n    This function checks whether a path exists between two nodes (node1,\n    node2) in graph G.\n\n    Special thanks to @ghirlekar for suggesting that we keep track of the\n    \"visited nodes\" to prevent infinite loops from happening. This also\n    removes the need to remove nodes from queue.\n\n    Reference: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/3\n\n    With thanks to @joshporter1 for the second bug fix. Originally there was\n    an extraneous \"if\" statement that guaranteed that the \"False\" case would\n    never be returned - because queue never changes in shape. Discovered at\n    PyCon 2017.\n\n    With thanks to @chendaniely for pointing out the extraneous \"break\".\n\n    If you would like to see @dgerlanc's implementation, see\n    https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/76\n    \"\"\"\n    visited_nodes = set()\n    queue = [node1]\n\n    for node in queue:\n        neighbors = list(G.neighbors(node))\n        if node2 in neighbors:\n            return True\n        else:\n            visited_nodes.add(node)\n            queue.extend([n for n in neighbors if n not in visited_nodes])\n\n    return False\n\n\ndef path_exists_deque(node1, node2, G):\n    \"\"\"An alternative implementation.\"\"\"\n    from collections import deque\n\n    visited_nodes = set()\n    queue = deque([node1])\n\n    while len(queue) &gt; 0:\n        node = queue.popleft()\n        neighbors = list(G.neighbors(node))\n        if node2 in neighbors:\n            return True\n        else:\n            visited_nodes.add(node)\n            queue.extend([n for n in neighbors if n not in visited_nodes])\n\n    return False\n\n\nimport nxviz as nv\nfrom nxviz import annotate, highlights\n\n\ndef plot_path_with_neighbors(G, n1, n2):\n    \"\"\"Plot a path with the heighbors of of the nodes along that path.\"\"\"\n    path = nx.shortest_path(G, n1, n2)\n    nodes = [*path]\n    for node in path:\n        nodes.extend(list(G.neighbors(node)))\n    nodes = list(set(nodes))\n\n    g = G.subgraph(nodes)\n    nv.arc(\n        g, sort_by=\"order\", node_color_by=\"order\", edge_enc_kwargs={\"alpha_scale\": 0.5}\n    )\n    for n in path:\n        highlights.arc_node(g, n, sort_by=\"order\")\n    for n1, n2 in zip(path[:-1], path[1:]):\n        highlights.arc_edge(g, n1, n2, sort_by=\"order\")\n\n\ndef plot_degree_betweenness(G):\n    \"\"\"Plot scatterplot between degree and betweenness centrality.\"\"\"\n    bc = pd.Series(nx.betweenness_centrality(G))\n    dc = pd.Series(nx.degree_centrality(G))\n\n    df = pd.DataFrame(dict(bc=bc, dc=dc))\n    ax = df.plot(x=\"dc\", y=\"bc\", kind=\"scatter\")\n    ax.set_ylabel(\"Betweenness\\nCentrality\")\n    ax.set_xlabel(\"Degree Centrality\")\n    sns.despine()\n\n</code>\n</pre>"},{"location":"02-algorithms/02-paths/#introduction","title":"Introduction","text":""},{"location":"02-algorithms/02-paths/#breadth-first-search","title":"Breadth-First Search","text":"<p>The BFS algorithm is a staple of computer science curricula, and for good reason: it teaches learners how to \"think on\" a graph, putting one in the position of  \"the dumb computer\" that can't use a visual cortex to  \"just know\" how to trace a path from one node to another. As a topic, learning how to do BFS additionally imparts algorithmic thinking to the learner.</p>"},{"location":"02-algorithms/02-paths/#exercise-design-the-algorithm","title":"Exercise: Design the algorithm","text":"<p>Try out this exercise to get some practice with algorithmic thinking.</p> <p>&gt; 1. On a piece of paper, conjure up a graph that has 15-20 nodes. Connect them any way you like. &gt; 1. Pick two nodes. Pretend that you're standing on one of the nodes, but you can't see any further beyond one neighbor away. &gt; 1. Work out how you can find a path from the node you're standing on to the other node, given that you can only see nodes that are one neighbor away but have an infinitely good memory.</p> <p>If you are successful at designing the algorithm, you should get the answer below.</p>"},{"location":"02-algorithms/02-paths/#exercise-implement-the-algorithm","title":"Exercise: Implement the algorithm","text":"<p>&gt; Now that you've seen how the algorithm works, try implementing it!</p>"},{"location":"02-algorithms/02-paths/#visualizing-paths","title":"Visualizing Paths","text":"<p>One of the objectives of that exercise before was to help you \"think on graphs\". Now that you've learned how to do so, you might be wondering, \"How do I visualize that path through the graph?\"</p> <p>Well first off, if you inspect the <code>test_path_exists</code> function above, you'll notice that NetworkX provides a <code>shortest_path()</code> function that you can use. Here's what using <code>nx.shortest_path()</code> looks like.</p>"},{"location":"02-algorithms/02-paths/#exercise-draw-path-with-neighbors-one-degree-out","title":"Exercise: Draw path with neighbors one degree out","text":"<p>Try out this next exercise:</p> <p>&gt; Extend graph drawing with the neighbors of each of those nodes. &gt; Use any of the nxviz plots (<code>nv.matrix</code>, <code>nv.arc</code>, <code>nv.circos</code>); &gt; try to see which one helps you tell the best story.</p>"},{"location":"02-algorithms/02-paths/#bottleneck-nodes","title":"Bottleneck nodes","text":"<p>We're now going to revisit the concept of an \"important node\", this time now leveraging what we know about paths.</p> <p>In the \"hubs\" chapter, we saw how a node that is \"important\" could be so because it is connected to many other nodes.</p> <p>Paths give us an alternative definition. If we imagine that we have to pass a message on a graph from one node to another, then there may be \"bottleneck\" nodes for which if they are removed, then messages have a harder time flowing through the graph.</p> <p>One metric that measures this form of importance is the \"betweenness centrality\" metric. On a graph through which a generic \"message\" is flowing, a node with a high betweenness centrality is one that has a high proportion of shortest paths flowing through it. In other words, it behaves like a bottleneck.</p>"},{"location":"02-algorithms/02-paths/#betweenness-centrality-in-networkx","title":"Betweenness centrality in NetworkX","text":"<p>NetworkX provides a \"betweenness centrality\" function that behaves consistently with the \"degree centrality\" function, in that it returns a mapping from node to metric:</p>"},{"location":"02-algorithms/02-paths/#exercise-compare-degree-and-betweenness-centrality","title":"Exercise: compare degree and betweenness centrality","text":"<p>&gt; Make a scatterplot of degree centrality on the x-axis &gt; and betweenness centrality on the y-axis. &gt; Do they correlate with one another?</p>"},{"location":"02-algorithms/02-paths/#think-about-it","title":"Think about it...","text":"<p>...does it make sense that degree centrality and betweenness centrality are not well-correlated?</p> <p>Can you think of a scenario where a node has a \"high\" betweenness centrality but a \"low\" degree centrality? Before peeking at the graph below, think about your answer for a moment.</p>"},{"location":"02-algorithms/02-paths/#recap","title":"Recap","text":"<p>In this chapter, you learned the following things:</p> <ol> <li>You figured out how to implement the breadth-first-search algorithm to find shortest paths.</li> <li>You learned how to extract subgraphs from a larger graph.</li> <li>You implemented visualizations of subgraphs, which should help you as you communicate with colleagues.</li> <li>You calculated betweenness centrality metrics for a graph, and visualized how they correlated with degree centrality.</li> </ol>"},{"location":"02-algorithms/02-paths/#solutions","title":"Solutions","text":"<p>Here are the solutions to the exercises above.</p>"},{"location":"02-algorithms/03-structures/","title":"Chapter 6: Structures","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"3DWSRCbPPJs\", width=\"100%\")\n</code></pre> <p>If you remember, at the beginning of this book, we saw a quote from John Quackenbush that essentially said that the reason a graph is interesting is because of its edges. In this chapter, we'll see this in action once again, as we are going to figure out how to leverage the edges to find special structures in a graph.</p> <pre><code>from nams import load_data as cf\nG = cf.load_physicians_network()\n</code></pre> <pre><code>from nams.solutions.structures import triangle_finding_strategies\n\n# triangle_finding_strategies()\n</code></pre> <pre><code>def in_triangle(G, node):\n    # Your answer here\n    pass\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import in_triangle\n\n# UNCOMMENT THE NEXT LINE TO SEE MY ANSWER\n# in_triangle??\n</code></pre> <p>Now, test your implementation below! The code cell will not error out if your answer is correct.</p> <pre><code>from random import sample\nimport networkx as nx\n\ndef test_in_triangle():\n    nodes = sample(list(G.nodes()), 10)\n    for node in nodes:\n        assert in_triangle(G, 3) == bool(nx.triangles(G, 3))\n\ntest_in_triangle()\n</code></pre> <p>As you can see from the test function above, NetworkX provides an <code>nx.triangles(G, node)</code> function. It returns the number of triangles that a node is involved in. We convert it to boolean as a hack to check whether or not a node is involved in a triangle relationship because 0 is equivalent to boolean <code>False</code>, while any non-zero number is equivalent to boolean <code>True</code>.</p> <pre><code>def get_triangle_neighbors(G, n):\n    # Your answer here\n    pass\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import get_triangle_neighbors\n\n# UNCOMMENT THE NEXT LINE TO SEE MY ANSWER\n# get_triangle_neighbors??\n</code></pre> <pre><code>def plot_triangle_relations(G, n):\n    # Your answer here\n    pass\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import plot_triangle_relations\n\nplot_triangle_relations(G, 3)\n</code></pre> <pre><code>from nams.solutions.structures import triadic_closure_algorithm\n\n# UNCOMMENT FOR MY ANSWER\n# triadic_closure_algorithm()\n</code></pre> <pre><code>def get_open_triangles_neighbors(G, n):\n    # Your answer here\n    pass\n\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import get_open_triangles_neighbors\n\n# UNCOMMENT THE NEXT LINE TO SEE MY ANSWER\n# get_open_triangles_neighbors??\n</code></pre> <pre><code>def plot_open_triangle_relations(G, n):\n    # Your answer here\n    pass\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import plot_open_triangle_relations\n\nplot_open_triangle_relations(G, 3)\n</code></pre> <pre><code>from nams.solutions.structures import simplest_clique\n\n# UNCOMMENT THE NEXT LINE TO SEE MY ANSWER\n# simplest_clique()\n</code></pre> <pre><code># I have truncated the output to the first 5 maximal cliques.\nlist(nx.find_cliques(G))[0:5]\n</code></pre> <pre>\n<code>[[np.int64(1), np.int64(2)],\n [np.int64(1), np.int64(3)],\n [np.int64(1), np.int64(4), np.int64(5), np.int64(6)],\n [np.int64(1), np.int64(7)],\n [np.int64(1), np.int64(72)]]</code>\n</pre> <pre><code>def size_k_maximal_cliques(G, k):\n    # Your answer here\n    pass\n\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import size_k_maximal_cliques\n</code></pre> <p>Now, test your implementation against the test function below.</p> <pre><code>def test_size_k_maximal_cliques(G, k):\n    clique_generator = size_k_maximal_cliques(G, k)\n    for clique in clique_generator:\n        assert len(clique) == k\n\ntest_size_k_maximal_cliques(G, 5)\n</code></pre> <pre><code>def find_k_cliques(G, k):\n    # your answer here\n    pass\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import find_k_cliques\n\ndef test_find_k_cliques(G, k):\n    for clique in find_k_cliques(G, k):\n        assert len(clique) == k\n\ntest_find_k_cliques(G, 3)\n</code></pre> <pre><code>import nxviz as nv\n\nnv.circos(G)\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <pre><code>from nams.solutions.structures import visual_insights\n\n# UNCOMMENT TO SEE MY ANSWER\n# visual_insights()\n</code></pre> <pre><code>ccsubgraph_nodes = list(nx.connected_components(G))\n</code></pre> <p>Let's see how many connected component subgraphs are present:</p> <pre><code>len(ccsubgraph_nodes)\n</code></pre> <pre>\n<code>4</code>\n</pre> <pre><code>def label_connected_component_subgraphs(G):\n    # Your answer here\n    return G\n\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import label_connected_component_subgraphs\nG_labelled = label_connected_component_subgraphs(G)\n\n# UNCOMMENT TO SEE THE ANSWER\n# label_connected_component_subgraphs??\n</code></pre> <p>&gt; Now, draw a CircosPlot with the node order and colouring &gt; dictated by the <code>subgraph</code> key.</p> <pre><code>def plot_cc_subgraph(G):\n    # Your answer here\n    pass\n\n\n# COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER\nfrom nams.solutions.structures import plot_cc_subgraph\nfrom nxviz import annotate\n\nplot_cc_subgraph(G_labelled)\nannotate.circos_group(G_labelled, group_by=\"subgraph\")\n</code></pre> <p>Using an arc plot will also clearly illuminate for us that there are no inter-group connections.</p> <pre><code>nv.arc(G_labelled, group_by=\"subgraph\", node_color_by=\"subgraph\")\nannotate.arc_group(G_labelled, group_by=\"subgraph\", rotation=0)\n</code></pre> <p>Voila! It looks quite clear that there are indeed four disjoint group of physicians.</p> <pre><code>from nams.solutions import structures\nimport inspect\n\nprint(inspect.getsource(structures))\n</code></pre> <pre>\n<code>\"\"\"Solutions to Structures chapter.\"\"\"\n\nfrom itertools import combinations\n\nimport networkx as nx\nfrom nxviz import circos\nfrom nams.functions import render_html\n\n\ndef triangle_finding_strategies():\n    \"\"\"\n    How to find triangles.\n    \"\"\"\n    ans = \"\"\"\nOne way would be to take one node, and look at its neighbors.\nIf its neighbors are also connected to one another,\nthen we have found a triangle.\n\nAnother way would be to start at a given node,\nand walk out two nodes.\nIf the starting node is the neighbor of the node two hops away,\nthen the path we traced traces out the nodes in a triangle.\n\"\"\"\n    return render_html(ans)\n\n\ndef in_triangle(G, node):\n    \"\"\"\n    Return whether a given node is present in a triangle relationship.\n    \"\"\"\n    for nbr1, nbr2 in combinations(G.neighbors(node), 2):\n        if G.has_edge(nbr1, nbr2):\n            return True\n    return False\n\n\ndef get_triangle_neighbors(G, node) -&gt; set:\n    \"\"\"\n    Return neighbors involved in triangle relationship with node.\n    \"\"\"\n    neighbors1 = set(G.neighbors(node))\n    triangle_nodes = set()\n    for nbr1, nbr2 in combinations(neighbors1, 2):\n        if G.has_edge(nbr1, nbr2):\n            triangle_nodes.add(nbr1)\n            triangle_nodes.add(nbr2)\n    return triangle_nodes\n\n\ndef plot_triangle_relations(G, node):\n    \"\"\"\n    Plot all triangle relationships for a given node.\n    \"\"\"\n    triangle_nbrs = get_triangle_neighbors(G, node)\n    triangle_nbrs.add(node)\n    nx.draw(G.subgraph(triangle_nbrs), with_labels=True)\n\n\ndef triadic_closure_algorithm():\n    \"\"\"\n    How to do triadic closure.\n    \"\"\"\n    ans = \"\"\"\nI would suggest the following strategy:\n\n1. Pick a node\n1. For every pair of neighbors:\n    1. If neighbors are not connected,\n    then this is a potential triangle to close.\n\nThis strategy gives you potential triadic closures\ngiven a \"center\" node `n`.\n\nThe other way is to trace out a path two degrees out\nand ask whether the terminal node is a neighbor\nof the starting node.\nIf not, then we have another triadic closure to make.\n\"\"\"\n    return render_html(ans)\n\n\ndef get_open_triangles_neighbors(G, node) -&gt; set:\n    \"\"\"\n    Return neighbors involved in open triangle relationships with a node.\n    \"\"\"\n    open_triangle_nodes = set()\n    neighbors = list(G.neighbors(node))\n\n    for n1, n2 in combinations(neighbors, 2):\n        if not G.has_edge(n1, n2):\n            open_triangle_nodes.add(n1)\n            open_triangle_nodes.add(n2)\n\n    return open_triangle_nodes\n\n\ndef plot_open_triangle_relations(G, node):\n    \"\"\"\n    Plot open triangle relationships for a given node.\n    \"\"\"\n    open_triangle_nbrs = get_open_triangles_neighbors(G, node)\n    open_triangle_nbrs.add(node)\n    nx.draw(G.subgraph(open_triangle_nbrs), with_labels=True)\n\n\ndef simplest_clique():\n    \"\"\"\n    Answer to \"what is the simplest clique\".\n    \"\"\"\n    return render_html(\"The simplest clique is an edge.\")\n\n\ndef size_k_maximal_cliques(G, k):\n    \"\"\"\n    Return all size-k maximal cliques.\n    \"\"\"\n    for clique in nx.find_cliques(G):\n        if len(clique) == k:\n            yield clique\n\n\ndef find_k_cliques(G, k):\n    \"\"\"\n    Find all cliques of size k.\n    \"\"\"\n    for clique in nx.find_cliques(G):\n        if len(clique) &gt;= k:\n            for nodeset in combinations(clique, k):\n                yield nodeset\n\n\ndef visual_insights():\n    \"\"\"\n    Answer to visual insights exercise.\n    \"\"\"\n    ans = \"\"\"\nWe might hypothesize that there are 3,\nmaybe 4 different \"communities\" of nodes\nthat are completely disjoint with one another,\ni.e. there is no path between them.\n\"\"\"\n    print(ans)\n\n\ndef label_connected_component_subgraphs(G):\n    \"\"\"Label all connected component subgraphs.\"\"\"\n    G = G.copy()\n    for i, nodeset in enumerate(nx.connected_components(G)):\n        for n in nodeset:\n            G.nodes[n][\"subgraph\"] = i\n    return G\n\n\ndef plot_cc_subgraph(G):\n    \"\"\"Plot all connected component subgraphs.\"\"\"\n    c = circos(G, node_color_by=\"subgraph\", group_by=\"subgraph\")\n\n</code>\n</pre>"},{"location":"02-algorithms/03-structures/#introduction","title":"Introduction","text":""},{"location":"02-algorithms/03-structures/#triangles","title":"Triangles","text":"<p>The first structure that we are going to learn about is triangles. Triangles are super interesting! They are what one might consider to be \"the simplest complex structure\" in a graph. Triangles can also have semantically-rich meaning depending on the application. To borrow a bad example, love triangles in social networks are generally frowned upon, while on the other hand, when we connect two people that we know together, we instead complete a triangle.</p>"},{"location":"02-algorithms/03-structures/#load-data","title":"Load Data","text":"<p>To learn about triangles, we are going to leverage a physician trust network. Here's the data description:</p> <p>&gt; This directed network captures innovation spread among 246 physicians  &gt; for towns in Illinois, Peoria, Bloomington, Quincy and Galesburg. &gt; The data was collected in 1966. &gt; A node represents a physician and an edge between two physicians &gt; shows that the left physician told that the right physician is his friend &gt; or that he turns to the right physician if he needs advice &gt; or is interested in a discussion. &gt; There always only exists one edge between two nodes &gt; even if more than one of the listed conditions are true.</p>"},{"location":"02-algorithms/03-structures/#exercise-finding-triangles-in-a-graph","title":"Exercise: Finding triangles in a graph","text":"<p>This exercise is going to flex your ability to \"think on a graph\", just as you did in the previous chapters.</p> <p>&gt; Leveraging what you know, can you think of a few strategies &gt; to find triangles in a graph?</p>"},{"location":"02-algorithms/03-structures/#exercise-identify-whether-a-node-is-in-a-triangle-relationship-or-not","title":"Exercise: Identify whether a node is in a triangle relationship or not","text":"<p>Let's now get down to implementing this next piece of code.</p> <p>&gt; Write a function that identifies whether a node is or is not in a triangle relationship. &gt; It should take in a graph <code>G</code> and a node <code>n</code>, &gt; and return a boolean True if the node <code>n</code> is in any triangle relationship &gt; and boolean False if the node <code>n</code> is not in any triangle relationship.</p> <p>A hint that may help you:</p> <p>&gt; Every graph object <code>G</code> has a <code>G.has_edge(n1, n2)</code> method that you can use to identify whether a graph has an edge between <code>n1</code> and <code>n2</code>.</p> <p>Also:</p> <p>&gt; <code>itertools.combinations</code> lets you iterate over every K-combination of items in an iterable.</p>"},{"location":"02-algorithms/03-structures/#exercise-extract-triangles-for-plotting","title":"Exercise: Extract triangles for plotting","text":"<p>We're going to leverage another piece of knowledge that you already have: the ability to extract subgraphs. We'll be plotting all of the triangles that a node is involved in.</p> <p>&gt; Given a node, write a function that extracts out &gt; all of the neighbors that it is in a triangle relationship with. &gt; Then, in a new function, &gt; implement code that plots only the subgraph &gt; that contains those nodes.</p>"},{"location":"02-algorithms/03-structures/#triadic-closure","title":"Triadic Closure","text":"<p>In professional circles, making connections between two people is one of the most valuable things you can do professionally. What you do in that moment is what we would call triadic closure. Algorithmically, we can do the same thing if we maintain a graph of connections!</p> <p>Essentially, what we are looking for are \"open\" or \"unfinished\" triangles\".</p> <p>In this section, we'll try our hand at implementing a rudimentary triadic closure system.</p>"},{"location":"02-algorithms/03-structures/#exercise-design-the-algorithm","title":"Exercise: Design the algorithm","text":"<p>&gt; What graph logic would you use to identify triadic closure opportunities? &gt; Try writing out your general strategy, or discuss it with someone.</p>"},{"location":"02-algorithms/03-structures/#exercise-implement-triadic-closure","title":"Exercise: Implement triadic closure.","text":"<p>Now, try your hand at implementing triadic closure.</p> <p>&gt; Write a function that takes in a graph <code>G</code> and a node <code>n</code>, &gt; and returns all of the neighbors that are potential triadic closures &gt; with <code>n</code> being the center node.</p>"},{"location":"02-algorithms/03-structures/#exercise-plot-the-open-triangles","title":"Exercise: Plot the open triangles","text":"<p>&gt; Now, write a function that takes in a graph <code>G</code> and a node <code>n</code>, &gt; and plots out that node <code>n</code> and all of the neighbors &gt; that it could help close triangles with.</p>"},{"location":"02-algorithms/03-structures/#cliques","title":"Cliques","text":"<p>Triangles are interesting in a graph theoretic setting because triangles are the simplest complex clique that exist.</p> <p>But wait! What is the definition of a \"clique\"?</p> <p>&gt; A \"clique\" is a set of nodes in a graph &gt; that are fully connected with one another &gt; by edges between them.</p>"},{"location":"02-algorithms/03-structures/#exercise-simplest-cliques","title":"Exercise: Simplest cliques","text":"<p>Given this definition, what is the simplest \"clique\" possible?</p>"},{"location":"02-algorithms/03-structures/#kk-cliques","title":"k-Cliques","text":"<p>Cliques are identified by their size k, which is the number of nodes that are present in the clique.</p> <p>A triangle is what we would consider to be a k-clique where k=3.</p> <p>A square with cross-diagonal connections is what we would consider to be a k-clique where k=4.</p> <p>By now, you should get the gist of the idea.</p>"},{"location":"02-algorithms/03-structures/#maximal-cliques","title":"Maximal Cliques","text":"<p>Related to this idea of a k-clique is another idea called \"maximal cliques\".</p> <p>Maximal cliques are defined as follows:</p> <p>&gt; A maximal clique is a subgraph of nodes in a graph &gt;  &gt; 1. to which no other node can be added to it and  &gt; 2. still remain a clique.</p> <p>NetworkX provides a way to find all maximal cliques:</p>"},{"location":"02-algorithms/03-structures/#exercise-finding-sized-kk-maximal-cliques","title":"Exercise: finding sized-k maximal cliques","text":"<p>&gt; Write a generator function that yields all maximal cliques of size k.</p> <p>I'm requesting a generator as a matter of good practice; you never know when the list you return might explode in memory consumption, so generators are a cheap and easy way to reduce memory usage.</p>"},{"location":"02-algorithms/03-structures/#clique-decomposition","title":"Clique Decomposition","text":"<p>One super neat property of cliques is that every clique of size k can be decomposed to the set of cliques of size k-1.</p> <p>Does this make sense to you? If not, think about triangles (3-cliques). They can be decomposed to three edges (2-cliques).</p> <p>Think again about 4-cliques. Housed within 4-cliques are four 3-cliques. Draw it out if you're still not convinced!</p>"},{"location":"02-algorithms/03-structures/#exercise-finding-all-kk-cliques-in-a-graph","title":"Exercise: finding all k-cliques in a graph","text":"<p>&gt; Knowing this property of k-cliques, &gt; write a generator function that yields all k-cliques in a graph, &gt; leveraging the <code>nx.find_cliques(G)</code> function.</p> <p>Some hints to help you along:</p> <p>&gt; If a k-clique can be decomposed to its k-1 cliques, &gt; it follows that the k-1 cliques can be decomposed into k-2 cliques, &gt; and so on until you hit 2-cliques. &gt; This implies that all cliques of size k &gt; house cliques of size n &lt; k, where n &gt;= 2.</p>"},{"location":"02-algorithms/03-structures/#connected-components","title":"Connected Components","text":"<p>Now that we've explored a lot around cliques, we're now going to explore this idea of \"connected components\". To do so, I am going to have you draw the graph that we are working with.</p>"},{"location":"02-algorithms/03-structures/#exercise-visual-insights","title":"Exercise: Visual insights","text":"<p>From this rendering of the CircosPlot, what visual insights do you have about the structure of the graph?</p>"},{"location":"02-algorithms/03-structures/#defining-connected-components","title":"Defining connected components","text":"<p>From Wikipedia:</p> <p>&gt; In graph theory, a connected component (or just component) of an undirected graph is a subgraph in which any two vertices are connected to each other by paths, and which is connected to no additional vertices in the supergraph.</p> <p>NetworkX provides a function to let us find all of the connected components:</p>"},{"location":"02-algorithms/03-structures/#exercise-visualizing-connected-component-subgraphs","title":"Exercise: visualizing connected component subgraphs","text":"<p>In this exercise, we're going to draw a circos plot of the graph,  but colour and order the nodes by their connected component subgraph.</p> <p>Recall Circos API:</p> <pre><code>c = CircosPlot(G, node_order='node_attribute', node_color='node_attribute')\nc.draw()\nplt.show()  # or plt.savefig(...)\n</code></pre> <p>Follow the steps along here to accomplish this.</p> <p>&gt; Firstly, label the nodes with a unique identifier for connected component subgraph &gt; that it resides in. &gt; Use <code>subgraph</code> to store this piece of metadata.</p>"},{"location":"02-algorithms/03-structures/#solutions","title":"Solutions","text":""},{"location":"03-practical/01-io/","title":"Chapter 7: Graph I/O","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"3sJnTpeFXZ4\", width=\"100%\")\n</code></pre> <p>In order to get you familiar with graph ideas, I have deliberately chosen to steer away from the more pedantic matters of loading graph data to and from disk. That said, the following scenario will eventually happen, where a graph dataset lands on your lap, and you'll need to load it in memory  and start analyzing it.</p> <p>Thus, we're going to go through graph I/O, specifically the APIs on how to convert graph data that comes to you into that magical NetworkX object <code>G</code>.</p> <p>Let's get going!</p> <pre><code>from pyprojroot import here\n</code></pre> <p>Firstly, we need to unzip the dataset:</p> <pre><code>import zipfile\nimport os\nfrom nams.load_data import datasets\n\n# This block of code checks to make sure that a particular directory is present.\nif \"divvy_2013\" not in os.listdir(datasets):\n    print(\"Unzipping the divvy_2013.zip file in the datasets folder.\")\n    with zipfile.ZipFile(datasets / \"divvy_2013.zip\", \"r\") as zip_ref:\n        zip_ref.extractall(datasets)\n</code></pre> <p>Now, let's load in both tables.</p> <p>First is the <code>stations</code> table:</p> <pre><code>import pandas as pd\n\nstations = pd.read_csv(\n    datasets / \"divvy_2013/Divvy_Stations_2013.csv\",\n    parse_dates=[\"online date\"],\n    encoding=\"utf-8\",\n)\nstations.head()\n</code></pre> id name latitude longitude dpcapacity landmark online date 0 5 State St &amp; Harrison St 41.873958 -87.627739 19 30 2013-06-28 1 13 Wilton Ave &amp; Diversey Pkwy 41.932500 -87.652681 19 66 2013-06-28 2 14 Morgan St &amp; 18th St 41.858086 -87.651073 15 163 2013-06-28 3 15 Racine Ave &amp; 18th St 41.858181 -87.656487 15 164 2013-06-28 4 16 Wood St &amp; North Ave 41.910329 -87.672516 15 223 2013-08-12 <pre><code>stations.describe()\n</code></pre> id latitude longitude dpcapacity landmark online date count 300.000000 300.000000 300.000000 300.000000 300.000000 300 mean 189.063333 41.896276 -87.648220 16.800000 192.013333 2013-08-13 22:48:00 min 5.000000 41.788746 -87.707857 11.000000 1.000000 2013-06-28 00:00:00 25% 108.750000 41.871814 -87.665824 15.000000 83.750000 2013-07-23 00:00:00 50% 196.500000 41.894611 -87.648626 15.000000 184.500000 2013-08-15 00:00:00 75% 276.250000 41.926397 -87.631810 19.000000 288.250000 2013-09-13 00:00:00 max 351.000000 41.978353 -87.580715 47.000000 440.000000 2013-10-29 00:00:00 std 99.484487 0.040952 0.023001 4.673987 120.534789 NaN <p>Now, let's load in the <code>trips</code> table.</p> <pre><code>trips = pd.read_csv(\n    datasets / \"divvy_2013/Divvy_Trips_2013.csv\", parse_dates=[\"starttime\", \"stoptime\"]\n)\ntrips.head()\n</code></pre> trip_id starttime stoptime bikeid tripduration from_station_id from_station_name to_station_id to_station_name usertype gender birthday 0 4118 2013-06-27 12:11:00 2013-06-27 12:16:00 480 316 85 Michigan Ave &amp; Oak St 28 Larrabee St &amp; Menomonee St Customer NaN NaN 1 4275 2013-06-27 14:44:00 2013-06-27 14:45:00 77 64 32 Racine Ave &amp; Congress Pkwy 32 Racine Ave &amp; Congress Pkwy Customer NaN NaN 2 4291 2013-06-27 14:58:00 2013-06-27 15:05:00 77 433 32 Racine Ave &amp; Congress Pkwy 19 Loomis St &amp; Taylor St Customer NaN NaN 3 4316 2013-06-27 15:06:00 2013-06-27 15:09:00 77 123 19 Loomis St &amp; Taylor St 19 Loomis St &amp; Taylor St Customer NaN NaN 4 4342 2013-06-27 15:13:00 2013-06-27 15:27:00 77 852 19 Loomis St &amp; Taylor St 55 Halsted St &amp; James M Rochford St Customer NaN NaN <pre><code>import janitor\n\ntrips_summary = (\n    trips.groupby([\"from_station_id\", \"to_station_id\"])\n    .count()\n    .reset_index()\n    .select_columns([\"from_station_id\", \"to_station_id\", \"trip_id\"])\n    .rename_column(\"trip_id\", \"num_trips\")\n)\n</code></pre> <pre><code>trips_summary.head()\n</code></pre> from_station_id to_station_id num_trips 0 5 5 232 1 5 13 1 2 5 14 15 3 5 15 9 4 5 16 4 <pre><code>import networkx as nx\n\nG = nx.from_pandas_edgelist(\n    df=trips_summary,\n    source=\"from_station_id\",\n    target=\"to_station_id\",\n    edge_attr=[\"num_trips\"],\n    create_using=nx.DiGraph,\n)\n</code></pre> <pre><code>print(G)\n</code></pre> <pre>\n<code>DiGraph with 300 nodes and 44422 edges\n</code>\n</pre> <p>You'll notice that the edge metadata have been added correctly: we have recorded in there the number of trips between stations.</p> <pre><code>list(G.edges(data=True))[0:5]\n</code></pre> <pre>\n<code>[(5, 5, {'num_trips': 232}),\n (5, 13, {'num_trips': 1}),\n (5, 14, {'num_trips': 15}),\n (5, 15, {'num_trips': 9}),\n (5, 16, {'num_trips': 4})]</code>\n</pre> <p>However, the node metadata is not present:</p> <pre><code>list(G.nodes(data=True))[0:5]\n</code></pre> <pre>\n<code>[(5, {}), (13, {}), (14, {}), (15, {}), (16, {})]</code>\n</pre> <pre><code>stations.head()\n</code></pre> id name latitude longitude dpcapacity landmark online date 0 5 State St &amp; Harrison St 41.873958 -87.627739 19 30 2013-06-28 1 13 Wilton Ave &amp; Diversey Pkwy 41.932500 -87.652681 19 66 2013-06-28 2 14 Morgan St &amp; 18th St 41.858086 -87.651073 15 163 2013-06-28 3 15 Racine Ave &amp; 18th St 41.858181 -87.656487 15 164 2013-06-28 4 16 Wood St &amp; North Ave 41.910329 -87.672516 15 223 2013-08-12 <p>The <code>id</code> column gives us the node ID in the graph, so if we set <code>id</code> to be the index, if we then also loop over each row, we can treat the rest of the columns as dictionary keys and values as dictionary values, and add the information into the graph.</p> <p>Let's see this in action.</p> <pre><code>for node, metadata in stations.set_index(\"id\").iterrows():\n    for key, val in metadata.items():\n        G.nodes[node][key] = val\n</code></pre> <p>Now, our node metadata should be populated.</p> <pre><code>list(G.nodes(data=True))[0:5]\n</code></pre> <pre>\n<code>[(5,\n  {'name': 'State St &amp; Harrison St',\n   'latitude': 41.87395806,\n   'longitude': -87.62773949,\n   'dpcapacity': 19,\n   'landmark': 30,\n   'online date': Timestamp('2013-06-28 00:00:00')}),\n (13,\n  {'name': 'Wilton Ave &amp; Diversey Pkwy',\n   'latitude': 41.93250008,\n   'longitude': -87.65268082,\n   'dpcapacity': 19,\n   'landmark': 66,\n   'online date': Timestamp('2013-06-28 00:00:00')}),\n (14,\n  {'name': 'Morgan St &amp; 18th St',\n   'latitude': 41.858086,\n   'longitude': -87.651073,\n   'dpcapacity': 15,\n   'landmark': 163,\n   'online date': Timestamp('2013-06-28 00:00:00')}),\n (15,\n  {'name': 'Racine Ave &amp; 18th St',\n   'latitude': 41.85818061,\n   'longitude': -87.65648665,\n   'dpcapacity': 15,\n   'landmark': 164,\n   'online date': Timestamp('2013-06-28 00:00:00')}),\n (16,\n  {'name': 'Wood St &amp; North Ave',\n   'latitude': 41.910329,\n   'longitude': -87.672516,\n   'dpcapacity': 15,\n   'landmark': 223,\n   'online date': Timestamp('2013-08-12 00:00:00')})]</code>\n</pre> <p>In <code>nxviz</code>, a <code>GeoPlot</code> object is available that allows you to quickly visualize a graph that has geographic data. However, being <code>matplotlib</code>-based, it is going to be quickly overwhelmed by the sheer number of edges.</p> <p>As such, we are going to first filter the edges.</p> <pre><code>def filter_graph(G, minimum_num_trips):\n    \"\"\"\n    Filter the graph such that\n    only edges that have minimum_num_trips or more\n    are present.\n    \"\"\"\n    G_filtered = G.____()\n    for _, _, _ in G._____(data=____):\n        if d[___________] &amp;lt; ___:\n            G_________.___________(_, _)\n    return G_filtered\n\n\nfrom nams.solutions.io import filter_graph\n\nG_filtered = filter_graph(G, 50)\n</code></pre> <p>A note on geospatial visualizations:</p> <p>&gt; As the creator of <code>nxviz</code>, &gt; I would recommend using proper geospatial packages &gt; to build custom geospatial graph viz, &gt; such as <code>pysal</code>.) &gt;  &gt; That said, <code>nxviz</code> can probably do what you need &gt; for a quick-and-dirty view of the data.</p> <pre><code>import nxviz as nv\n\nc = nv.geo(G_filtered, node_color_by=\"dpcapacity\")\n</code></pre> <pre>\n<code>/home/runner/work/Network-Analysis-Made-Simple/Network-Analysis-Made-Simple/.pixi/envs/default/lib/python3.11/site-packages/nxviz/encodings.py:42: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  cmap = get_cmap(\"viridis\")\n</code>\n</pre> <p>Does that look familiar to you? Looks quite a bit like Chicago, I'd say :)</p> <p>Jesting aside, this visualization does help illustrate that the majority of trips occur between stations that are near the city center.</p> <pre><code>import pickle\n\nwith open(\"/tmp/divvy.pkl\", \"wb\") as f:\n    pickle.dump(G, f, pickle.HIGHEST_PROTOCOL)\n</code></pre> <p>And just to show that it can be loaded back into memory:</p> <pre><code>with open(\"/tmp/divvy.pkl\", \"rb\") as f:\n    G_loaded = pickle.load(f)\n</code></pre> <pre><code>def test_graph_integrity(G):\n    \"\"\"Test integrity of raw Divvy graph.\"\"\"\n    # Your solution here\n    pass\n\n\nfrom nams.solutions.io import test_graph_integrity\n\ntest_graph_integrity(G)\n</code></pre> <pre><code>from nams.solutions import io\nimport inspect\n\nprint(inspect.getsource(io))\n</code></pre> <pre>\n<code>\"\"\"Solutions to I/O chapter\"\"\"\n\n\ndef filter_graph(G, minimum_num_trips):\n    \"\"\"\n    Filter the graph such that\n    only edges that have minimum_num_trips or more\n    are present.\n    \"\"\"\n    G_filtered = G.copy()\n    for u, v, d in G.edges(data=True):\n        if d[\"num_trips\"] &lt; minimum_num_trips:\n            G_filtered.remove_edge(u, v)\n    return G_filtered\n\n\ndef test_graph_integrity(G):\n    \"\"\"Test integrity of raw Divvy graph.\"\"\"\n    assert len(G.nodes()) == 300\n    assert len(G.edges()) == 44422\n\n</code>\n</pre>"},{"location":"03-practical/01-io/#introduction","title":"Introduction","text":""},{"location":"03-practical/01-io/#graph-data-as-tables","title":"Graph Data as Tables","text":"<p>Let's recall what we've learned in the introductory chapters. Graphs can be represented using two sets:</p> <ul> <li>Node set</li> <li>Edge set</li> </ul>"},{"location":"03-practical/01-io/#node-set-as-tables","title":"Node set as tables","text":"<p>Let's say we had a graph with 3 nodes in it: <code>A, B, C</code>. We could represent it in plain text, computer-readable format:</p> <pre><code>A\nB\nC\n</code></pre> <p>Suppose the nodes also had metadata. Then, we could tag on metadata as well:</p> <pre><code>A, circle, 5\nB, circle, 7\nC, square, 9\n</code></pre> <p>Does this look familiar to you? Yes, node sets can be stored in CSV format, with one of the columns being node ID, and the rest of the columns being metadata.</p>"},{"location":"03-practical/01-io/#edge-set-as-tables","title":"Edge set as tables","text":"<p>If, between the nodes, we had 4 edges (this is a directed graph), we can also represent those edges in plain text, computer-readable format:</p> <pre><code>A, C\nB, C\nA, B\nC, A\n</code></pre> <p>And let's say we also had other metadata, we can represent it in the same CSV format:</p> <pre><code>A, C, red\nB, C, orange\nA, B, yellow\nC, A, green\n</code></pre> <p>If you've been in the data world for a while, this should not look foreign to you. Yes, edge sets can be stored in CSV format too! Two of the columns represent the nodes involved in an edge, and the rest of the columns represent the metadata.</p>"},{"location":"03-practical/01-io/#combined-representation","title":"Combined Representation","text":"<p>In fact, one might also choose to combine the node set and edge set tables together in a merged format:</p> <pre><code>n1, n2, colour, shape1, num1, shape2, num2\nA,  C,  red,    circle, 5,    square, 9\nB,  C,  orange, circle, 7,    square, 9\nA,  B,  yellow, circle, 5,    circle, 7\nC,  A,  green,  square, 9,    circle, 5\n</code></pre> <p>In this chapter, the datasets that we will be looking at are going to be formatted in both ways. Let's get going.</p>"},{"location":"03-practical/01-io/#dataset","title":"Dataset","text":"<p>We will be working with the Divvy bike sharing dataset.</p> <p>&gt; Divvy is a bike sharing service in Chicago. &gt; Since 2013, Divvy has released their bike sharing dataset to the public. &gt; The 2013 dataset is comprised of two files:  &gt; - <code>Divvy_Stations_2013.csv</code>, containing the stations in the system, and &gt; - <code>DivvyTrips_2013.csv</code>, containing the trips.</p> <p>Let's dig into the data!</p>"},{"location":"03-practical/01-io/#graph-model","title":"Graph Model","text":"<p>Given the data, if we wished to use a graph as a data model for the number of trips between stations, then naturally, nodes would be the stations, and edges would be trips between them.</p> <p>This graph would be directed, as one could have more trips from station A to B and less in the reverse.</p> <p>With this definition, we can begin graph construction!</p>"},{"location":"03-practical/01-io/#create-networkx-graph-from-pandas-edgelist","title":"Create NetworkX graph from pandas edgelist","text":"<p>NetworkX provides an extremely convenient way to load data from a pandas DataFrame:</p>"},{"location":"03-practical/01-io/#inspect-the-graph","title":"Inspect the graph","text":"<p>Once the graph is in memory, we can inspect it to get out summary graph statistics.</p>"},{"location":"03-practical/01-io/#annotate-node-metadata","title":"Annotate node metadata","text":"<p>We have rich station data on hand, such as the longitude and latitude of each station, and it would be a pity to discard it, especially when we can potentially use it as part of the analysis or for visualization purposes. Let's see how we can add this information in.</p> <p>Firstly, recall what the <code>stations</code> dataframe looked like:</p>"},{"location":"03-practical/01-io/#exercise-filter-graph-edges","title":"Exercise: Filter graph edges","text":"<p>&gt; Leveraging what you know about how to manipulate graphs, &gt; now try filtering edges. &gt;</p> <p>Hint: NetworkX graph objects can be deep-copied using <code>G.copy()</code>:</p> <pre><code>G_copy = G.copy()\n</code></pre> <p>Hint: NetworkX graph objects also let you remove edges:</p> <pre><code>G.remove_edge(node1, node2)  # does not return anything\n</code></pre>"},{"location":"03-practical/01-io/#visualize-using-geoplot","title":"Visualize using GeoPlot","text":"<p><code>nxviz</code> provides a GeoPlot object that lets you quickly visualize geospatial graph data.</p>"},{"location":"03-practical/01-io/#pickling-graphs","title":"Pickling Graphs","text":"<p>Since NetworkX graphs are Python objects, the canonical way to save them is by pickling them.</p> <p>Here's an example in action:</p>"},{"location":"03-practical/01-io/#exercise-checking-graph-integrity","title":"Exercise: checking graph integrity","text":"<p>If you get a graph dataset as a pickle, you should always check it against reference properties to make sure of its data integrity.</p> <p>&gt; Write a function that tests that the graph &gt; has the correct number of nodes and edges inside it.</p>"},{"location":"03-practical/01-io/#other-text-formats","title":"Other text formats","text":"<p>CSV files and <code>pandas</code> DataFrames give us a convenient way to store graph data, and if possible, do insist with your data collaborators that they provide you with graph data that are in this format. If they don't, however, no sweat! After all, Python is super versatile.</p> <p>In this ebook, we have loaded data in from non-CSV sources, sometimes by parsing text files raw, sometimes by treating special characters as delimiters in a CSV-like file, and sometimes by resorting to parsing JSON.</p> <p>You can see other examples of how we load data by browsing through the source file of <code>load_data.py</code> and studying how we construct graph objects.</p>"},{"location":"03-practical/01-io/#solutions","title":"Solutions","text":"<p>The solutions to this chapter's exercises are below</p>"},{"location":"03-practical/02-testing/","title":"Chapter 8: Testing","text":"<pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"SdbKs-crm-g\", width=\"100%\")\n</code></pre> <p>By this point in the book, you should have observed that we have written a number of tests for our data.</p> <p>An apology to geospatial experts:  I genuinely don't know the bounding box lat/lon coordinates of Chicago, so if you know those coordinates, please reach out so I can update the test.</p>"},{"location":"03-practical/02-testing/#introduction","title":"Introduction","text":""},{"location":"03-practical/02-testing/#why-test","title":"Why test?","text":""},{"location":"03-practical/02-testing/#if-you-like-it-put-a-ring-on-it","title":"If you like it, put a ring on it...","text":"<p>...and if you rely on it, test it.</p> <p>I am personally a proponent of writing tests for our data because as data scientists, the fields of our data, and their correct values, form the \"data programming interface\" (DPI) much like function signatures form the \"application programming interface\" (API). Since we test the APIs that we rely on, we probably should test the DPIs that we rely on too.</p>"},{"location":"03-practical/02-testing/#what-to-test","title":"What to test","text":"<p>When thinking about what part of the data to test, it can be confusing. After all, data are seemingly generated from random processes (my Bayesian foxtail has been revealed), and it seems difficult to test random processes.</p> <p>That said, from my experience handling data, I can suggest a few principles.</p>"},{"location":"03-practical/02-testing/#test-invariants","title":"Test invariants","text":"<p>Firstly, we test invariant properties of the data. Put in plain language, things we know ought to be true.</p> <p>Using the Divvy bike dataset example, we know that every node ought to have a station name. Thus, the minimum that we can test is that the <code>station_name</code> attribute is present on every node. As an example:</p> <pre><code>def test_divvy_nodes(G):\n    \"\"\"Test node metadata on Divvy dataset.\"\"\"\n    for n, d in G.nodes(data=True):\n        assert \"station_name\" in d.keys()\n</code></pre>"},{"location":"03-practical/02-testing/#test-nullity","title":"Test nullity","text":"<p>Secondly, we can test that values that ought not to be null should not be null.</p> <p>Using the Divvy bike dataset example again, if we also know that the station name cannot be null or an empty string, then we can bake that into the test.</p> <pre><code>def test_divvy_nodes(G):\n    \"\"\"Test node metadata on Divvy dataset.\"\"\"\n    for n, d in G.nodes(data=True):\n        assert \"station_name\" in d.keys()\n        assert bool(d[\"station_name\"])\n</code></pre>"},{"location":"03-practical/02-testing/#test-boundaries","title":"Test boundaries","text":"<p>We can also test boundary values. For example, within the city of Chicago, we know that latitude and longitude values ought to be within the vicinity of <code>41.85003, -87.65005</code>. If we get data values that are, say, outside the range of <code>[41, 42]; [-88, -87]</code>, then we know that we have data issues as well.</p> <p>Here's an example:</p> <pre><code>def test_divvy_nodes(G):\n    \"\"\"Test node metadata on Divvy dataset.\"\"\"\n    for n, d in G.nodes(data=True):\n        # Test for station names.\n        assert \"station_name\" in d.keys()\n        assert bool(d[\"station_name\"])\n\n        # Test for longitude/latitude\n        assert d[\"latitude\"] &amp;gt;= 41 and d[\"latitude\"] &amp;lt;= 42\n        assert d[\"longitude\"] &amp;gt;= -88 and d[\"longitude\"] &amp;lt;= -87\n</code></pre>"},{"location":"03-practical/02-testing/#continuous-data-testing","title":"Continuous data testing","text":"<p>The key idea with testing is to have tests that continuously run all the time in the background without you ever needing to intervene to kickstart it off. It's like having a bot in the background always running checks for you so you don't have to kickstart them.</p> <p>To do so, you should be equipped with a few tools. I won't go into them in-depth here, as I will be writing a \"continuous data testing\" essay in the near future. That said, here is the gist.</p> <p>Firstly, use <code>pytest</code> to get set up with testing. You essentially write a <code>test_something.py</code> file in which you write your test suite, and your test functions are all nothinng more than simple functions.</p> <pre><code># test_data.py\ndef test_divvy_nodes(G):\n    \"\"\"Test node metadata on Divvy dataset.\"\"\"\n    for n, d in G.nodes(data=True):\n        # Test for station names.\n        assert \"station_name\" in d.keys()\n        assert bool(d[\"station_name\"])\n\n        # Test for longitude/latitude\n        assert d[\"latitude\"] &amp;gt;= 41 and d[\"latitude\"] &amp;lt;= 42\n        assert d[\"longitude\"] &amp;gt;= -88 and d[\"longitude\"] &amp;lt;= -87\n</code></pre> <p>At the command line, if you ran <code>pytest</code>, it will automatically discover all functions prefixed with <code>test_</code> in all <code>.py</code> files underneath the current working directory.</p> <p>Secondly, set up a continuous pipelining system to continuously run data tests. For example, you can set up Jenkins, Travis, Azure Pipelines, Prefect, and more, depending on what your organization has bought into.</p> <p>Sometimes data tests take longer than software tests, especially if you are pulling dumps from a database, so you might want to run this portion of tests in a separate pipeline instead.</p>"},{"location":"03-practical/02-testing/#further-reading","title":"Further reading","text":"<ul> <li>In my essays collection, I wrote about testing data.</li> <li>Itamar Turner-Trauring has written about keeping tests quick and speedy, which is extremely crucial to keeping yourself motivated to write tests.</li> </ul>"},{"location":"04-advanced/01-bipartite/","title":"Chapter 9: Bipartite Graphs","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"BYOK12I9vgI\", width=\"100%\")\n</code></pre> <p>In this chapter, we will look at bipartite graphs and their applications.</p> <pre><code>from nams import load_data as cf\n\nG = cf.load_crime_network()\nfor n, d in G.nodes(data=True):\n    G.nodes[n][\"degree\"] = G.degree(n)\n</code></pre> <p>If you inspect the nodes, you will see that they contain a special metadata keyword: <code>bipartite</code>. This is a special keyword that NetworkX can use  to identify nodes of a given partition.</p> <pre><code>import nxviz as nv\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(7, 7))\nnv.circos(\n    G,\n    sort_by=\"degree\",\n    group_by=\"bipartite\",\n    node_color_by=\"bipartite\",\n    node_enc_kwargs={\"size_scale\": 3},\n)\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <pre><code>import networkx as nx\n\n\ndef extract_partition_nodes(G: nx.Graph, partition: str):\n    nodeset = [_ for _, _ in _______ if ____________]\n    if _____________:\n        raise Exception(f\"No nodes exist in the partition {partition}!\")\n    return nodeset\n\n\nfrom nams.solutions.bipartite import extract_partition_nodes\n\n# Uncomment the next line to see the answer.\n# extract_partition_nodes??\n</code></pre> <pre><code>from nams.solutions.bipartite import (\n    draw_bipartite_graph_example,\n    bipartite_example_graph,\n)\nfrom nxviz import annotate\nimport matplotlib.pyplot as plt\n\nbG = bipartite_example_graph()\npG = nx.bipartite.projection.projected_graph(bG, \"abcd\")\nax = draw_bipartite_graph_example()\nplt.sca(ax[0])\nannotate.parallel_labels(bG, group_by=\"bipartite\")\nplt.sca(ax[1])\nannotate.arc_labels(pG)\n</code></pre> <p>As shown in the figure above, we start first with a bipartite graph with two node sets, the \"alphabet\" set and the \"numeric\" set. The projection of this bipartite graph onto the \"alphabet\" node set is a graph that is constructed such that it only contains the \"alphabet\" nodes, and edges join the \"alphabet\" nodes because they share a connection to a \"numeric\" node. The red edge on the right is basically the red path traced on the left.</p> <pre><code>from networkx.algorithms import bipartite\n\nbipartite.is_bipartite(G)\n</code></pre> <pre>\n<code>True</code>\n</pre> <p>Now that we've confirmed that the graph is indeed bipartite, we can use the NetworkX bipartite submodule functions to generate the bipartite projection onto one of the node partitions.</p> <p>First off, we need to extract nodes from a particular partition.</p> <pre><code>person_nodes = extract_partition_nodes(G, \"person\")\ncrime_nodes = extract_partition_nodes(G, \"crime\")\n</code></pre> <p>Next, we can compute the projection:</p> <pre><code>person_graph = bipartite.projected_graph(G, person_nodes)\ncrime_graph = bipartite.projected_graph(G, crime_nodes)\n</code></pre> <p>And with that, we have our projected graphs!</p> <p>Go ahead and inspect them:</p> <pre><code>list(person_graph.edges(data=True))[0:5]\n</code></pre> <pre>\n<code>[('p1', 'p336', {}),\n ('p1', 'p756', {}),\n ('p1', 'p93', {}),\n ('p1', 'p694', {}),\n ('p2', 'p660', {})]</code>\n</pre> <pre><code>list(crime_graph.edges(data=True))[0:5]\n</code></pre> <pre>\n<code>[('c1', 'c4', {}),\n ('c1', 'c2', {}),\n ('c1', 'c3', {}),\n ('c2', 'c4', {}),\n ('c2', 'c3', {})]</code>\n</pre> <p>Now, what is the interpretation of these projected graphs?</p> <ul> <li>For <code>person_graph</code>, we have found individuals who are linked by shared participation (whether witness or suspect) in a crime.</li> <li>For <code>crime_graph</code>, we have found crimes that are linked by shared involvement by people.</li> </ul> <p>Just by this graph, we already can find out pretty useful information. Let's use an exercise that leverages what you already know to extract useful information from the projected graph.</p> <pre><code>import pandas as pd\n\n\ndef find_most_similar_crimes(cG: nx.Graph):\n    \"\"\"\n    Find the crimes that are most similar to other crimes.\n    \"\"\"\n    dcs = ______________\n    return ___________________\n\n\nfrom nams.solutions.bipartite import find_most_similar_crimes\n\nfind_most_similar_crimes(crime_graph)\n</code></pre> <pre>\n<code>c110    0.136364\nc47     0.070909\nc23     0.070909\nc95     0.063636\nc14     0.061818\nc352    0.060000\nc432    0.060000\nc417    0.058182\nc525    0.058182\nc160    0.058182\ndtype: float64</code>\n</pre> <pre><code>def find_most_similar_people(pG: nx.Graph):\n    \"\"\"\n    Find the persons that are most similar to other persons.\n    \"\"\"\n    dcs = ______________\n    return ___________________\n\n\nfrom nams.solutions.bipartite import find_most_similar_people\n\nfind_most_similar_people(person_graph)\n</code></pre> <pre>\n<code>p425    0.061594\np2      0.057971\np356    0.053140\np56     0.039855\np695    0.039855\np497    0.036232\np715    0.035024\np10     0.033816\np815    0.032609\np74     0.030193\ndtype: float64</code>\n</pre> <pre><code>weighted_person_graph = bipartite.weighted_projected_graph(G, person_nodes)\nlist(weighted_person_graph.edges(data=True))[0:5]\n</code></pre> <pre>\n<code>[('p1', 'p336', {'weight': 1}),\n ('p1', 'p756', {'weight': 1}),\n ('p1', 'p93', {'weight': 1}),\n ('p1', 'p694', {'weight': 1}),\n ('p2', 'p660', {'weight': 1})]</code>\n</pre> <pre><code>list(G.neighbors(\"p1\"))\n</code></pre> <pre>\n<code>['c1', 'c2', 'c3', 'c4']</code>\n</pre> <pre><code>def find_connected_persons(G, person, crime):\n    # Step 0: Check that the given \"person\" and \"crime\" are connected.\n    if _____________________________:\n        raise ValueError(\n            f\"Graph does not have a connection between {person} and {crime}!\"\n        )\n\n    # Step 1: calculate weighted projection for person nodes.\n    person_nodes = ____________________________________\n    person_graph = bipartite.________________________(_, ____________)\n\n    # Step 2: Find neighbors of the given `person` node in projected graph.\n    candidate_neighbors = ___________________________________\n\n    # Step 3: Remove candidate neighbors from the set if they are implicated in the given crime.\n    for p in G.neighbors(crime):\n        if ________________________:\n            _____________________________\n\n    # Step 4: Rank-order the candidate neighbors by number of shared connections.\n    _________ = []\n    ## You might need a for-loop here\n    return pd.DataFrame(__________).sort_values(\"________\", ascending=False)\n\n\nfrom nams.solutions.bipartite import find_connected_persons\n\nfind_connected_persons(G, \"p2\", \"c10\")\n</code></pre> node weight 8 p67 4 24 p338 2 27 p356 2 37 p361 2 3 p211 1 1 p665 1 0 p660 1 6 p4 1 7 p287 1 9 p578 1 10 p495 1 11 p90 1 4 p309 1 5 p499 1 2 p286 1 14 p690 1 13 p186 1 12 p360 1 15 p661 1 19 p439 1 20 p820 1 17 p471 1 16 p781 1 22 p563 1 21 p304 1 25 p806 1 23 p587 1 26 p603 1 28 p498 1 29 p223 1 18 p300 1 30 p320 1 31 p39 1 33 p475 1 32 p782 1 34 p608 1 35 p401 1 36 p528 1 38 p48 1 39 p305 1 40 p449 1 41 p773 1 42 p710 1 43 p5 1 44 p716 1 45 p620 1 46 p768 1 <pre><code>from nams.solutions.bipartite import bipartite_degree_centrality_denominator\nfrom nams.functions import render_html\n\nrender_html(bipartite_degree_centrality_denominator())\n</code></pre> <p>The total number of neighbors that a node can possibly have is the number of nodes in the other partition. This comes naturally from the definition of a bipartite graph, where nodes can only be connected to nodes in the other partition.</p> <pre><code>def find_most_crime_person(G, person_nodes):\n    dcs = __________________________\n    return ___________________________\n\n\nfrom nams.solutions.bipartite import find_most_crime_person\n\nfind_most_crime_person(G, person_nodes)\n</code></pre> <pre>\n<code>'p815'</code>\n</pre> <pre><code>from nams.solutions import bipartite\nimport inspect\n\nprint(inspect.getsource(bipartite))\n</code></pre> <pre>\n<code>import networkx as nx\nimport pandas as pd\nfrom nams.functions import render_html\n\n\ndef extract_partition_nodes(G: nx.Graph, partition: str):\n    nodeset = [n for n, d in G.nodes(data=True) if d[\"bipartite\"] == partition]\n    if len(nodeset) == 0:\n        raise Exception(f\"No nodes exist in the partition {partition}!\")\n    return nodeset\n\n\ndef bipartite_example_graph():\n    bG = nx.Graph()\n    bG.add_nodes_from(\"abcd\", bipartite=\"letters\")\n    bG.add_nodes_from(range(1, 4), bipartite=\"numbers\")\n    bG.add_edges_from([(\"a\", 1), (\"b\", 1), (\"b\", 3), (\"c\", 2), (\"c\", 3), (\"d\", 1)])\n\n    return bG\n\n\ndef draw_bipartite_graph_example():\n    \"\"\"Draw an example bipartite graph and its corresponding projection.\"\"\"\n    import matplotlib.pyplot as plt\n    import nxviz as nv\n    from nxviz import annotate, plots, highlights\n\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n    plt.sca(ax[0])\n    bG = bipartite_example_graph()\n    nv.parallel(bG, group_by=\"bipartite\", node_color_by=\"bipartite\")\n    annotate.parallel_group(bG, group_by=\"bipartite\", y_offset=-0.5)\n    highlights.parallel_edge(bG, \"a\", 1, group_by=\"bipartite\")\n    highlights.parallel_edge(bG, \"b\", 1, group_by=\"bipartite\")\n\n    pG = nx.bipartite.projected_graph(bG, nodes=list(\"abcd\"))\n    plt.sca(ax[1])\n    nv.arc(pG)\n    highlights.arc_edge(pG, \"a\", \"b\")\n    return ax\n\n\ndef find_most_similar_crimes(cG: nx.Graph):\n    \"\"\"\n    Find the crimes that are most similar to other crimes.\n    \"\"\"\n    dcs = pd.Series(nx.degree_centrality(cG))\n    return dcs.sort_values(ascending=False).head(10)\n\n\ndef find_most_similar_people(pG: nx.Graph):\n    \"\"\"\n    Find the persons that are most similar to other persons.\n    \"\"\"\n    dcs = pd.Series(nx.degree_centrality(pG))\n    return dcs.sort_values(ascending=False).head(10)\n\n\ndef find_connected_persons(G, person, crime):\n    \"\"\"Answer to exercise on people implicated in crimes\"\"\"\n    # Step 0: Check that the given \"person\" and \"crime\" are connected.\n    if not G.has_edge(person, crime):\n        raise ValueError(\n            f\"Graph does not have a connection between {person} and {crime}!\"\n        )\n\n    # Step 1: calculate weighted projection for person nodes.\n    person_nodes = extract_partition_nodes(G, \"person\")\n    person_graph = nx.bipartite.weighted_projected_graph(G, person_nodes)\n\n    # Step 2: Find neighbors of the given `person` node in projected graph.\n    candidate_neighbors = set(person_graph.neighbors(person))\n\n    # Step 3: Remove candidate neighbors from the set if they are implicated in the given crime.\n    for p in G.neighbors(crime):\n        if p in candidate_neighbors:\n            candidate_neighbors.remove(p)\n\n    # Step 4: Rank-order the candidate neighbors by number of shared connections.\n    data = []\n    for nbr in candidate_neighbors:\n        data.append(dict(node=nbr, weight=person_graph.edges[person, nbr][\"weight\"]))\n    return pd.DataFrame(data).sort_values(\"weight\", ascending=False)\n\n\ndef bipartite_degree_centrality_denominator():\n    \"\"\"Answer to bipartite graph denominator for degree centrality.\"\"\"\n\n    ans = \"\"\"\nThe total number of neighbors that a node can _possibly_ have\nis the number of nodes in the other partition.\nThis comes naturally from the definition of a bipartite graph,\nwhere nodes can _only_ be connected to nodes in the other partition.\n\"\"\"\n    return ans\n\n\ndef find_most_crime_person(G, person_nodes):\n    dcs = (\n        pd.Series(nx.bipartite.degree_centrality(G, person_nodes))\n        .sort_values(ascending=False)\n        .to_frame()\n    )\n    return dcs.reset_index().query(\"index.str.contains('p')\").iloc[0][\"index\"]\n\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"04-advanced/01-bipartite/#introduction","title":"Introduction","text":""},{"location":"04-advanced/01-bipartite/#what-are-bipartite-graphs","title":"What are bipartite graphs?","text":"<p>As the name suggests, bipartite have two (bi) node partitions (partite). In other words, we can assign nodes to one of the two partitions. (By contrast, all of the graphs that we have seen before are unipartite: they only have a single partition.)</p>"},{"location":"04-advanced/01-bipartite/#rules-for-bipartite-graphs","title":"Rules for bipartite graphs","text":"<p>With unipartite graphs, you might remember a few rules that apply.</p> <p>Firstly, nodes and edges belong to a set. This means the node set contains only unique members, i.e. no node can be duplicated. The same applies for the edge set.</p> <p>On top of those two basic rules, bipartite graphs add an additional rule: Edges can only occur between nodes of different partitions. In other words, nodes within the same partition  are not allowed to be connected to one another.</p>"},{"location":"04-advanced/01-bipartite/#applications-of-bipartite-graphs","title":"Applications of bipartite graphs","text":"<p>Where do we see bipartite graphs being used? Here's one that is very relevant to e-commerce, which touches our daily lives:</p> <p>&gt; We can model customer purchases of products using a bipartite graph. &gt; Here, the two node sets are customer nodes and product nodes, &gt; and edges indicate that a customer C purchased a product P.</p> <p>On the basis of this graph, we can do interesting analyses, such as finding customers that are similar to one another on the basis of their shared product purchases.</p> <p>Can you think of other situations where a bipartite graph model can be useful?</p>"},{"location":"04-advanced/01-bipartite/#dataset","title":"Dataset","text":"<p>Here's another application in crime analysis, which is relevant to the example that we will use in this chapter:</p> <p>&gt; This bipartite network contains persons &gt; who appeared in at least one crime case  &gt; as either a suspect, a victim, a witness  &gt; or both a suspect and victim at the same time.  &gt; A left node represents a person and a right node represents a crime.  &gt; An edge between two nodes shows that  &gt; the left node was involved in the crime  &gt; represented by the right node.</p> <p>This crime dataset was also sourced from Konect.</p>"},{"location":"04-advanced/01-bipartite/#visualize-the-crime-network","title":"Visualize the crime network","text":"<p>To help us get our bearings right, let's visualize the crime network.</p>"},{"location":"04-advanced/01-bipartite/#exercise-extract-each-node-set","title":"Exercise: Extract each node set","text":"<p>A useful thing to be able to do is to extract each partition's node set. This will become handy when interacting with NetworkX's bipartite algorithms later on.</p> <p>&gt; Write a function that extracts all of the nodes  &gt; from specified node partition. &gt; It should also raise a plain Exception &gt; if no nodes exist in that specified partition. &gt; (as a precuation against users putting in invalid partition names).</p>"},{"location":"04-advanced/01-bipartite/#bipartite-graph-projections","title":"Bipartite Graph Projections","text":"<p>In a bipartite graph, one task that can be useful to do is to calculate the projection of a graph onto one of its nodes.</p> <p>What do we mean by the \"projection of a graph\"? It is best visualized using this figure:</p>"},{"location":"04-advanced/01-bipartite/#computing-graph-projections","title":"Computing graph projections","text":"<p>How does one compute graph projections using NetworkX? Turns out, NetworkX has a <code>bipartite</code> submodule, which gives us all of the facilities that we need to interact with bipartite algorithms.</p> <p>First of all, we need to check that the graph is indeed a bipartite graph. NetworkX provides a function for us to do so:</p>"},{"location":"04-advanced/01-bipartite/#exercise-find-the-crimes-that-have-the-most-shared-connections-with-other-crimes","title":"Exercise: find the crime(s) that have the most shared connections with other crimes","text":"<p>&gt; Find crimes that are most similar to one another &gt; on the basis of the number of shared connections to individuals.</p> <p>Hint: This is a degree centrality problem!</p>"},{"location":"04-advanced/01-bipartite/#exercise-find-the-individuals-that-have-the-most-shared-connections-with-other-individuals","title":"Exercise: find the individual(s) that have the most shared connections with other individuals","text":"<p>&gt; Now do the analogous thing for individuals!</p>"},{"location":"04-advanced/01-bipartite/#weighted-projection","title":"Weighted Projection","text":"<p>Though we were able to find out which graphs were connected with one another, we did not record in the resulting projected graph the strength by which the two nodes were connected. To preserve this information, we need another function:</p>"},{"location":"04-advanced/01-bipartite/#exercise-find-the-people-that-can-help-with-investigating-a-crimes-person","title":"Exercise: Find the people that can help with investigating a <code>crime</code>'s <code>person</code>.","text":"<p>Let's pretend that we are a detective trying to solve a crime, and that we right now need to find other individuals who were not implicated in the same exact crime as an individual was, but who might be able to give us information about that individual because they were implicated in other crimes with that individual.</p> <p>&gt; Implement a function that takes in a bipartite graph <code>G</code>, a string <code>person</code> and a string <code>crime</code>, &gt; and returns a list of other <code>person</code>s that were not implicated in the <code>crime</code>, &gt; but were connected to the <code>person</code> via other crimes. &gt; It should return a ranked list, &gt; based on the number of shared crimes (from highest to lowest) &gt; because the ranking will help with triage.</p>"},{"location":"04-advanced/01-bipartite/#degree-centrality","title":"Degree Centrality","text":"<p>The degree centrality metric is something we can calculate for bipartite graphs. Recall that the degree centrality metric is the number of neighbors of a node divided by the total number of possible neighbors.</p> <p>In a unipartite graph, the denominator can be the total number of nodes less one (if self-loops are not allowed) or simply the total number of nodes (if self loops are allowed).</p>"},{"location":"04-advanced/01-bipartite/#exercise-what-is-the-denominator-for-bipartite-graphs","title":"Exercise: What is the denominator for bipartite graphs?","text":"<p>Think about it for a moment, then write down your answer.</p>"},{"location":"04-advanced/01-bipartite/#exercise-which-persons-are-implicated-in-the-most-number-of-crimes","title":"Exercise: Which <code>persons</code> are implicated in the most number of crimes?","text":"<p>&gt; Find the <code>persons</code> (singular or plural) who are connected to the most number of crimes.</p> <p>To do so, you will need to use <code>nx.bipartite.degree_centrality</code>, rather than the regular <code>nx.degree_centrality</code> function.</p> <p><code>nx.bipartite.degree_centrality</code> requires that you pass in a node set from one of the partitions so that it can correctly partition nodes on the other set. What is returned, though, is the degree centrality for nodes in both sets. Here is an example to show you how the function is used:</p> <pre><code>dcs = nx.bipartite.degree_centrality(my_graph, nodes_from_one_partition)\n</code></pre>"},{"location":"04-advanced/01-bipartite/#solutions","title":"Solutions","text":"<p>Here are the solutions to the exercises above.</p>"},{"location":"04-advanced/02-linalg/","title":"Chapter 10: Linear Algebra","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"uTHihJiRELc\", width=\"100%\")\n</code></pre> <p>In this chapter, we will look at the relationship between graphs and linear algebra.</p> <p>The deep connection between these two topics is super interesting, and I'd like to show it to you through an exploration of three topics:</p> <ol> <li>Path finding</li> <li>Message passing</li> <li>Bipartite projections</li> </ol> <pre><code>import networkx as nx\n\nnodes = list(range(4))\nG1 = nx.Graph()\nG1.add_nodes_from(nodes)\nG1.add_edges_from(zip(nodes, nodes[1:]))\n</code></pre> <p>we can visualize the graph:</p> <pre><code>nx.draw(G1, with_labels=True)\n</code></pre> <p>and we can visualize its adjacency matrix:</p> <pre><code>import nxviz as nv\n\nm = nv.matrix(G1)\n</code></pre> <p>and we can obtain the adjacency matrix as a NumPy array:</p> <pre><code>A1 = nx.to_numpy_array(G1, nodelist=sorted(G1.nodes()))\nA1\n</code></pre> <pre>\n<code>array([[0., 1., 0., 0.],\n       [1., 0., 1., 0.],\n       [0., 1., 0., 1.],\n       [0., 0., 1., 0.]])</code>\n</pre> <pre><code>import numpy as np\n\nnp.linalg.matrix_power(A1, 2)\n</code></pre> <pre>\n<code>array([[1., 0., 1., 0.],\n       [0., 2., 0., 1.],\n       [1., 0., 2., 0.],\n       [0., 1., 0., 1.]])</code>\n</pre> <pre><code>from nams.solutions.linalg import adjacency_matrix_power\nfrom nams.functions import render_html\n\nrender_html(adjacency_matrix_power())\n</code></pre> <ol> <li>The diagonals equal to the degree of each node.</li> <li>The off-diagonals also contain values, which correspond to the number of paths that exist of length 2 between the node on the row axis and the node on the column axis.</li> </ol> <p>In fact, the diagonal also takes on the same meaning!</p> <p>For the terminal nodes, there is only 1 path from itself back to itself, while for the middle nodes, there are 2 paths from itself back to itself!</p> <pre><code>np.linalg.matrix_power(A1, 3)\n</code></pre> <pre>\n<code>array([[0., 2., 0., 1.],\n       [2., 0., 3., 0.],\n       [0., 3., 0., 2.],\n       [1., 0., 2., 0.]])</code>\n</pre> <p>You should be able to convince yourself that:</p> <ol> <li>There's no way to go from a node back to itself in 3 steps, thus explaining the diagonals, and </li> <li>The off-diagonals take on the correct values when you think about them in terms of \"ways to go from one node to another\".</li> </ol> <pre><code>G2 = nx.DiGraph()\nG2.add_nodes_from(nodes)\nG2.add_edges_from(zip(nodes, nodes[1:]))\nnx.draw(G2, with_labels=True)\n</code></pre> <pre><code>A2 = nx.to_numpy_array(G2)\nnp.linalg.matrix_power(A2, 2)\n</code></pre> <pre>\n<code>array([[0., 0., 1., 0.],\n       [0., 0., 0., 1.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])</code>\n</pre> <pre><code>np.linalg.matrix_power(A2, 3)\n</code></pre> <pre>\n<code>array([[0., 0., 0., 1.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])</code>\n</pre> <pre><code>np.linalg.matrix_power(A2, 4)\n</code></pre> <pre>\n<code>array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])</code>\n</pre> <pre><code>M = np.array([1, 0, 0, 0])\nM\n</code></pre> <pre>\n<code>array([1, 0, 0, 0])</code>\n</pre> <p>Notice where the position of the value <code>1</code> is - at the first node.</p> <p>If we take M and matrix multiply it against A2, let's see what we get:</p> <pre><code>M @ A2\n</code></pre> <pre>\n<code>array([0., 1., 0., 0.])</code>\n</pre> <p>The message has been passed onto the next node! And if we pass the message one more time:</p> <pre><code>M @ A2 @ A2\n</code></pre> <pre>\n<code>array([0., 0., 1., 0.])</code>\n</pre> <p>Now, the message lies on the 3rd node!</p> <p>We can make an animation to visualize this more clearly.  There are comments in the code to explain what's going on!</p> <pre><code>def propagate(G, msg, n_frames):\n    \"\"\"\n    Computes the node values based on propagation.\n\n    Intended to be used before or when being passed into the\n    anim() function (defined below).\n\n    :param G: A NetworkX Graph.\n    :param msg: The initial state of the message.\n    :returns: A list of 1/0 representing message status at\n        each node.\n    \"\"\"\n    # Initialize a list to store message states at each timestep.\n    msg_states = []\n\n    # Set a variable `new_msg` to be the initial message state.\n    new_msg = msg\n\n    # Get the adjacency matrix of the graph G.\n    A = nx.to_numpy_array(G)\n\n    # Perform message passing at each time step\n    for i in range(n_frames):\n        msg_states.append(new_msg)\n        new_msg = new_msg @ A\n\n    # Return the message states.\n    return msg_states\n</code></pre> <pre><code>from IPython.display import HTML\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\n\ndef update_func(step, nodes, colors):\n    \"\"\"\n    The update function for each animation time step.\n\n    :param step: Passed in from matplotlib's FuncAnimation. Must\n        be present in the function signature.\n    :param nodes: Returned from nx.draw_networkx_edges(). Is an\n        array of colors.\n    :param colors: A list of pre-computed colors.\n    \"\"\"\n    nodes.set_array(colors[step].ravel())\n    return nodes\n\n\ndef anim(G, initial_state, n_frames=4):\n    \"\"\"\n    Animation function!\n    \"\"\"\n    # First, pre-compute the message passing states over all frames.\n    colors = propagate(G, initial_state, n_frames)\n    # Instantiate a figure\n    fig = plt.figure()\n    # Precompute node positions so that they stay fixed over the entire animation\n    pos = nx.kamada_kawai_layout(G)\n    # Draw nodes to screen\n    nodes = nx.draw_networkx_nodes(\n        G, pos=pos, node_color=colors[0].ravel(), node_size=20\n    )\n    # Draw edges to screen\n    ax = nx.draw_networkx_edges(G, pos)\n    # Finally, return the animation through matplotlib.\n    return animation.FuncAnimation(\n        fig, update_func, frames=range(n_frames), fargs=(nodes, colors)\n    )\n\n\n# Initialize the message\nmsg = np.zeros(len(G2))\nmsg[0] = 1\n\n# Animate the graph with message propagation.\nHTML(anim(G2, msg, n_frames=4).to_html5_video())\n</code></pre>    Your browser does not support the video tag.  <p>Let's start by looking at a toy bipartite graph, a \"customer-product\" purchase record graph, with 4 products and 3 customers. The matrix representation might be as follows:</p> <pre><code># Rows = customers, columns = products, 1 = customer purchased product, 0 = customer did not purchase product.\ncp_mat = np.array([[0, 1, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1]])\n</code></pre> <p>From this \"bi-adjacency\" matrix, one can compute the projection onto the customers, matrix multiplying the matrix with its transpose.</p> <pre><code>c_mat = cp_mat @ cp_mat.T  # c_mat means \"customer matrix\"\nc_mat\n</code></pre> <pre>\n<code>array([[1, 0, 1],\n       [0, 2, 2],\n       [1, 2, 4]])</code>\n</pre> <p>What we get is the connectivity matrix of the customers, based on shared purchases.  The diagonals are the degree of the customers in the original graph,  i.e. the number of purchases they originally made,  and the off-diagonals are the connectivity matrix, based on shared products.</p> <p>To get the products matrix, we make the transposed matrix the left side of the matrix multiplication.</p> <pre><code>p_mat = cp_mat.T @ cp_mat  # p_mat means \"product matrix\"\np_mat\n</code></pre> <pre>\n<code>array([[2, 1, 2, 1],\n       [1, 2, 1, 1],\n       [2, 1, 2, 1],\n       [1, 1, 1, 1]])</code>\n</pre> <p>You may now try to convince yourself that the diagonals are the number of times a customer purchased that product, and the off-diagonals are the connectivity matrix of the products, weighted by how similar two customers are.</p> <pre><code>from nams import load_data as cf\n\nG_amzn = cf.load_amazon_reviews()\n</code></pre> <pre>\n<code>\n  0%|          | 0/64706 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n 24%|\u2588\u2588\u258d       | 15820/64706 [00:00&lt;00:00, 158188.84it/s]</code>\n</pre> <pre>\n<code>\n 50%|\u2588\u2588\u2588\u2588\u2589     | 32199/64706 [00:00&lt;00:00, 161476.16it/s]</code>\n</pre> <pre>\n<code>\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 48347/64706 [00:00&lt;00:00, 112490.61it/s]</code>\n</pre> <pre>\n<code>\n 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 63731/64706 [00:00&lt;00:00, 125405.76it/s]</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00&lt;00:00, 129069.62it/s]</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>\n  0%|          | 0/64706 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00&lt;00:00, 801464.25it/s]</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre>\n<code>\n  0%|          | 0/64706 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00&lt;00:00, 849904.59it/s]</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <p>Remember that with bipartite graphs, it is useful to obtain nodes from one of the partitions.</p> <pre><code>from nams.solutions.bipartite import extract_partition_nodes\n</code></pre> <pre><code>customer_nodes = extract_partition_nodes(G_amzn, \"customer\")\nmat = nx.bipartite.biadjacency_matrix(G_amzn, row_order=customer_nodes)\n</code></pre> <p>You'll notice that this matrix is extremely large! There are 5541 customers and 3568 products, for a total matrix size of 5541 \\times 3568 = 19770288, but it is stored in a sparse format because only 64706 elements are filled in.</p> <pre><code>mat\n</code></pre> <pre>\n<code>&lt;Compressed Sparse Row sparse array of dtype 'int64'\n    with 64706 stored elements and shape (5541, 3568)&gt;</code>\n</pre> <pre><code>customer_mat = mat @ mat.T\n</code></pre> <p>Next, get the diagonals of the customer-customer matrix. Recall here that in <code>customer_mat</code>, the diagonals correspond to the degree of the customer nodes in the bipartite matrix.</p> <p>SciPy sparse matrices provide a <code>.diagonal()</code> method that returns the diagonal elements.</p> <pre><code># Get the diagonal.\ndegrees = customer_mat.diagonal()\n</code></pre> <p>Finally, find the index of the customer that has the highest degree.</p> <pre><code>cust_idx = np.argmax(degrees)\ncust_idx\n</code></pre> <pre>\n<code>np.int64(294)</code>\n</pre> <p>We can verify this independently by sorting the customer nodes by degree.</p> <pre><code>import pandas as pd\nimport janitor\n\n# There's some pandas-fu we need to use to get this correct.\ndeg = (\n    pd.Series(dict(nx.degree(G_amzn, customer_nodes)))\n    .to_frame()\n    .reset_index()\n    .rename_column(\"index\", \"customer\")\n    .rename_column(0, \"num_reviews\")\n    .sort_values(\"num_reviews\", ascending=False)\n)\ndeg.head()\n</code></pre> customer num_reviews 294 A9Q28YTLYREO7 578 86 A3HU0B9XUEVHIM 375 77 A3KJ6JAZPH382D 301 307 A3C6ZCBUNXUT7V 261 218 A8IFUOL8S9BZC 256 <p>Indeed, customer 294 was the one who had the most number of reviews!</p> <pre><code>import scipy.sparse as sp\n</code></pre> <pre><code># Construct diagonal elements.\ncustomer_diags = sp.diags(degrees)\n# Subtract off-diagonals.\noff_diagonals = customer_mat - customer_diags\n# Compute index of most similar individuals.\nnp.unravel_index(np.argmax(off_diagonals), customer_mat.shape)\n</code></pre> <pre>\n<code>(np.int64(86), np.int64(294))</code>\n</pre> <pre><code>from time import time\n\nstart = time()\n\n# Compute the projection\nG_cust = nx.bipartite.weighted_projected_graph(G_amzn, customer_nodes)\n\n# Identify the most similar customers\nmost_similar_customers = sorted(\n    G_cust.edges(data=True), key=lambda x: x[2][\"weight\"], reverse=True\n)[0]\n\nend = time()\nprint(f\"{end - start:.3f} seconds\")\nprint(f\"Most similar customers: {most_similar_customers}\")\n</code></pre> <pre>\n<code>9.579 seconds\nMost similar customers: ('A3HU0B9XUEVHIM', 'A9Q28YTLYREO7', {'weight': 154})\n</code>\n</pre> <pre><code>start = time()\n\n# Compute the projection using matrices\nmat = nx.bipartite.matrix.biadjacency_matrix(G_amzn, customer_nodes)\ncust_mat = mat @ mat.T\n\n# Identify the most similar customers\ndegrees = customer_mat.diagonal()\ncustomer_diags = sp.diags(degrees)\noff_diagonals = customer_mat - customer_diags\nc1, c2 = np.unravel_index(np.argmax(off_diagonals), customer_mat.shape)\n\nend = time()\nprint(f\"{end - start:.3f} seconds\")\nprint(\n    f\"Most similar customers: {customer_nodes[c1]}, {customer_nodes[c2]}, {cust_mat[c1, c2]}\"\n)\n</code></pre> <pre>\n<code>0.343 seconds\nMost similar customers: A3HU0B9XUEVHIM, A9Q28YTLYREO7, 154\n</code>\n</pre> <p>On a modern PC, the matrix computation should be about 10-50X faster using the matrix form compared to the object-oriented form. (The web server that is used to build the book might not necessarily have the software stack to do this though, so the time you see reported might not reflect the expected speedups.) I'd encourage you to fire up a Binder session or clone the book locally  to test out the code yourself.</p> <p>You may notice that it's much easier to read the \"objects\" code,  but the matrix code way outperforms the object code.  This tradeoff is common in computing, and shouldn't surprise you. That said, the speed gain alone is a great reason to use matrices!</p>"},{"location":"04-advanced/02-linalg/#introduction","title":"Introduction","text":""},{"location":"04-advanced/02-linalg/#preliminaries","title":"Preliminaries","text":"<p>Before we go deep into the linear algebra piece though, we have to first make sure some ideas are clear.</p> <p>The most important thing that we need when treating graphs in linear algebra form is the adjacency matrix. For example, for four nodes joined in a chain:</p>"},{"location":"04-advanced/02-linalg/#symmetry","title":"Symmetry","text":"<p>Remember that for an undirected graph, the adjacency matrix will be symmetric about the diagonal, while for a directed graph, the adjacency matrix will be asymmetric.</p>"},{"location":"04-advanced/02-linalg/#path-finding","title":"Path finding","text":"<p>In the Paths chapter, we can use the breadth-first search algorithm to find a shortest path between any two nodes.</p> <p>As it turns out, using adjacency matrices, we can answer a related question, which is how many paths exist of length K between two nodes.</p> <p>To see how, we need to see the relationship between matrix powers and graph path lengths.</p> <p>Let's take the adjacency matrix above, raise it to the second power, and see what it tells us.</p>"},{"location":"04-advanced/02-linalg/#exercise-adjacency-matrix-power","title":"Exercise: adjacency matrix power?","text":"<p>&gt; What do you think the values in the adjacency matrix are related to? &gt; If studying in a group, discuss with your neighbors; &gt; if working on this alone, write down your thoughts.</p>"},{"location":"04-advanced/02-linalg/#higher-matrix-powers","title":"Higher matrix powers","text":"<p>The semantic meaning of adjacency matrix powers is preserved even if we go to higher powers. For example, if we go to the 3rd matrix power:</p>"},{"location":"04-advanced/02-linalg/#with-directed-graphs","title":"With directed graphs?","text":"<p>Does the \"number of steps\" interpretation hold with directed graphs? Yes it does! Let's see it in action.</p>"},{"location":"04-advanced/02-linalg/#exercise-directed-graph-matrix-power","title":"Exercise: directed graph matrix power","text":"<p>&gt; Convince yourself that the resulting adjacency matrix power &gt; contains the same semantic meaning &gt; as that for an undirected graph, &gt; that is, &gt; the number of ways to go from \"row\" node to \"column\" node &gt; in K steps. &gt; (I have provided three different matrix powers for you.)</p>"},{"location":"04-advanced/02-linalg/#message-passing","title":"Message Passing","text":"<p>Let's now dive into the second topic here, that of message passing.</p> <p>To show how message passing works on a graph, let's start with the directed linear chain, as this will make things easier to understand.</p>"},{"location":"04-advanced/02-linalg/#message-representation-in-matrix-form","title":"\"Message\" representation in matrix form","text":"<p>Our graph adjacency matrix contains nodes ordered in a particular fashion along the rows and columns. We can also create a \"message\" matrix M, using the same ordering of nodes along the rows, with columns instead representing a \"message\" that is intended to be \"passed\" from one node to another:</p>"},{"location":"04-advanced/02-linalg/#bipartite-graphs-matrices","title":"Bipartite Graphs &amp; Matrices","text":"<p>The section on message passing above assumed unipartite graphs, or at least graphs for which messages can be meaningfully passed between nodes. </p> <p>In this section, we will look at bipartite graphs. </p> <p>Recall from before the definition of a bipartite graph:</p> <ul> <li>Nodes are separated into two partitions (hence 'bi'-'partite').</li> <li>Edges can only occur between nodes of different partitions.</li> </ul> <p>Bipartite graphs have a natural matrix representation, known as the biadjacency matrix. Nodes on one partition are the rows, and nodes on the other partition are the columns.</p> <p>NetworkX's <code>bipartite</code> module provides a function for computing the biadjacency matrix of a bipartite graph.</p>"},{"location":"04-advanced/02-linalg/#exercises","title":"Exercises","text":"<p>In the following exercises, you will now play with a customer-product graph from Amazon. This dataset was downloaded from UCSD's Julian McAuley's website, and corresponds to the digital music dataset.</p> <p>This is a bipartite graph. The two partitions are:</p> <ul> <li><code>customers</code>: The customers that were doing the reviews.</li> <li><code>products</code>: The music that was being reviewed.</li> </ul> <p>In the original dataset (see the original JSON in the <code>datasets/</code> directory), they are referred to as:</p> <ul> <li><code>customers</code>: <code>reviewerID</code></li> <li><code>products</code>: <code>asin</code></li> </ul>"},{"location":"04-advanced/02-linalg/#example-finding-customers-who-reviewed-the-most-number-of-music-items","title":"Example: finding customers who reviewed the most number of music items.","text":"<p>Let's find out which customers reviewed the most number of music items.</p> <p>To do so, you can break the problem into a few steps.</p> <p>First off, we compute the customer projection using matrix operations.</p>"},{"location":"04-advanced/02-linalg/#example-finding-similar-customers","title":"Example: finding similar customers","text":"<p>Let's now also compute which two customers are similar, based on shared reviews. To do so involves the following steps:</p> <ol> <li>We construct a sparse matrix consisting of only the diagonals. <code>scipy.sparse.diags(elements)</code> will construct a sparse diagonal matrix based on the elements inside <code>elements</code>.</li> <li>Subtract the diagonals from the customer matrix projection. This yields the customer-customer similarity matrix, which should only consist of the off-diagonal elements of the customer matrix projection.</li> <li>Finally, get the indices where the weight (shared number of between the customers is highest. (This code is provided for you.)</li> </ol>"},{"location":"04-advanced/02-linalg/#performance-object-vs-matrices","title":"Performance: Object vs. Matrices","text":"<p>Finally, to motivate why you might want to use matrices rather than graph objects to compute some of these statistics, let's time the two ways of getting to the same answer.</p>"},{"location":"04-advanced/02-linalg/#objects","title":"Objects","text":"<p>Let's first use NetworkX's built-in machinery to find customers that are most similar.</p>"},{"location":"04-advanced/02-linalg/#matrices","title":"Matrices","text":"<p>Now, let's implement the same thing in matrix form.</p>"},{"location":"04-advanced/02-linalg/#acceleration-on-a-gpu","title":"Acceleration on a GPU","text":"<p>If your appetite has been whipped up for even more acceleration and you have a GPU on your daily compute, then you're very much in luck!</p> <p>The RAPIDS.AI project has a package called cuGraph, which provides GPU-accelerated graph algorithms. As over release 0.16.0, all cuGraph algorithms will be able to accept NetworkX graph objects! This came about through online conversations on GitHub and Twitter, which for us, personally, speaks volumes to the power of open source projects!</p> <p>Because cuGraph does presume that you have access to a GPU, and because we assume most readers of this book might not have access to one easily, we'll delegate teaching how to install and use cuGraph to the cuGraph devs and their documentation. Nonetheless, if you do have the ability to install and use the RAPIDS stack, definitely check it out!</p>"},{"location":"04-advanced/03-stats/","title":"Chapter 11: Statistical Inference","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>from IPython.display import YouTubeVideo\n\nYouTubeVideo(id=\"P-0CJpO3spg\", width=\"100%\")\n</code></pre> <p>In this chapter, we are going to take a look at how to perform statistical inference on graphs.</p> <pre><code>import networkx as nx\n\n\nG_er = nx.erdos_renyi_graph(n=30, p=0.2)\n</code></pre> <pre><code>nx.draw(G_er)\n</code></pre> <p>You can verify that there's approximately 20% of \\frac{30^2 - 30}{2} = 435.</p> <pre><code>len(G_er.edges())\n</code></pre> <pre>\n<code>82</code>\n</pre> <pre><code>len(G_er.edges()) / 435\n</code></pre> <pre>\n<code>0.18850574712643678</code>\n</pre> <p>We can also look at the degree distribution:</p> <pre><code>import pandas as pd\nfrom nams.functions import ecdf\nimport matplotlib.pyplot as plt\n\nx, y = ecdf(pd.Series(dict(nx.degree(G_er))))\nplt.scatter(x, y)\n</code></pre> <pre>\n<code>&lt;matplotlib.collections.PathCollection at 0x7fb0d23d94d0&gt;</code>\n</pre> <pre><code>G_ba = nx.barabasi_albert_graph(n=30, m=3)\nnx.draw(G_ba)\n</code></pre> <pre><code>len(G_ba.edges())\n</code></pre> <pre>\n<code>81</code>\n</pre> <p>And the degree distribution:</p> <pre><code>x, y = ecdf(pd.Series(dict(nx.degree(G_ba))))\nplt.scatter(x, y)\n</code></pre> <pre>\n<code>&lt;matplotlib.collections.PathCollection at 0x7fb0d2108e90&gt;</code>\n</pre> <p>You can see that even though the number of edges between the two graphs are similar, their degree distribution is wildly different.</p> <pre><code>from nams import load_data as cf\n\nG = cf.load_propro_network()\nfor n, d in G.nodes(data=True):\n    G.nodes[n][\"degree\"] = G.degree(n)\n</code></pre> <p>As is always the case, let's make sure we know some basic stats of the graph.</p> <pre><code>len(G.nodes())\n</code></pre> <pre>\n<code>1870</code>\n</pre> <pre><code>len(G.edges())\n</code></pre> <pre>\n<code>2277</code>\n</pre> <p>Let's also examine the degree distribution of the graph.</p> <pre><code>x, y = ecdf(pd.Series(dict(nx.degree(G))))\nplt.scatter(x, y)\n</code></pre> <pre>\n<code>&lt;matplotlib.collections.PathCollection at 0x7fb0d2154110&gt;</code>\n</pre> <p>Finally, we should visualize the graph to get a feel for it.</p> <pre><code>import nxviz as nv\nfrom nxviz import annotate\n\nnv.circos(\n    G, sort_by=\"degree\", node_color_by=\"degree\", node_enc_kwargs={\"size_scale\": 10}\n)\nannotate.node_colormapping(G, color_by=\"degree\")\n</code></pre> <p>One thing we might infer from this visualization is that the vast majority of nodes have a very small degree, while a very small number of nodes have a high degree. That would prompt us to think: what process could be responsible for generating this graph?</p> <pre><code>from ipywidgets import interact, IntSlider\n\nm = IntSlider(value=2, min=1, max=10)\n\n\n@interact(m=m)\ndef compare_barabasi_albert_graph(m):\n    fig, ax = plt.subplots()\n    G_ba = nx.barabasi_albert_graph(n=len(G.nodes()), m=m)\n    x, y = ecdf(pd.Series(dict(nx.degree(G_ba))))\n    ax.scatter(x, y, label=\"Barabasi-Albert Graph\")\n\n    x, y = ecdf(pd.Series(dict(nx.degree(G))))\n    ax.scatter(x, y, label=\"Protein Interaction Network\")\n    ax.legend()\n</code></pre> <pre><code>from ipywidgets import FloatSlider\n\np = FloatSlider(value=0.6, min=0, max=0.1, step=0.001)\n\n\n@interact(p=p)\ndef compare_erdos_renyi_graph(p):\n    fig, ax = plt.subplots()\n    G_er = nx.erdos_renyi_graph(n=len(G.nodes()), p=p)\n    x, y = ecdf(pd.Series(dict(nx.degree(G_er))))\n    ax.scatter(x, y, label=\"Erdos-Renyi Graph\")\n\n    x, y = ecdf(pd.Series(dict(nx.degree(G))))\n    ax.scatter(x, y, label=\"Protein Interaction Network\")\n    ax.legend()\n    ax.set_title(f\"p={p}\")\n</code></pre> <p>Given the degree distribution only, which model do you think better describes the generation of a protein-protein interaction network?</p> <pre><code>from scipy.stats import wasserstein_distance\n\n\ndef erdos_renyi_degdist(n, p):\n    \"\"\"Return a Pandas series of degree distribution of an Erdos-Renyi graph.\"\"\"\n    G = nx.erdos_renyi_graph(n=n, p=p)\n    return pd.Series(dict(nx.degree(G)))\n\n\ndef barabasi_albert_degdist(n, m):\n    \"\"\"Return a Pandas series of degree distribution of an Barabasi-Albert graph.\"\"\"\n    G = nx.barabasi_albert_graph(n=n, m=m)\n    return pd.Series(dict(nx.degree(G)))\n</code></pre> <pre><code>deg = pd.Series(dict(nx.degree(G)))\n\ner_deg = erdos_renyi_degdist(n=len(G.nodes()), p=0.001)\nba_deg = barabasi_albert_degdist(n=len(G.nodes()), m=1)\nwasserstein_distance(deg, er_deg), wasserstein_distance(deg, ba_deg)\n</code></pre> <pre>\n<code>(np.float64(0.8192513368983954), np.float64(0.4770053475935835))</code>\n</pre> <p>Notice that because the graphs are instantiated in a non-deterministic fashion, re-running the cell above will give you different values for each new graph generated.</p> <p>Let's now plot the wasserstein distance to our graph data for the two particular Erdos-Renyi and Barabasi-Albert graph models shown above.</p> <pre><code>import matplotlib.pyplot as plt\nfrom tqdm.autonotebook import tqdm\n\ner_dist = []\nba_dist = []\nfor _ in tqdm(range(100)):\n    er_deg = erdos_renyi_degdist(n=len(G.nodes()), p=0.001)\n    er_dist.append(wasserstein_distance(deg, er_deg))\n\n    ba_deg = barabasi_albert_degdist(n=len(G.nodes()), m=1)\n    ba_dist.append(wasserstein_distance(deg, ba_deg))\n\n# er_degs = [erdos_renyi_degdist(n=len(G.nodes()), p=0.001) for _ in range(100)]\n</code></pre> <pre><code>import seaborn as sns\nimport janitor\n\n\ndata = (\n    pd.DataFrame(\n        {\n            \"Erdos-Renyi\": er_dist,\n            \"Barabasi-Albert\": ba_dist,\n        }\n    )\n    .melt(value_vars=[\"Erdos-Renyi\", \"Barabasi-Albert\"])\n    .rename_columns({\"variable\": \"Graph Model\", \"value\": \"Wasserstein Distance\"})\n)\nsns.swarmplot(data=data, x=\"Graph Model\", y=\"Wasserstein Distance\")\n</code></pre> <pre>\n<code>&lt;Axes: xlabel='Graph Model', ylabel='Wasserstein Distance'&gt;</code>\n</pre> <p>From this, we might conclude that the Barabasi-Albert graph with m=1 has the better fit to the protein-protein interaction network graph.</p>"},{"location":"04-advanced/03-stats/#introduction","title":"Introduction","text":""},{"location":"04-advanced/03-stats/#statistics-refresher","title":"Statistics refresher","text":"<p>Before we can proceed with statistical inference on graphs, we must first refresh ourselves with some ideas from the world of statistics. Otherwise, the methods that we will end up using may seem a tad weird, and hence difficult to follow along.</p> <p>To review statistical ideas, let's set up a few statements and explore what they mean.</p>"},{"location":"04-advanced/03-stats/#we-are-concerned-with-models-of-randomness","title":"We are concerned with models of randomness","text":"<p>As with all things statistics, we are concerned with models of randomness. Here, probability distributions give us a way to think about random events and how to assign credibility points to them.</p>"},{"location":"04-advanced/03-stats/#in-an-abstract-fashion","title":"In an abstract fashion...","text":"<p>The supremely abstract way of thinking about a probability distribution is that it is the space of all possibilities of \"stuff\" with different credibility points distributed amongst each possible \"thing\".</p>"},{"location":"04-advanced/03-stats/#more-concretely-the-coin-flip","title":"More concretely: the coin flip","text":"<p>A more concrete example is to consider the coin flip. Here, the space of all possibilities of \"stuff\" is the set of \"heads\" and \"tails\". If we have a fair coin, then we have 0.5 credibility points distributed to each of \"heads\" and \"tails\".</p>"},{"location":"04-advanced/03-stats/#another-example-dice-rolls","title":"Another example: dice rolls","text":"<p>Another concrete example is to consider the six-sided dice. Here, the space of all possibilities of \"stuff\" is the set of numbers in the range [1, 6]. If we have a fair dice, then we have 1/6 credibility points assigned to each of the numbers. (Unfair dice will have an unequal distribution of credibility points across each face.)</p>"},{"location":"04-advanced/03-stats/#a-graph-based-example-social-networks","title":"A graph-based example: social networks","text":"<p>If we receive an undirected social network graph with 5 nodes and 6 edges, we have to keep in mind that this graph with 6 edges was merely one of 15 \\choose 6 ways to construct 5 node, 6 edge graphs. (15 comes up because there are 15 edges that can be constructed in a 5-node undirected graph.)</p>"},{"location":"04-advanced/03-stats/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>A commonplace task in statistical inferences is calculating the probability of observing a value or something more extreme under an assumed \"null\" model of reality. This is what we commonly call \"hypothesis testing\", and where the oft-misunderstood term \"p-value\" shows up.</p>"},{"location":"04-advanced/03-stats/#hypothesis-testing-in-coin-flips-by-simulation","title":"Hypothesis testing in coin flips, by simulation","text":"<p>As an example, hypothesis testing in coin flips follows this logic:</p> <ul> <li>I observe that 8 out of 10 coin tosses give me heads, giving me a probability of heads p=0.8 (a summary statistic).</li> <li>Under a \"null distribution\" of a fair coin, I simulate the distribution of probability of heads (the summary statistic) that I would get from 10 coin tosses.</li> <li>Finally, I use that distribution to calculate the probability of observing p=0.8 or more extreme.</li> </ul>"},{"location":"04-advanced/03-stats/#hypothesis-testing-in-graphs","title":"Hypothesis testing in graphs","text":"<p>The same protocol applies when we perform hypothesis testing on graphs.</p> <p>Firstly, we calculate a summary statistic that describes our graph.</p> <p>Secondly, we propose a null graph model, and calculate our summary statistic under simulated versions of that null graph model.</p> <p>Thirdly, we look at the probability of observing the summary statistic value that we calculated in step 1 or more extreme, under the assumed graph null model distribution.</p>"},{"location":"04-advanced/03-stats/#stochastic-graph-creation-models","title":"Stochastic graph creation models","text":"<p>Since we are going to be dealing with models of randomness in graphs, let's take a look at some examples.</p>"},{"location":"04-advanced/03-stats/#erdos-renyi-aka-binomial-graph","title":"Erdos-Renyi (a.k.a. \"binomial\") graph","text":"<p>On easy one to study is the Erdos-Renyi graph, also known as the \"binomial\" graph.</p> <p>The data generation story here is that we instantiate an undirected graph with n nodes, giving \\frac{n^2 - n}{2} possible edges. Each edge has a probability p of being created.</p>"},{"location":"04-advanced/03-stats/#barabasi-albert-graph","title":"Barabasi-Albert Graph","text":"<p>The data generating story of this graph generator is essentially that nodes that have lots of edges preferentially get new edges attached onto them.  This is what we call a \"preferential attachment\" process.</p>"},{"location":"04-advanced/03-stats/#load-data","title":"Load Data","text":"<p>For this notebook, we are going to look at a protein-protein interaction network, and test the hypothesis that this network was not generated by the data generating process described by an Erdos-Renyi graph.</p> <p>Let's load a protein-protein interaction network dataset.</p> <p>&gt; This undirected network contains protein interactions contained in yeast. &gt; Research showed that proteins with a high degree &gt; were more important for the surivial of the yeast than others. &gt; A node represents a protein and an edge represents a metabolic interaction between two proteins.  &gt; The network contains loops.</p>"},{"location":"04-advanced/03-stats/#inferring-graph-generating-model","title":"Inferring Graph Generating Model","text":"<p>Given a graph dataset, how do we identify which data generating model provides the best fit?</p> <p>One way to do this is to compare characteristics of a graph generating model against the characteristics of the graph. The logic here is that if we have a good graph generating model for the data, we should, in theory, observe the observed graph's characteristics in the graphs generated by the graph generating model.</p>"},{"location":"04-advanced/03-stats/#comparison-of-degree-distribution","title":"Comparison of degree distribution","text":"<p>Let's compare the degree distribution between the data, a few Erdos-Renyi graphs, and a few Barabasi-Albert graphs.</p>"},{"location":"04-advanced/03-stats/#comparison-with-barabasi-albert-graphs","title":"Comparison with Barabasi-Albert graphs","text":""},{"location":"04-advanced/03-stats/#comparison-with-erdos-renyi-graphs","title":"Comparison with Erdos-Renyi graphs","text":""},{"location":"04-advanced/03-stats/#quantitative-model-comparison","title":"Quantitative Model Comparison","text":"<p>Each time we plug in a value of m for the Barabasi-Albert graph model, we are using one of many possible Barabasi-Albert graph models, each with a different m. Similarly, each time we choose a different p for the Erdos-Renyi model, we are using one of many possible Erdos-Renyi graph models, each with a different p.</p> <p>To quantitatively compare degree distributions, we can use the Wasserstein distance between the data. Let's see how to implement this.</p>"},{"location":"04-advanced/03-stats/#interpretation","title":"Interpretation","text":"<p>That statement, accurate as it might be, still does not connect the dots to biology.</p> <p>Let's think about the generative model for this graph. The Barabasi-Albert graph gives us a model for \"rich gets richer\". Given the current state of the graph, if we want to add a new edge, we first pick a node with probability proportional to the number of edges it already has. Then, we pick another node with probability proportional to the number of edges that it has too. Finally, we add an edge there. This has the effect of \"enriching\" nodes that have a large number of edges with more edges.</p> <p>How might this connect to biology?</p> <p>We can't necessarily provide a concrete answer, but this model might help raise new hypotheses.</p> <p>For example, if protein-protein interactions of the \"binding\" kind are driven by subdomains, then proteins that acquire a domain through recombination may end up being able to bind to everything else that the domain was able to. In this fashion, proteins with that particular binding domain gain new edges more readily.</p> <p>Testing these hypotheses would be a totally different matter, and at this point, I submit the above hypothesis with a large amount of salt thrown over my shoulder. In other words, the hypothesized mechanism could be completely wrong. However, I hope that this example illustrated that the usage of a \"graph generative model\" can help us narrow down hypotheses about the observed world.</p>"},{"location":"05-casestudies/01-gameofthrones/","title":"Chapter 12: Game of Thrones","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport pandas as pd\nimport networkx as nx\nimport community\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>from nams import load_data as cf\n\nbooks = cf.load_game_of_thrones_data()\n</code></pre> <p>The resulting DataFrame books has 5 columns: Source, Target, Type, weight, and book. Source and target are the two nodes that are linked by an edge. As we know a network can have directed or undirected edges and in this network all the edges are undirected. The weight attribute of every edge tells us the number of interactions that the characters have had over the book, and the book column tells us the book number.</p> <p>Let's have a look at the data.</p> <pre><code># We also add this weight_inv to our dataset.\n# Why? we will discuss it in a later section.\nbooks[\"weight_inv\"] = 1 / books.weight\n</code></pre> <pre><code>books.head()\n</code></pre> Source Target Type weight book weight_inv id 0 Addam-Marbrand Jaime-Lannister Undirected 3 1.0 0.333333 1 Addam-Marbrand Tywin-Lannister Undirected 6 1.0 0.166667 2 Aegon-I-Targaryen Daenerys-Targaryen Undirected 5 1.0 0.200000 3 Aegon-I-Targaryen Eddard-Stark Undirected 4 1.0 0.250000 4 Aemon-Targaryen-(Maester-Aemon) Alliser-Thorne Undirected 4 1.0 0.250000 <p>From the above data we can see that the characters Addam Marbrand and Tywin Lannister have interacted 6 times in the first book.</p> <p>We can investigate this data by using the pandas DataFrame. Let's find all the interactions of Robb Stark in the third book.</p> <pre><code>robbstark = books.query(\"book == 3\").query(\n    \"Source == 'Robb-Stark' or Target == 'Robb-Stark'\"\n)\n</code></pre> <pre><code>robbstark.head()\n</code></pre> Source Target Type weight book weight_inv id 1468 Aegon-Frey-(son-of-Stevron) Robb-Stark Undirected 5 3.0 0.200000 1582 Arya-Stark Robb-Stark Undirected 14 3.0 0.071429 1604 Balon-Greyjoy Robb-Stark Undirected 6 3.0 0.166667 1677 Bran-Stark Robb-Stark Undirected 18 3.0 0.055556 1683 Brandon-Stark Robb-Stark Undirected 3 3.0 0.333333 <p>As you can see this data easily translates to a network problem. Now it's time to create a network. We create a graph for each book. It's possible to create one <code>MultiGraph</code>(Graph with multiple edges between nodes) instead of 5 graphs, but it is easier to analyse and manipulate individual <code>Graph</code> objects rather than a <code>MultiGraph</code>.</p> <pre><code># example of creating a MultiGraph\n\n# all_books_multigraph = nx.from_pandas_edgelist(\n#            books, source='Source', target='Target',\n#            edge_attr=['weight', 'book'],\n#            create_using=nx.MultiGraph)\n</code></pre> <pre><code># we create a list of graph objects using\n# nx.from_pandas_edgelist and specifying\n# the edge attributes.\n\ngraphs = [\n    nx.from_pandas_edgelist(\n        books[books.book == i],\n        source=\"Source\",\n        target=\"Target\",\n        edge_attr=[\"weight\", \"weight_inv\"],\n    )\n    for i in range(1, 6)\n]\n</code></pre> <pre><code># The Graph object associated with the first book.\ngraphs[0]\n</code></pre> <pre>\n<code>&lt;networkx.classes.graph.Graph at 0x7efbbf6e7a50&gt;</code>\n</pre> <pre><code># To access the relationship edges in the graph with\n# the edge attribute weight data (data=True)\nrelationships = list(graphs[0].edges(data=True))\n</code></pre> <pre><code>relationships[0:3]\n</code></pre> <pre>\n<code>[('Addam-Marbrand',\n  'Jaime-Lannister',\n  {'weight': 3, 'weight_inv': 0.3333333333333333}),\n ('Addam-Marbrand',\n  'Tywin-Lannister',\n  {'weight': 6, 'weight_inv': 0.16666666666666666}),\n ('Jaime-Lannister', 'Aerys-II-Targaryen', {'weight': 5, 'weight_inv': 0.2})]</code>\n</pre> <pre><code># We use the in-built degree_centrality method\ndeg_cen_book1 = nx.degree_centrality(graphs[0])\ndeg_cen_book5 = nx.degree_centrality(graphs[4])\n</code></pre> <p><code>degree_centrality</code> returns a dictionary and to access the results we can directly use the name of the character.</p> <pre><code>deg_cen_book1[\"Daenerys-Targaryen\"]\n</code></pre> <pre>\n<code>0.11290322580645162</code>\n</pre> <p>Top 5 important characters in the first book according to degree centrality.</p> <pre><code># The following expression sorts the dictionary by\n# degree centrality and returns the top 5 from a graph\n\nsorted(deg_cen_book1.items(), key=lambda x: x[1], reverse=True)[0:5]\n</code></pre> <pre>\n<code>[('Eddard-Stark', 0.3548387096774194),\n ('Robert-Baratheon', 0.2688172043010753),\n ('Tyrion-Lannister', 0.24731182795698928),\n ('Catelyn-Stark', 0.23118279569892475),\n ('Jon-Snow', 0.19892473118279572)]</code>\n</pre> <p>Top 5 important characters in the fifth book according to degree centrality.</p> <pre><code>sorted(deg_cen_book5.items(), key=lambda x: x[1], reverse=True)[0:5]\n</code></pre> <pre>\n<code>[('Jon-Snow', 0.1962025316455696),\n ('Daenerys-Targaryen', 0.18354430379746836),\n ('Stannis-Baratheon', 0.14873417721518986),\n ('Tyrion-Lannister', 0.10443037974683544),\n ('Theon-Greyjoy', 0.10443037974683544)]</code>\n</pre> <p>To visualize the distribution of degree centrality let's plot a histogram of degree centrality.</p> <pre><code>plt.hist(deg_cen_book1.values(), bins=30)\nplt.show()\n</code></pre> <p>The above plot shows something that is expected, a high portion of characters aren't connected to lot of other characters while some characters are highly connected all through the network. A close real world example of this is a social network like Twitter where a few people have millions of connections(followers) but majority of users aren't connected to that many other users. This exponential decay like property resembles power law in real life networks.</p> <pre><code># A log-log plot to show the \"signature\" of power law in graphs.\nfrom collections import Counter\n\nhist = Counter(deg_cen_book1.values())\nplt.scatter(np.log2(list(hist.keys())), np.log2(list(hist.values())), alpha=0.9)\nplt.show()\n</code></pre> <pre><code>from nams.solutions.got import weighted_degree\n\nplt.hist(list(weighted_degree(graphs[0], \"weight\").values()), bins=30)\nplt.show()\n</code></pre> <pre><code>sorted(weighted_degree(graphs[0], \"weight\").items(), key=lambda x: x[1], reverse=True)[\n    0:5\n]\n</code></pre> <pre>\n<code>[('Eddard-Stark', 1284),\n ('Robert-Baratheon', 941),\n ('Jon-Snow', 784),\n ('Tyrion-Lannister', 650),\n ('Sansa-Stark', 545)]</code>\n</pre> <pre><code># First check unweighted (just the structure)\n\nsorted(nx.betweenness_centrality(graphs[0]).items(), key=lambda x: x[1], reverse=True)[\n    0:10\n]\n</code></pre> <pre>\n<code>[('Eddard-Stark', 0.2696038913836117),\n ('Robert-Baratheon', 0.21403028397371796),\n ('Tyrion-Lannister', 0.1902124972697492),\n ('Jon-Snow', 0.17158135899829566),\n ('Catelyn-Stark', 0.1513952715347627),\n ('Daenerys-Targaryen', 0.08627015537511595),\n ('Robb-Stark', 0.07298399629664767),\n ('Drogo', 0.06481224290874964),\n ('Bran-Stark', 0.05579958811784442),\n ('Sansa-Stark', 0.03714483664326785)]</code>\n</pre> <pre><code># Let's care about interactions now\n\nsorted(\n    nx.betweenness_centrality(graphs[0], weight=\"weight_inv\").items(),\n    key=lambda x: x[1],\n    reverse=True,\n)[0:10]\n</code></pre> <pre>\n<code>[('Eddard-Stark', 0.5926474861958733),\n ('Catelyn-Stark', 0.36855565242662014),\n ('Jon-Snow', 0.3514094739901191),\n ('Robert-Baratheon', 0.3329991281604185),\n ('Tyrion-Lannister', 0.27137460040685846),\n ('Daenerys-Targaryen', 0.202615518744551),\n ('Bran-Stark', 0.0945655332752107),\n ('Robb-Stark', 0.09177564661435629),\n ('Arya-Stark', 0.06939843068875327),\n ('Sansa-Stark', 0.06870095902353966)]</code>\n</pre> <p>We can see there are some differences between the unweighted and weighted centrality measures. Another thing to note is that we are using the weight_inv attribute instead of weight(the number of interactions between characters). This decision is based on the way we want to assign the notion of \"importance\" of a character. The basic idea behind betweenness centrality is to find nodes which are essential to the structure of the network. As betweenness centrality computes shortest paths underneath, in the case of weighted betweenness centrality it will end up penalising characters with high number of interactions. By using weight_inv we will prop up the characters with high interactions with other characters.</p> <pre><code># by default weight attribute in PageRank is weight\n# so we use weight=None to find the unweighted results\nsorted(nx.pagerank(graphs[0], weight=None).items(), key=lambda x: x[1], reverse=True)[\n    0:10\n]\n</code></pre> <pre>\n<code>[('Eddard-Stark', 0.04550213151218522),\n ('Tyrion-Lannister', 0.03301449867019286),\n ('Catelyn-Stark', 0.030189729639356143),\n ('Robert-Baratheon', 0.02982527420823678),\n ('Jon-Snow', 0.026840579868198603),\n ('Robb-Stark', 0.02155918676623074),\n ('Sansa-Stark', 0.019998050283349653),\n ('Bran-Stark', 0.019940543257472354),\n ('Jaime-Lannister', 0.017500555351025744),\n ('Cersei-Lannister', 0.017074603349338583)]</code>\n</pre> <pre><code>sorted(\n    nx.pagerank(graphs[0], weight=\"weight\").items(), key=lambda x: x[1], reverse=True\n)[0:10]\n</code></pre> <pre>\n<code>[('Eddard-Stark', 0.07236162026570049),\n ('Robert-Baratheon', 0.04849367196106829),\n ('Jon-Snow', 0.04770801150205558),\n ('Tyrion-Lannister', 0.04367631315626212),\n ('Catelyn-Stark', 0.034666613211363564),\n ('Bran-Stark', 0.02977004993932226),\n ('Robb-Stark', 0.029214217154195955),\n ('Daenerys-Targaryen', 0.027098612952214246),\n ('Sansa-Stark', 0.02694466267974004),\n ('Cersei-Lannister', 0.02162037092438613)]</code>\n</pre> <pre><code>from nams.solutions.got import correlation_centrality\n\ncorrelation_centrality(graphs[0])\n</code></pre> 0 1 2 3 0 1.000000 0.910373 0.992119 0.949258 1 0.910373 1.000000 0.879240 0.790526 2 0.992119 0.879240 1.000000 0.955060 3 0.949258 0.790526 0.955060 1.000000 <pre><code>evol = [nx.degree_centrality(graph) for graph in graphs]\nevol_df = pd.DataFrame.from_records(evol).fillna(0)\nevol_df[[\"Eddard-Stark\", \"Tyrion-Lannister\", \"Jon-Snow\"]].plot()\nplt.show()\n</code></pre> <pre><code>set_of_char = set()\nfor i in range(5):\n    set_of_char |= set(list(evol_df.T[i].sort_values(ascending=False)[0:5].index))\nset_of_char\n</code></pre> <pre>\n<code>{'Arya-Stark',\n 'Brienne-of-Tarth',\n 'Catelyn-Stark',\n 'Cersei-Lannister',\n 'Daenerys-Targaryen',\n 'Eddard-Stark',\n 'Jaime-Lannister',\n 'Joffrey-Baratheon',\n 'Jon-Snow',\n 'Margaery-Tyrell',\n 'Robb-Stark',\n 'Robert-Baratheon',\n 'Sansa-Stark',\n 'Stannis-Baratheon',\n 'Theon-Greyjoy',\n 'Tyrion-Lannister'}</code>\n</pre> <pre><code>from nams.solutions.got import evol_betweenness\n</code></pre> <pre><code>evol_betweenness(graphs)\n</code></pre> <pre><code>sorted(nx.degree_centrality(graphs[4]).items(), key=lambda x: x[1], reverse=True)[:5]\n</code></pre> <pre>\n<code>[('Jon-Snow', 0.1962025316455696),\n ('Daenerys-Targaryen', 0.18354430379746836),\n ('Stannis-Baratheon', 0.14873417721518986),\n ('Tyrion-Lannister', 0.10443037974683544),\n ('Theon-Greyjoy', 0.10443037974683544)]</code>\n</pre> <pre><code>sorted(nx.betweenness_centrality(graphs[4]).items(), key=lambda x: x[1], reverse=True)[\n    :5\n]\n</code></pre> <pre>\n<code>[('Stannis-Baratheon', 0.45283060689247934),\n ('Daenerys-Targaryen', 0.2959459062106149),\n ('Jon-Snow', 0.24484873673158666),\n ('Tyrion-Lannister', 0.20961613179551256),\n ('Robert-Baratheon', 0.17716906651536968)]</code>\n</pre> <pre><code>nx.draw(nx.barbell_graph(5, 1), with_labels=True)\n</code></pre> <p>As we know the a higher betweenness centrality means that the node is crucial for the structure of the network, and in the case of Stannis Baratheon in the fifth book it seems like Stannis Baratheon has characterstics similar to that of node 5 in the above example as it seems to be the holding the network together.</p> <p>As evident from the betweenness centrality scores of the above example of barbell graph, node 5 is the most important node in this network.</p> <pre><code>nx.betweenness_centrality(nx.barbell_graph(5, 1))\n</code></pre> <pre>\n<code>{0: 0.0,\n 1: 0.0,\n 2: 0.0,\n 3: 0.0,\n 4: 0.5333333333333333,\n 6: 0.5333333333333333,\n 7: 0.0,\n 8: 0.0,\n 9: 0.0,\n 10: 0.0,\n 5: 0.5555555555555556}</code>\n</pre> <pre><code>import nxviz as nv\nfrom nxviz import annotate\n\nplt.figure(figsize=(8, 8))\n\npartition = community.best_partition(graphs[0], randomize=False)\n\n# Annotate nodes' partitions\nfor n in graphs[0].nodes():\n    graphs[0].nodes[n][\"partition\"] = partition[n]\n    graphs[0].nodes[n][\"degree\"] = graphs[0].degree(n)\n\nnv.matrix(graphs[0], group_by=\"partition\", sort_by=\"degree\", node_color_by=\"partition\")\nannotate.matrix_block(graphs[0], group_by=\"partition\", color_by=\"partition\")\nannotate.matrix_group(graphs[0], group_by=\"partition\", offset=-8)\n</code></pre> <p>A common defining quality of a community is that the within-community edges are denser than the between-community edges.</p> <pre><code># louvain community detection find us 8 different set of communities\npartition_dict = {}\nfor character, par in partition.items():\n    if par in partition_dict:\n        partition_dict[par].append(character)\n    else:\n        partition_dict[par] = [character]\n</code></pre> <pre><code>len(partition_dict)\n</code></pre> <pre>\n<code>8</code>\n</pre> <pre><code>partition_dict[2]\n</code></pre> <pre>\n<code>['Bran-Stark',\n 'Rickon-Stark',\n 'Robb-Stark',\n 'Luwin',\n 'Theon-Greyjoy',\n 'Hali',\n 'Hallis-Mollen',\n 'Hodor',\n 'Hullen',\n 'Joseth',\n 'Nan',\n 'Osha',\n 'Rickard-Karstark',\n 'Rickard-Stark',\n 'Stiv',\n 'Jon-Umber-(Greatjon)',\n 'Galbart-Glover',\n 'Roose-Bolton',\n 'Maege-Mormont']</code>\n</pre> <p>If we plot these communities of the network we see a denser network as compared to the original network which contains all the characters.</p> <pre><code>nx.draw(nx.subgraph(graphs[0], partition_dict[3]))\n</code></pre> <pre><code>nx.draw(nx.subgraph(graphs[0], partition_dict[1]))\n</code></pre> <p>We can test this by calculating the density of the network and the community.</p> <p>Like in the following example the network between characters in a community is 5 times more dense than the original network.</p> <pre><code>nx.density(nx.subgraph(graphs[0], partition_dict[4])) / nx.density(graphs[0])\n</code></pre> <pre>\n<code>25.42543859649123</code>\n</pre> <pre><code>from nams.solutions.got import most_important_node_in_partition\n</code></pre> <pre><code>most_important_node_in_partition(graphs[0], partition_dict)\n</code></pre> <pre>\n<code>{7: 'Tyrion-Lannister',\n 1: 'Daenerys-Targaryen',\n 6: 'Eddard-Stark',\n 3: 'Jon-Snow',\n 5: 'Sansa-Stark',\n 2: 'Robb-Stark',\n 0: 'Waymar-Royce',\n 4: 'Danwell-Frey'}</code>\n</pre> <pre><code>from nams.solutions import got\nimport inspect\n\nprint(inspect.getsource(got))\n</code></pre> <pre>\n<code>import pandas as pd\nimport networkx as nx\n\n\ndef weighted_degree(G, weight):\n    result = dict()\n    for node in G.nodes():\n        weight_degree = 0\n        for n in G.edges([node], data=True):\n            weight_degree += n[2][\"weight\"]\n        result[node] = weight_degree\n    return result\n\n\ndef correlation_centrality(G):\n    cor = pd.DataFrame.from_records(\n        [\n            nx.pagerank(G, weight=\"weight\"),\n            nx.betweenness_centrality(G, weight=\"weight_inv\"),\n            weighted_degree(G, \"weight\"),\n            nx.degree_centrality(G),\n        ]\n    )\n    return cor.T.corr()\n\n\ndef evol_betweenness(graphs):\n    evol = [nx.betweenness_centrality(graph, weight=\"weight_inv\") for graph in graphs]\n    evol_df = pd.DataFrame.from_records(evol).fillna(0)\n\n    set_of_char = set()\n    for i in range(5):\n        set_of_char |= set(list(evol_df.T[i].sort_values(ascending=False)[0:5].index))\n\n    evol_df[list(set_of_char)].plot(figsize=(19, 10))\n\n\ndef most_important_node_in_partition(graph, partition_dict):\n    max_d = {}\n    deg = nx.degree_centrality(graph)\n    for group in partition_dict:\n        temp = 0\n        for character in partition_dict[group]:\n            if deg[character] &gt; temp:\n                max_d[group] = character\n                temp = deg[character]\n    return max_d\n\n</code>\n</pre>"},{"location":"05-casestudies/01-gameofthrones/#introduction","title":"Introduction","text":"<p>In this chapter, we will use Game of Thrones as a case study to practice our newly learnt skills of network analysis.</p> <p>It is suprising right? What is the relationship between a fatansy TV show/novel and network science or Python(not dragons).</p> <p>If you haven't heard of Game of Thrones, then you must be really good at hiding. Game of Thrones is a hugely popular television series by HBO based on the (also) hugely popular book series A Song of Ice and Fire by George R.R. Martin. In this notebook, we will analyze the co-occurrence network of the characters in the Game of Thrones books. Here, two characters are considered to co-occur if their names appear in the vicinity of 15 words from one another in the books.</p> <p>The figure below is a precusor of what we will analyse in this chapter.</p> <p></p> <p>The dataset is publicly avaiable for the 5 books at https://github.com/mathbeveridge/asoiaf. This is an interaction network and were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books. The edge weight corresponds to the number of interactions. </p> <p>Blog: https://networkofthrones.wordpress.com</p>"},{"location":"05-casestudies/01-gameofthrones/#finding-the-most-important-node-ie-character-in-these-networks","title":"Finding the most important node i.e character in these networks.","text":"<p>Let's use our network analysis knowledge to decrypt these Graphs that we have just created.</p> <p>Is it Jon Snow, Tyrion, Daenerys, or someone else? Let's see! Network Science offers us many different metrics to measure the importance of a node in a network as we saw in the first part of the tutorial. Note that there is no \"correct\" way of calculating the most important node in a network, every metric has a different meaning.</p> <p>First, let's measure the importance of a node in a network by looking at the number of neighbors it has, that is, the number of nodes it is connected to. For example, an influential account on Twitter, where the follower-followee relationship forms the network, is an account which has a high number of followers. This measure of importance is called degree centrality.</p> <p>Using this measure, let's extract the top ten important characters from the first book (<code>graphs[0]</code>) and the fifth book (<code>graphs[4]</code>).</p> <p>NOTE: We are using zero-indexing and that's why the graph of the first book is acceseed by <code>graphs[0]</code>.</p>"},{"location":"05-casestudies/01-gameofthrones/#exercise","title":"Exercise","text":"<p>Create a new centrality measure, weighted_degree(Graph, weight) which takes in Graph and the weight attribute and returns a weighted degree dictionary. Weighted degree is calculated by summing the weight of the all edges of a node and find the top five characters according to this measure.</p>"},{"location":"05-casestudies/01-gameofthrones/#betweeness-centrality","title":"Betweeness centrality","text":"<p>Let's do this for Betweeness centrality and check if this makes any difference. As different centrality method use different measures underneath, they find nodes which are important in the network. A centrality method like Betweeness centrality finds nodes which are structurally important to the network, which binds the network together and densely.</p>"},{"location":"05-casestudies/01-gameofthrones/#pagerank","title":"PageRank","text":"<p>The billion dollar algorithm, PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.</p> <p>NOTE: We don't need to worry about weight and weight_inv in PageRank as the algorithm uses weights in the opposite sense (larger weights are better). This may seem confusing as different centrality measures have different definition of weights. So it is always better to have a look at documentation before using weights in a centrality measure.</p>"},{"location":"05-casestudies/01-gameofthrones/#exercise_1","title":"Exercise","text":""},{"location":"05-casestudies/01-gameofthrones/#is-there-a-correlation-between-these-techniques","title":"Is there a correlation between these techniques?","text":"<p>Find the correlation between these four techniques.</p> <ul> <li>pagerank (weight = 'weight')</li> <li>betweenness_centrality (weight = 'weight_inv')</li> <li>weighted_degree</li> <li>degree centrality</li> </ul> <p>HINT: Use pandas correlation </p>"},{"location":"05-casestudies/01-gameofthrones/#evolution-of-importance-of-characters-over-the-books","title":"Evolution of importance of characters over the books","text":"<p>According to degree centrality the most important character in the first book is Eddard Stark but he is not even in the top 10 of the fifth book. The importance changes over the course of five books, because you know stuff happens ;)</p> <p>Let's look at the evolution of degree centrality of a couple of characters like Eddard Stark, Jon Snow, Tyrion which showed up in the top 10 of degree centrality in first book.</p> <p>We create a dataframe with character columns and index as books where every entry is the degree centrality of the character in that particular book and plot the evolution of degree centrality Eddard Stark, Jon Snow and Tyrion. We can see that the importance of Eddard Stark in the network dies off and with Jon Snow there is a drop in the fourth book but a sudden rise in the fifth book</p>"},{"location":"05-casestudies/01-gameofthrones/#exercise_2","title":"Exercise","text":"<p>Plot the evolution of betweenness centrality of the above mentioned characters over the 5 books.</p>"},{"location":"05-casestudies/01-gameofthrones/#so-whats-up-with-stannis-baratheon","title":"So what's up with Stannis Baratheon?","text":""},{"location":"05-casestudies/01-gameofthrones/#community-detection-in-networks","title":"Community detection in Networks","text":"<p>A network is said to have community structure if the nodes of the network can be easily grouped into (potentially overlapping) sets of nodes such that each set of nodes is densely connected internally. There are multiple algorithms and definitions to calculate these communites in a network.</p> <p>We will use louvain community detection algorithm to find the modules in our graph.</p>"},{"location":"05-casestudies/01-gameofthrones/#exercise_3","title":"Exercise","text":"<p>Find the most important node in the partitions according to degree centrality of the nodes using the partition_dict we have already created.</p>"},{"location":"05-casestudies/01-gameofthrones/#solutions","title":"Solutions","text":"<p>Here are the solutions to the exercises above.</p>"},{"location":"05-casestudies/02-airport/","title":"Chapter 13: Airport Network","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport networkx as nx\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <pre><code>from nams import load_data as cf\n\npass_air_data = cf.load_airports_data()\n</code></pre> <p>In the <code>pass_air_data</code> dataframe we have the information of number of people that fly every year on a particular route on the list of airlines that fly that route.</p> <pre><code>pass_air_data.head()\n</code></pre> YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS id 0 1990 ABE ACY {'US Airways Inc.'} 73.0 1 1990 ABE ATL {'Eastern Air Lines Inc.'} 73172.0 2 1990 ABE AVL {'Westair Airlines Inc.'} 0.0 3 1990 ABE AVP {'Westair Airlines Inc.', 'US Airways Inc.', '... 8397.0 4 1990 ABE BHM {'Eastern Air Lines Inc.'} 59.0 <p>Every row in this dataset is a unique route between 2 airports in United States territory in a particular year. Let's see how many people flew from New York JFK to Austin in 2006.</p> <p>NOTE: This will be a fun chapter if you are an aviation geek and like guessing airport IATA codes.</p> <pre><code>jfk_aus_2006 = pass_air_data.query(\"YEAR == 2006\").query(\n    \"ORIGIN == 'JFK' and DEST == 'AUS'\"\n)\n\njfk_aus_2006.head()\n</code></pre> YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS id 215634 2006 JFK AUS {'Shuttle America Corp.', 'Ameristar Air Cargo... 105290.0 <p>From the above pandas query we see that according to this dataset 105290 passengers travelled from JFK to AUS in the year 2006.</p> <p>But how does this dataset translate to an applied network analysis problem? In the previous chapter we created different graph objects for every book. Let's create a graph object which encompasses all the edges.</p> <p>NetworkX provides us with Multi(Di)Graphs to model networks with multiple edges between two nodes.</p> <p>In this case every row in the dataframe represents a directed edge between two airports, common sense suggests that if there is a flight from airport A to airport B there should definitely be a flight from airport B to airport A, i.e direction of the edge shouldn't matter. But in this dataset we have data for individual directions (A -&gt; B and B -&gt; A) so we create a MultiDiGraph.</p> <pre><code>passenger_graph = nx.from_pandas_edgelist(\n    pass_air_data,\n    source=\"ORIGIN\",\n    target=\"DEST\",\n    edge_key=\"YEAR\",\n    edge_attr=[\"PASSENGERS\", \"UNIQUE_CARRIER_NAME\"],\n    create_using=nx.MultiDiGraph(),\n)\n</code></pre> <p>We have created a MultiDiGraph object <code>passenger_graph</code> which contains all the information from the dataframe <code>pass_air_data</code>. <code>ORIGIN</code> and <code>DEST</code> represent the column names in the dataframe <code>pass_air_data</code> used to construct the edge. As this is a <code>MultiDiGraph</code> we can also give a name/key to the multiple edges between two nodes and <code>edge_key</code> is used to represent that name and in this graph <code>YEAR</code> is used to distinguish between multiple edges between two nodes. <code>PASSENGERS</code> and <code>UNIQUE_CARRIER_NAME</code> are added as edge attributes which can be accessed using the nodes and the key form the MultiDiGraph object.</p> <p>Let's check if can access the same information (the 2006 route between JFK and AUS) using our <code>passenger_graph</code>.</p> <p>To check an edge between two nodes in a Graph we can use the syntax <code>GraphObject[source][target]</code> and further specify the edge attribute using <code>GraphObject[source][target][attribute]</code>.</p> <pre><code>passenger_graph[\"JFK\"][\"AUS\"][2006]\n</code></pre> <pre>\n<code>{'PASSENGERS': 105290.0,\n 'UNIQUE_CARRIER_NAME': \"{'Shuttle America Corp.', 'Ameristar Air Cargo', 'JetBlue Airways', 'United Parcel Service'}\"}</code>\n</pre> <p>Now let's use our new constructed passenger graph to look at the evolution of passenger load over 25 years.</p> <pre><code># Route betweeen New York-JFK and SFO\n\nvalues = [\n    (year, attr[\"PASSENGERS\"]) for year, attr in passenger_graph[\"JFK\"][\"SFO\"].items()\n]\nx, y = zip(*values)\nplt.plot(x, y)\nplt.show()\n</code></pre> <p>We see some usual trends all across the datasets like steep drops in 2001 (after 9/11) and 2008 (recession).</p> <pre><code># Route betweeen SFO and Chicago-ORD\n\nvalues = [\n    (year, attr[\"PASSENGERS\"]) for year, attr in passenger_graph[\"SFO\"][\"ORD\"].items()\n]\nx, y = zip(*values)\nplt.plot(x, y)\nplt.show()\n</code></pre> <p>To find the overall trend, we can use our <code>pass_air_data</code> dataframe to calculate total passengers flown in a year.</p> <pre><code>pass_air_data.groupby([\"YEAR\"]).sum()[\"PASSENGERS\"].plot()\nplt.show()\n</code></pre> <pre><code>from nams.solutions.airport import busiest_route, plot_time_series\n</code></pre> <pre><code>busiest_route(pass_air_data, 1990).head()\n</code></pre> YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS id 3917 1990 LAX HNL {'Heavylift Cargo Airlines Lt', 'Hawaiian Airl... 1827164.0 <pre><code>plot_time_series(pass_air_data, \"LAX\", \"HNL\")\n</code></pre> <pre><code>busiest_route(pass_air_data, 2015).head()\n</code></pre> YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS id 445978 2015 LAX SFO {'Hawaiian Airlines Inc.', 'Delta Air Lines In... 1869066.0 <pre><code>plot_time_series(pass_air_data, \"LAX\", \"SFO\")\n</code></pre> <p>Before moving to the next part of the chapter let's create a method to extract edges from <code>passenger_graph</code> for a particular year so we can better analyse the data on a granular scale.</p> <pre><code>def year_network(G, year):\n    \"\"\"Extract edges for a particular year from\n    a MultiGraph. The edge is also populated with\n    two attributes, weight and weight_inv where\n    weight is the number of passengers and\n    weight_inv the inverse of it.\n    \"\"\"\n    year_network = nx.DiGraph()\n    for edge in G.edges:\n        source, target, edge_year = edge\n        if edge_year == year:\n            attr = G[source][target][edge_year]\n            year_network.add_edge(\n                source,\n                target,\n                weight=attr[\"PASSENGERS\"],\n                weight_inv=1 / (attr[\"PASSENGERS\"] if attr[\"PASSENGERS\"] != 0.0 else 1),\n                airlines=attr[\"UNIQUE_CARRIER_NAME\"],\n            )\n    return year_network\n</code></pre> <pre><code>pass_2015_network = year_network(passenger_graph, 2015)\n</code></pre> <pre><code># Extracted a Directed Graph from the Multi Directed Graph\n# Number of nodes = airports\n# Number of edges = routes\n\nprint(pass_2015_network)\n</code></pre> <pre>\n<code>DiGraph with 1258 nodes and 25354 edges\n</code>\n</pre> <pre><code># Loadin the GPS coordinates of all the airports\nfrom nams import load_data as cf\n\nlat_long = cf.load_airports_GPS_data()\nlat_long.columns = [\n    \"CODE4\",\n    \"CODE3\",\n    \"CITY\",\n    \"PROVINCE\",\n    \"COUNTRY\",\n    \"UNKNOWN1\",\n    \"UNKNOWN2\",\n    \"UNKNOWN3\",\n    \"UNKNOWN4\",\n    \"UNKNOWN5\",\n    \"UNKNOWN6\",\n    \"UNKNOWN7\",\n    \"UNKNOWN8\",\n    \"UNKNOWN9\",\n    \"LATITUDE\",\n    \"LONGITUDE\",\n]\nlat_long\nwanted_nodes = list(pass_2015_network.nodes())\nus_airports = (\n    lat_long.query(\"CODE3 in @wanted_nodes\")\n    .drop_duplicates(subset=[\"CODE3\"])\n    .set_index(\"CODE3\")\n)\nus_airports.head()\n# us_airports\n</code></pre> CODE4 CITY PROVINCE COUNTRY UNKNOWN1 UNKNOWN2 UNKNOWN3 UNKNOWN4 UNKNOWN5 UNKNOWN6 UNKNOWN7 UNKNOWN8 UNKNOWN9 LATITUDE LONGITUDE CODE3 ABI KABI ABILENE RGNL ABILENE USA 32 24 40 N 99 40 54 W 546 32.411 -99.682 ABQ KABQ NaN ALBUQUERQUE USA 0 0 0 U 0 0 0 U 0 0.000 0.000 ACK KACK NANTUCKET MEM NANTUCKET USA 41 15 10 N 70 3 36 W 15 41.253 -70.060 ACT KACT WACO RGNL WACO USA 31 36 40 N 97 13 49 W 158 31.611 -97.230 ACY KACY ATLANTIC CITY INTERNATIONAL ATLANTIC CITY USA 39 27 27 N 74 34 37 W 23 39.458 -74.577 <pre><code># Annotate graph with latitude and longitude\nno_gps = []\nfor n, d in pass_2015_network.nodes(data=True):\n    try:\n        pass_2015_network.nodes[n][\"longitude\"] = us_airports.loc[n, \"LONGITUDE\"]\n        pass_2015_network.nodes[n][\"latitude\"] = us_airports.loc[n, \"LATITUDE\"]\n        pass_2015_network.nodes[n][\"degree\"] = pass_2015_network.degree(n)\n\n    # Some of the nodes are not represented\n    except KeyError:\n        no_gps.append(n)\n\n# Get subgraph of nodes that do have GPS coords\nhas_gps = set(pass_2015_network.nodes()).difference(no_gps)\ng = pass_2015_network.subgraph(has_gps)\n</code></pre> <p>Let's first plot only the nodes, i.e airports. Places like Guam, US Virgin Islands are also included in this dataset as they are treated as domestic airports in this dataset.</p> <pre><code>import nxviz as nv\nfrom nxviz import nodes, plots, edges\n\nplt.figure(figsize=(20, 9))\npos = nodes.geo(g, encodings_kwargs={\"size_scale\": 1})\nplots.aspect_equal()\nplots.despine()\n</code></pre> <p>Let's also plot the routes(edges).</p> <pre><code>import nxviz as nv\nfrom nxviz import nodes, plots, edges, annotate\n\nplt.figure(figsize=(20, 9))\npos = nodes.geo(g, color_by=\"degree\", encodings_kwargs={\"size_scale\": 1})\nedges.line(g, pos, encodings_kwargs={\"alpha_scale\": 0.1})\nannotate.node_colormapping(g, color_by=\"degree\")\nplots.aspect_equal()\nplots.despine()\n</code></pre> <p>Before we proceed further, let's take a detour to briefly discuss directed networks and PageRank.</p> <pre><code># Create an empty directed graph object\nG = nx.DiGraph()\n# Add an edge from 1 to 2 with weight 4\nG.add_edge(1, 2, weight=4)\n</code></pre> <pre><code>print(G.edges(data=True))\n</code></pre> <pre>\n<code>[(1, 2, {'weight': 4})]\n</code>\n</pre> <pre><code># Access edge from 1 to 2\nG[1][2]\n</code></pre> <pre>\n<code>{'weight': 4}</code>\n</pre> <p>What happens when we try to access the edge from 2 to 1?</p> <p><pre><code>G[2][1]\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-137-d6b8db3142ef&gt; in &lt;module&gt;\n      1 # Access edge from 2 to 1\n----&amp;gt; 2 G[2][1]\n\n~/miniconda3/envs/nams/lib/python3.7/site-packages/networkx/classes/coreviews.py in __getitem__(self, key)\n     52 \n     53     def __getitem__(self, key):\n---&amp;gt; 54         return self._atlas[key]\n     55 \n     56     def copy(self):\n\nKeyError: 1\n</code></pre> </p> <p>As expected we get an error when we try to access the edge between 2 to 1 as this is a directed graph.</p> <pre><code>G.add_edges_from([(1, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2)])\nnx.draw_circular(G, with_labels=True)\n\n# nv.circos(G, node_enc_kwargs={\"size_scale\": 0.3})\n</code></pre> <p>Just by looking at the example above, we can conclude that node 2 should have the highest PageRank score as all the nodes are pointing towards it.</p> <p>This is confirmed by calculating the PageRank of this graph.</p> <pre><code>nx.pagerank(G)\n</code></pre> <pre>\n<code>{1: 0.08264481801983278,\n 2: 0.5041310918810032,\n 3: 0.08264481801983278,\n 4: 0.08264481801983278,\n 5: 0.08264481801983278,\n 6: 0.08264481801983278,\n 7: 0.08264481801983278}</code>\n</pre> <p>What happens when we add an edge from node 5 to node 6.</p> <pre><code>G.add_edge(5, 6)\n# nv.circos(G, node_enc_kwargs={\"size_scale\": 0.3})\nnx.draw_circular(G, with_labels=True)\n</code></pre> <pre><code>nx.pagerank(G)\n</code></pre> <pre>\n<code>{1: 0.08024854052495894,\n 2: 0.48440287805609844,\n 3: 0.08024854052495894,\n 4: 0.08024854052495894,\n 5: 0.08024854052495894,\n 6: 0.11435441931910648,\n 7: 0.08024854052495894}</code>\n</pre> <p>As expected there was some change in the scores (an increase for 6) but the overall trend stays the same, with node 2 leading the pack.</p> <pre><code>G.add_edge(2, 8)\nnx.draw_circular(G, with_labels=True)\n</code></pre> <p>Now we have an added an edge from 2 to a new node 8. As node 2 already has a high PageRank score, this should be passed on node 8. Let's see how much difference this can make.</p> <pre><code>nx.pagerank(G)\n</code></pre> <pre>\n<code>{1: 0.05378612718073914,\n 2: 0.3246687852772877,\n 3: 0.05378612718073914,\n 4: 0.05378612718073914,\n 5: 0.05378612718073914,\n 6: 0.07664541922580978,\n 7: 0.05378612718073914,\n 8: 0.32975515959320667}</code>\n</pre> <p>In this example, node 8 is now even more \"important\" than node 2 even though node 8 has only incoming connection.</p> <p>Let's move back to Airports and use this knowledge to analyse the network.</p> <p>Let's try to calculate the PageRank of <code>passenger_graph</code>.</p> <p><pre><code>nx.pagerank(passenger_graph)\n\n---------------------------------------------------------------------------\nNetworkXNotImplemented                    Traceback (most recent call last)\n&lt;ipython-input-144-15a6f513bf9b&gt; in &lt;module&gt;\n      1 # Let's try to calulate the PageRank measures of this graph.\n----&amp;gt; 2 nx.pagerank(passenger_graph)\n\n&lt;decorator-gen-435&gt; in pagerank(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\n\n~/miniconda3/envs/nams/lib/python3.7/site-packages/networkx/utils/decorators.py in _not_implemented_for(not_implement_for_func, *args, **kwargs)\n     78         if match:\n     79             msg = 'not implemented for %s type' % ' '.join(graph_types)\n---&amp;gt; 80             raise nx.NetworkXNotImplemented(msg)\n     81         else:\n     82             return not_implement_for_func(*args, **kwargs)\n\nNetworkXNotImplemented: not implemented for multigraph type\n</code></pre> </p> <p>As PageRank isn't defined for a MultiGraph in NetworkX we need to use our extracted yearly sub networks.</p> <pre><code># As pagerank will take weighted measure\n# by default we pass in None to make this\n# calculation for unweighted network\nPR_2015_scores = nx.pagerank(pass_2015_network, weight=None)\n</code></pre> <pre><code># Let's check the PageRank score for JFK\nPR_2015_scores[\"JFK\"]\n</code></pre> <pre>\n<code>0.0036376572979606578</code>\n</pre> <pre><code># top 10 airports according to unweighted PageRank\ntop_10_pr = sorted(PR_2015_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n</code></pre> <pre><code># top 10 airports according to unweighted betweenness centrality\ntop_10_bc = sorted(\n    nx.betweenness_centrality(pass_2015_network, weight=None).items(),\n    key=lambda x: x[1],\n    reverse=True,\n)[0:10]\n</code></pre> <pre><code># top 10 airports according to degree centrality\ntop_10_dc = sorted(\n    nx.degree_centrality(pass_2015_network).items(), key=lambda x: x[1], reverse=True\n)[0:10]\n</code></pre> <p>Before looking at the results do think about what we just calculated and try to guess which airports should come out at the top and be ready to be surprised :D</p> <pre><code># PageRank\ntop_10_pr\n</code></pre> <pre>\n<code>[('ANC', 0.010425531156396333),\n ('HPN', 0.008715287139161582),\n ('FAI', 0.007865131822111038),\n ('DFW', 0.007168038232113772),\n ('DEN', 0.0065572795198030115),\n ('ATL', 0.006367579588749721),\n ('ORD', 0.006178836107660131),\n ('YIP', 0.005821525504523929),\n ('ADQ', 0.005482597083474197),\n ('MSP', 0.005481962582230956)]</code>\n</pre> <pre><code># Betweenness Centrality\ntop_10_bc\n</code></pre> <pre>\n<code>[('ANC', 0.28907458480586606),\n ('FAI', 0.08042857784594384),\n ('SEA', 0.06745549919241699),\n ('HPN', 0.06046810178534726),\n ('ORD', 0.045544143864829294),\n ('ADQ', 0.040170160000905696),\n ('DEN', 0.038543251364241436),\n ('BFI', 0.03811277548952854),\n ('MSP', 0.03774809342340624),\n ('TEB', 0.036229439542316354)]</code>\n</pre> <pre><code># Degree Centrality\ntop_10_dc\n</code></pre> <pre>\n<code>[('ATL', 0.3643595863166269),\n ('ORD', 0.354813046937152),\n ('DFW', 0.3420843277645187),\n ('MSP', 0.3261734287987271),\n ('DEN', 0.31821797931583135),\n ('ANC', 0.3046937151949085),\n ('MEM', 0.29196499602227527),\n ('LAX', 0.2840095465393795),\n ('IAH', 0.28082736674622116),\n ('DTW', 0.27446300715990457)]</code>\n</pre> <p>The Degree Centrality results do make sense at first glance, ATL is Atlanta, ORD is Chicago, these are defintely airports one would expect to be at the top of a list which calculates \"importance\" of an airport. But when we look at PageRank and Betweenness Centrality we have an unexpected airport 'ANC'. Do think about measures like PageRank and Betweenness Centrality and what they calculate. Do note that currently we have used the core structure of the network, no other metadata like number of passengers. These are calculations on the unweighted network.</p> <p>'ANC' is the airport code of Anchorage airport, a place in Alaska, and according to pagerank and betweenness centrality it is the most important airport in this network. Isn't that weird? Thoughts?</p> <p>Looks like 'ANC' is essential to the core structure of the network, as it is the main airport connecting Alaska with other parts of US. This explains the high Betweenness Centrality score and there are flights from other major airports to 'ANC' which explains the high PageRank score.</p> <p>Related blog post: https://toreopsahl.com/2011/08/12/why-anchorage-is-not-that-important-binary-ties-and-sample-selection/</p> <p>Let's look at weighted version, i.e taking into account the number of people flying to these places.</p> <pre><code># Recall from the last chapter we use weight_inv\n# while calculating betweenness centrality\nsorted(\n    nx.betweenness_centrality(pass_2015_network, weight=\"weight_inv\").items(),\n    key=lambda x: x[1],\n    reverse=True,\n)[0:10]\n</code></pre> <pre>\n<code>[('SEA', 0.4192179843829966),\n ('ATL', 0.3589665389741017),\n ('ANC', 0.32425767084369994),\n ('LAX', 0.2668567170342895),\n ('ORD', 0.10008664852621497),\n ('DEN', 0.0964658422388763),\n ('MSP', 0.09300021788810685),\n ('DFW', 0.0926644126226465),\n ('FAI', 0.08824779747216016),\n ('BOS', 0.08259764427486331)]</code>\n</pre> <pre><code>sorted(\n    nx.pagerank(pass_2015_network, weight=\"weight\").items(),\n    key=lambda x: x[1],\n    reverse=True,\n)[0:10]\n</code></pre> <pre>\n<code>[('ATL', 0.037535963029303135),\n ('ORD', 0.02832976612273937),\n ('SEA', 0.028274564067008255),\n ('ANC', 0.02712786664756704),\n ('DFW', 0.025700504188894403),\n ('DEN', 0.025260024346433284),\n ('LAX', 0.023940434986084523),\n ('PHX', 0.018373176636420244),\n ('CLT', 0.017807039300630757),\n ('LAS', 0.01764968314104996)]</code>\n</pre> <p>When we adjust for number of passengers we see that we have a reshuffle in the \"importance\" rankings, and they do make a bit more sense now. According to weighted PageRank, Atlanta, Chicago, Seattle the top 3 airports while Anchorage is at 4th rank now.</p> <p>To get an even better picture of this we should do the analyse with more metadata about the routes not just the number of passengers.</p> <p>We can use the inbuilt networkx method <code>average_shortest_path_length</code> to find the average shortest path length of a network.</p> <pre><code>nx.average_shortest_path_length(pass_2015_network)\n\n---------------------------------------------------------------------------\nNetworkXError                             Traceback (most recent call last)\n&lt;ipython-input-157-acfe9bf3572a&gt; in &lt;module&gt;\n----&amp;gt; 1 nx.average_shortest_path_length(pass_2015_network)\n\n~/miniconda3/envs/nams/lib/python3.7/site-packages/networkx/algorithms/shortest_paths/generic.py in average_shortest_path_length(G, weight, method)\n    401     # Shortest path length is undefined if the graph is disconnected.\n    402     if G.is_directed() and not nx.is_weakly_connected(G):\n--&amp;gt; 403         raise nx.NetworkXError(\"Graph is not weakly connected.\")\n    404     if not G.is_directed() and not nx.is_connected(G):\n    405         raise nx.NetworkXError(\"Graph is not connected.\")\n\nNetworkXError: Graph is not weakly connected.\n</code></pre> <p></p> <p>Wait, What? This network is not \"connected\" (ignore the term weakly for the moment). That seems weird. It means that there are nodes which aren't reachable from other set of nodes, which isn't good news in especially a transporation network.</p> <p>Let's have a look at these far flung airports which aren't reachable.</p> <pre><code>components = list(nx.weakly_connected_components(pass_2015_network))\n</code></pre> <pre><code># There are 3 weakly connected components in the network.\nfor c in components:\n    print(len(c))\n</code></pre> <pre>\n<code>1255\n2\n1\n</code>\n</pre> <pre><code># Let's look at the component with 2 and 1 airports respectively.\nprint(components[1])\nprint(components[2])\n</code></pre> <pre>\n<code>{'SSB', 'SPB'}\n{'AIK'}\n</code>\n</pre> <p>The airports <code>SSB</code> and <code>SPB</code> are codes for Seaplanes airports and they have flights to each other so it makes sense that they aren't connected to the larger network of airports.</p> <p>The airport is even more weird as it is in a component in itself, i.e there is a flight from <code>AIK</code> to <code>AIK</code>. After investigating further it just seems like an anomaly in this dataset.</p> <pre><code>AIK_DEST_2015 = pass_air_data[\n    (pass_air_data[\"YEAR\"] == 2015) &amp;amp; (pass_air_data[\"DEST\"] == \"AIK\")\n]\nAIK_DEST_2015.head()\n</code></pre> YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS id 433338 2015 AIK AIK {'Wright Air Service'} 0.0 <pre><code># Let's get rid of them, we don't like them\npass_2015_network.remove_nodes_from([\"SPB\", \"SSB\", \"AIK\"])\n</code></pre> <pre><code># Our network is now weakly connected\nnx.is_weakly_connected(pass_2015_network)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># It's not strongly connected\nnx.is_strongly_connected(pass_2015_network)\n</code></pre> <pre>\n<code>False</code>\n</pre> <pre><code># NOTE: The notion of strongly and weakly exists only for directed graphs.\nG = nx.DiGraph()\n\n# Let's create a cycle directed graph, 1 -&amp;gt; 2 -&amp;gt; 3 -&amp;gt; 1\nG.add_edge(1, 2)\nG.add_edge(2, 3)\nG.add_edge(3, 1)\nnx.draw(G, with_labels=True)\n</code></pre> <p>In the above example we can reach any node irrespective of where we start traversing the network, if we start from 2 we can reach 1 via 3. In this network every node is \"reachable\" from one another, i.e the network is strongly connected.</p> <pre><code>nx.is_strongly_connected(G)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># Let's add a new connection\nG.add_edge(3, 4)\nnx.draw(G, with_labels=True)\n</code></pre> <p>It's evident from the example above that we can't traverse the network graph. If we start from node 4 we are stuck at the node, we don't have any way of leaving node 4. This is assuming we strictly follow the direction of edges. In this case the network isn't strongly connected but if we look at the structure and assume the directions of edges don't matter than we can go to any other node in the network even if we start from node 4.</p> <p>In the case an undirected copy of directed network is connected we call the directed network as weakly connected.</p> <pre><code>nx.is_strongly_connected(G)\n</code></pre> <pre>\n<code>False</code>\n</pre> <pre><code>nx.is_weakly_connected(G)\n</code></pre> <pre>\n<code>True</code>\n</pre> <p>Let's go back to our airport network of 2015.</p> <p>After removing those 3 airports the network is weakly connected.</p> <pre><code>nx.is_weakly_connected(pass_2015_network)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>nx.is_strongly_connected(pass_2015_network)\n</code></pre> <pre>\n<code>False</code>\n</pre> <p>But our network is still not strongly connected, which essentially means there are airports in the network where you can fly into but not fly back, which doesn't really seem okay</p> <pre><code>strongly_connected_components = list(\n    nx.strongly_connected_components(pass_2015_network)\n)\n</code></pre> <pre><code># Let's look at one of the examples of a strong connected component\nstrongly_connected_components[0]\n</code></pre> <pre>\n<code>{'BCE'}</code>\n</pre> <pre><code>bce_2015 = pass_air_data.query(\"YEAR == 2015\").query(\"ORIGIN == 'BCE' or DEST == 'BCE'\")\nbce_2015\n</code></pre> YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS id 451074 2015 PGA BCE {'Grand Canyon Airlines, Inc. d/b/a Grand Cany... 8.0 <p>As we can see above you can fly into <code>BCE</code> but can't fly out, weird indeed. These airport are small airports with one off schedules flights. For the purposes of our analyses we can ignore such airports.</p> <pre><code># Let's find the biggest strongly connected component\npass_2015_strong_nodes = max(strongly_connected_components, key=len)\n</code></pre> <pre><code># Create a subgraph with the nodes in the\n# biggest strongly connected component\npass_2015_strong = pass_2015_network.subgraph(nodes=pass_2015_strong_nodes)\n</code></pre> <pre><code>nx.is_strongly_connected(pass_2015_strong)\n</code></pre> <pre>\n<code>True</code>\n</pre> <p>After removing multiple airports we now have a strongly connected airport network. We can now travel from one airport to any other airport in the network.</p> <pre><code># We started with 1258 airports\nlen(pass_2015_strong)\n</code></pre> <pre>\n<code>1190</code>\n</pre> <pre><code>nx.average_shortest_path_length(pass_2015_strong)\n</code></pre> <pre>\n<code>3.174661992635574</code>\n</pre> <p>The 3.17 number above represents the average length between 2 airports in the network which means that it's possible to go from one airport to another in this network under 3 layovers, which sounds nice. A more reachable network is better, not necessearily in terms of revenue for the airline but for social health of the air transport network.</p> <pre><code>from nams.solutions.airport import add_opinated_edges\n</code></pre> <pre><code>new_routes_network = add_opinated_edges(pass_2015_strong)\n</code></pre> <pre><code>nx.average_shortest_path_length(new_routes_network)\n</code></pre> <pre>\n<code>3.0888508809747615</code>\n</pre> <p>Using an opinionated heuristic we were able to reduce the average shortest path length of the network. Check the solution below to understand the idea behind the heuristic, do try to come up with your own heuristics.</p> <pre><code># We have access to the airlines that fly the route in the edge attribute airlines\npass_2015_network[\"JFK\"][\"SFO\"]\n</code></pre> <pre>\n<code>{'weight': 1179941.0,\n 'weight_inv': 8.4750000211875e-07,\n 'airlines': \"{'Delta Air Lines Inc.', 'Virgin America', 'American Airlines Inc.', 'Sun Country Airlines d/b/a MN Airlines', 'JetBlue Airways', 'Vision Airlines', 'United Air Lines Inc.'}\"}</code>\n</pre> <pre><code># A helper function to extract the airlines names from the edge attribute\ndef str_to_list(a):\n    return a[1:-1].split(\", \")\n</code></pre> <pre><code>for origin, dest in pass_2015_network.edges():\n    pass_2015_network[origin][dest][\"airlines_list\"] = str_to_list(\n        (pass_2015_network[origin][dest][\"airlines\"])\n    )\n</code></pre> <p>Let's extract the network of United Airlines from our airport network.</p> <pre><code>united_network = nx.DiGraph()\nfor origin, dest in pass_2015_network.edges():\n    if \"'United Air Lines Inc.'\" in pass_2015_network[origin][dest][\"airlines_list\"]:\n        united_network.add_edge(\n            origin, dest, weight=pass_2015_network[origin][dest][\"weight\"]\n        )\n</code></pre> <pre><code># number of nodes -&amp;gt; airports\n# number of edges -&amp;gt; routes\nprint(united_network)\n</code></pre> <pre>\n<code>DiGraph with 194 nodes and 1894 edges\n</code>\n</pre> <pre><code># Let's find United Hubs according to PageRank\nsorted(\n    nx.pagerank(united_network, weight=\"weight\").items(),\n    key=lambda x: x[1],\n    reverse=True,\n)[0:5]\n</code></pre> <pre>\n<code>[('ORD', 0.08385772266571428),\n ('DEN', 0.06816244850418422),\n ('LAX', 0.05306523414724009),\n ('IAH', 0.0444106090283792),\n ('SFO', 0.043261970302830285)]</code>\n</pre> <pre><code># Let's find United Hubs according to Degree Centrality\nsorted(nx.degree_centrality(united_network).items(), key=lambda x: x[1], reverse=True)[\n    0:5\n]\n</code></pre> <pre>\n<code>[('ORD', 1.0),\n ('IAH', 0.9274611398963731),\n ('DEN', 0.8756476683937824),\n ('EWR', 0.8134715025906736),\n ('SFO', 0.6839378238341969)]</code>\n</pre> <pre><code>from nams.solutions import airport\nimport inspect\n\nprint(inspect.getsource(airport))\n</code></pre> <pre>\n<code>import networkx as nx\nimport pandas as pd\n\n\ndef busiest_route(pass_air_data, year):\n    return pass_air_data[\n        pass_air_data.groupby([\"YEAR\"])[\"PASSENGERS\"].transform(max)\n        == pass_air_data[\"PASSENGERS\"]\n    ].query(f\"YEAR == {year}\")\n\n\ndef plot_time_series(pass_air_data, origin, dest):\n    pass_air_data.query(f\"ORIGIN == '{origin}' and DEST == '{dest}'\").plot(\n        \"YEAR\", \"PASSENGERS\"\n    )\n\n\ndef add_opinated_edges(G):\n    G = nx.DiGraph(G)\n    sort_degree = sorted(\n        nx.degree_centrality(G).items(), key=lambda x: x[1], reverse=True\n    )\n    top_count = 0\n    for n, v in sort_degree:\n        count = 0\n        for node, val in sort_degree:\n            if node != n:\n                if node not in G._adj[n]:\n                    G.add_edge(n, node)\n                    count += 1\n                    if count == 25:\n                        break\n        top_count += 1\n        if top_count == 20:\n            break\n    return G\n\n</code>\n</pre>"},{"location":"05-casestudies/02-airport/#introduction","title":"Introduction","text":"<p>In this chapter, we will analyse the evolution of US Airport Network between 1990 and 2015. This dataset contains data for 25 years[1995-2015] of flights between various US airports and metadata about these routes. Taken from Bureau of Transportation Statistics, United States Department of Transportation.</p> <p>Let's see what can we make out of this!</p>"},{"location":"05-casestudies/02-airport/#exercise","title":"Exercise","text":"<p>Find the busiest route in 1990 and in 2015 according to number of passengers, and plot the time series of number of passengers on these routes.</p> <p>You can use the DataFrame instead of working with the network. It will be faster :)</p>"},{"location":"05-casestudies/02-airport/#visualise-the-airports","title":"Visualise the airports","text":""},{"location":"05-casestudies/02-airport/#directed-graphs-and-pagerank","title":"Directed Graphs and PageRank","text":"<p>The figure below explains the basic idea behind the PageRank algorithm. The \"importance\" of the node depends on the incoming links to the node, i.e if an \"important\" node A points towards a node B it will increase the PageRank score of node B, and this is run iteratively. In the given figure, even though node C is only connected to one node it is considered \"important\" as the connection is to node B, which is an \"important\" node.</p> <p></p> <p>Source: Wikipedia</p> <p>To better understand this let's work through an example.</p>"},{"location":"05-casestudies/02-airport/#importants-hubs-in-the-airport-network","title":"Importants Hubs in the Airport Network","text":"<p>So let's have a look at the important nodes in this network, i.e. important airports in this network. We'll use centrality measures like pagerank, betweenness centrality and degree centrality which we gone through in this book.</p>"},{"location":"05-casestudies/02-airport/#how-reachable-is-this-network","title":"How reachable is this network?","text":"<p>Let's assume you are the Head of Data Science of an airline and your job is to make your airline network as \"connected\" as possible.</p> <p>To translate this problem statement to network science, we calculate the average shortest path length of this network, it gives us an idea about the number of jumps we need to make around the network to go from one airport to any other airport in this network on average.</p>"},{"location":"05-casestudies/02-airport/#strongly-vs-weakly-connected-graphs","title":"Strongly vs weakly connected graphs.","text":"<p>Let's go through an example to understand weakly and strongly connected directed graphs.</p>"},{"location":"05-casestudies/02-airport/#exercise_1","title":"Exercise","text":"<p>How can we decrease the average shortest path length of this network?</p> <p>Think of an effective way to add new edges to decrease the average shortest path length. Let's see if we can come up with a nice way to do this.</p> <p>The rules are simple: - You can't add more than 2% of the current edges( ~500 edges)</p>"},{"location":"05-casestudies/02-airport/#can-we-find-airline-specific-reachability","title":"Can we find airline specific reachability?","text":"<p>Let's see how we can use the airline metadata to calculate the reachability of a specific airline.</p>"},{"location":"05-casestudies/02-airport/#solutions","title":"Solutions","text":"<p>Here are the solutions to the exercises above.</p>"},{"location":"devdocs/style/","title":"Style Guide","text":"<p>This is the style guide for writing notebooks and markdown files for the book.</p> <p>Intended as a guide when there is ambiguity in how to format something. Updated it when new decisions are made for uncertain circumstances.</p>"},{"location":"devdocs/style/#notebooks","title":"Notebooks","text":""},{"location":"devdocs/style/#headers","title":"Headers","text":"<p>Jupyter notebook headers should begin at the 2nd level. In other words:</p> <pre><code>## Introdction (this is correct!)\n</code></pre> <p>should be the first header, and not:</p> <pre><code># Introdction (this is wrong!)\n</code></pre> <p>This allows <code>mkdocs</code> to insert the \"Chapter X\" heading at the top of the compiled Markdown document.</p>"},{"location":"devdocs/style/#exercises","title":"Exercises","text":"<p>Exercises should be at the 3rd level of headers.</p> <p>For exercises that yield a plot, allow the exercise cell to be executed.</p> <p>For exercises that modify an object that is used later, allow the exercise cell to be executed.</p> <p>For exercises that are implementation-oriented, and do not affect notebook state, it is recommended that the execution be commented out to save on execution time.</p> <p>For exercises that require answering a question, wrap the answer in a triple quote string, use the <code>markdown</code> package to parse it into HTML, and then use IPython's HTML display facility to show the answer in beautiful HTML. A convenience function called <code>render_html</code> is provided. Here's an example:</p> <pre><code>from nams.functions import render_html\n\ndef bipartite_degree_centrality_denominator():\n    ans = \"\"\"\nSome answer goes here!\nWritten in **Markdown**.\n\"\"\"\n    return render_html(ans)\n</code></pre> <p>Indentation is super important!</p> <p>Left indentation on the answer string cannot be present, otherwise the answer will not render correctly in HTML form!</p>"},{"location":"devdocs/style/#solutions","title":"Solutions","text":"<p>Exercise solutions should be placed in the corresponding <code>nams.solutions.&lt;notebook_name_without_extension&gt;</code> Python submodule.</p> <p>Code solutions should always be present at the bottom of the notebook.</p> <p>Use the following code block to help:</p> <pre><code>import inspect\nfrom nams.solutions import {{ notebook_name }}\n\nprint(inspect({{ notebook_name }}))\n</code></pre>"},{"location":"devdocs/style/#execution","title":"Execution","text":"<p>Notebooks should run from top-to-bottom without erroring out.</p> <p>Notebooks ideally should run in under 10 seconds. However, if a notebook needs up to 30 seconds to finish execution, that is acceptable. No notebook should take on the order of minutes to finish.</p>"}]}